{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042e36bf-97d6-4785-84e4-ddfb0fe2fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2e02ef-d029-4069-b940-427b9a2eeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fa8105-da77-4959-9e63-cd1fe96d74d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>30000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sudhanshu</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Paul</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Harsha</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>15000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Shubham</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>18000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Name  Age  Experience  Salary\n",
       "0      Krish   31          10   30000\n",
       "1  Sudhanshu   30           8   25000\n",
       "2      Sunny   29           4   20000\n",
       "3       Paul   24           3   20000\n",
       "4     Harsha   21           1   15000\n",
       "5    Shubham   23           2   18000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b031757-3522-4490-9b07-92fa8e0328f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.read_csv('test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10f9e83-c4d0-483c-af0d-7a5df734eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea983",
   "metadata": {},
   "source": [
    "This starts the spark session and enables us to run it in a single-node cluster called the 'Master' node, or host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62403c9-9ce1-4d7b-bec8-2d719b51eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7974a605-8477-4cf4-ad82-6ce14634ea77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27666a9f9a0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123a2a4a-7298-4024-89d0-0b93dcdd0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92bf280f-1fb0-4fb5-9acb-80365454729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|      _c0|_c1|       _c2|   _c3|\n",
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2dabe54-3a79-4264-897d-98c57f6ac5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string, Salary: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3159cc0d-1d2c-4aaf-9ab1-e91d918ec64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da3c242-6577-437a-9f6b-05e5240c9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91131f54-6b4d-4193-85b8-1e85dc84bd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac53d87-7e00-45e8-8566-741efcd1df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3343069-d437-4c54-8ebc-3c25f9febdef",
   "metadata": {},
   "source": [
    "This is a SQL dataframe, similar to a Pandas dataframe (if you want to convert to a Pandas dataframe simply apply the .toPandas() method). Let's check to see if we can read the first few rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "527246b2-df1e-43e6-b785-cb0b7c9804d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', Age='31', Experience='10', Salary='30000'),\n",
       " Row(Name='Sudhanshu', Age='30', Experience='8', Salary='25000'),\n",
       " Row(Name='Sunny', Age='29', Experience='4', Salary='20000')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c072adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3969f7",
   "metadata": {},
   "source": [
    "The 'select( )' method must be used to identify a column name to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e84501d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a618cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    Krish|\n",
      "|Sudhanshu|\n",
      "|    Sunny|\n",
      "|     Paul|\n",
      "|   Harsha|\n",
      "|  Shubham|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b693c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d661",
   "metadata": {},
   "source": [
    "Selecting more than one column to reveal the row entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "becdc1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     Name|Experience|\n",
      "+---------+----------+\n",
      "|    Krish|        10|\n",
      "|Sudhanshu|         8|\n",
      "|    Sunny|         4|\n",
      "|     Paul|         3|\n",
      "|   Harsha|         1|\n",
      "|  Shubham|         2|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da19052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9e0a68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'),\n",
       " ('Age', 'string'),\n",
       " ('Experience', 'string'),\n",
       " ('Salary', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d97264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Age: string, Experience: string, Salary: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7448e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------------------+-----------------+------------------+\n",
      "|summary|  Name|               Age|       Experience|            Salary|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "|  count|     6|                 6|                6|                 6|\n",
      "|   mean|  null|26.333333333333332|4.666666666666667|21333.333333333332|\n",
      "| stddev|  null| 4.179314138308661|3.559026084010437| 5354.126134736337|\n",
      "|    min|Harsha|                21|                1|             15000|\n",
      "|    max| Sunny|                31|                8|             30000|\n",
      "+-------+------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f107e",
   "metadata": {},
   "source": [
    "Obviously no numeric values can be used for the string 'Name' variable. The min and max values for the 'Name' variable have been determined by the index number values which happen to be lowest for Krish and highest for Sunny.\n",
    "\n",
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9fee0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string, Salary: string, Experience After 2 Years: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b26a4",
   "metadata": {},
   "source": [
    "In order for this 'withColumn' method to be reflected it must be assigned to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcc3e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6efbd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string, Salary: string, Experience After 2 Years: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdceee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+------------------------+\n",
      "|     Name|Age|Experience|Salary|Experience After 2 Years|\n",
      "+---------+---+----------+------+------------------------+\n",
      "|    Krish| 31|        10| 30000|                    12.0|\n",
      "|Sudhanshu| 30|         8| 25000|                    10.0|\n",
      "|    Sunny| 29|         4| 20000|                     6.0|\n",
      "|     Paul| 24|         3| 20000|                     5.0|\n",
      "|   Harsha| 21|         1| 15000|                     3.0|\n",
      "|  Shubham| 23|         2| 18000|                     4.0|\n",
      "+---------+---+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2262f",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6efa087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Experience After 2 Years').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4105978",
   "metadata": {},
   "source": [
    "Once again, assign this method to a variable, so in order to see that the column has been dropped assign it to the df_pyspark variable once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5e2b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39475b",
   "metadata": {},
   "source": [
    "## Re-naming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e8dbec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "| New Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8377af",
   "metadata": {},
   "source": [
    "## PySpark Handling Missing Values\n",
    "1. Dropping Columns\n",
    "2. Dropping Rows\n",
    "3. Various Parameter in Dropping Functionalities\n",
    "4. Handling Missing Values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "546b5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faf030bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7834a1",
   "metadata": {},
   "source": [
    "To see the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b34f5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbcf40",
   "metadata": {},
   "source": [
    "Save the dataset as a dataframe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c77e73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58ef7f",
   "metadata": {},
   "source": [
    "This could also be achieved by applying the .toDF() method to the spark dataset, then assigning it to the 'df_pyspark' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5b45992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f593fb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f620ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb1ef",
   "metadata": {},
   "source": [
    "## Dropping the Columns (Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22b83760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|null|      null| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b3b44",
   "metadata": {},
   "source": [
    "To reset the dataframe simply use the show( ) method again. The 'Name' column will only be dropped permanently if the value of the expression is assigned. It could be assigned once again to 'df_pyspark', or to a completely different variable name such as 'df', or perhaps something more easily remembered like 'no_name' but the important point to remember is that until the expression is assigned it will not be stored in local memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0c79102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeccf11",
   "metadata": {},
   "source": [
    "## Dropping Specific Rows\n",
    "\n",
    "This will drop any rows with Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe272ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ff4e2",
   "metadata": {},
   "source": [
    "### Drop Function\n",
    "\n",
    "Looking at the arguments in the drop( ) function we have: 'how', 'thresh' and 'subset'. I can view these simply by placing the cursor at the function parentheses and typing Shift-Tab. This will show a little drop down comment bubble explaining configuration options for each function argument. (Actually, it's also important to note that key-value args always follow positional args in the order).\n",
    "\n",
    "Hitting the '+' icon in the top right of the dropdown bubble expands the explanation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e1c4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how argument\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d6f69",
   "metadata": {},
   "source": [
    "In this example dataset the drop( ) method has removed 'any' instance which contains a value of null. If we set the 'how' arg to 'all', then it would only remove an instance or row in the dataset if all values were null within that instance or row. See below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd5aa499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how argument set to 'all'\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232095",
   "metadata": {},
   "source": [
    "So the how argument doesn't remove any of the instances this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78df6ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh\n",
    "df_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b00797",
   "metadata": {},
   "source": [
    "The 'threshold' argument means that the row or instance will only be dropped if there are more non-null values than the threshold specified! If set to two then there must be at least 3 non-null values in the row before it's dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03cbc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "|    null| 34|        10| 38000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset\n",
    "df_pyspark.na.drop(how='any', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa154d",
   "metadata": {},
   "source": [
    "Only those null values which appear in the Experience column will have their rows dropped. This is a form of dataset slicing in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f902bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b51cb1",
   "metadata": {},
   "source": [
    "## Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ec6e876-46ef-4cc5-8650-76c18dae68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| Age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|      Sudhansh|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|Missing Values|  34|        10| 38000|\n",
      "|Missing Values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db765f",
   "metadata": {},
   "source": [
    "This doesn't seem to be working! Why not? The fill method only appears to be changing the values in one column, the 'Name' column. It should be changing the null values in all columns. \n",
    "\n",
    "According to the PySpark documentation there are two main arguments to address which are the positional replacement 'value' (a string, number, or simply \" \") for the Null items in your dataset and the 'subset' key-value argument which can be set to None, or a list [], or tuple () of 'column_name' values. It can represent only one column's values, or several column values if desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bff497b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "| Missing|  34|        10| 38000|\n",
      "| Missing|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value='Missing', subset=None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b44f7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "| Missing|  34|        10| 38000|\n",
      "| Missing|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value='Missing', subset=['Name','Age','Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58706cb",
   "metadata": {},
   "source": [
    "Try setting fill(value=0) for all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31508928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "|  Mahesh|  0|         0| 40000|\n",
      "|    null| 34|        10| 38000|\n",
      "|    null| 36|         0|     0|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7419d4",
   "metadata": {},
   "source": [
    "This hasn't worked either which tells me the problem lies in the different datatypes associated with each attribute or column. The 'Name' column is the only type which is a String. All the other columns are Integer, so this is preventing the operation from occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef37fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|        |  34|        10| 38000|\n",
      "|        |  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.na.fill(\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589df6b",
   "metadata": {},
   "source": [
    "Storing the na.fill( ) method in a variable doesn't seem to work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2e2ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Krish|           31|           10|        30000|\n",
      "|     Sudhansh|           30|            8|        25000|\n",
      "|        Sunny|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|       Harsha|           21|            1|        15000|\n",
      "|      Shubham|           23|            2|        18000|\n",
      "|       Mahesh|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file using PySpark\n",
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Cast integer columns to string data type\n",
    "df_pyspark = df_pyspark.withColumn('Age', df_pyspark['Age'].cast('string'))\n",
    "df_pyspark = df_pyspark.withColumn('Experience', df_pyspark['Experience'].cast('string'))\n",
    "df_pyspark = df_pyspark.withColumn('Salary', df_pyspark['Salary'].cast('string'))\n",
    "\n",
    "# Replace all null values with 'Missing Value'\n",
    "df_pyspark = df_pyspark.fillna('Missing Value')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ac9525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Krish|           31|           10|        30000|\n",
      "|     Sudhansh|           30|            8|        25000|\n",
      "|        Sunny|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|       Harsha|           21|            1|        15000|\n",
      "|      Shubham|           23|            2|        18000|\n",
      "|       Mahesh|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Value', ['Experience','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd8b1839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Krish|           31|           10|        30000|\n",
      "|     Sudhansh|           30|            8|        25000|\n",
      "|        Sunny|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|       Harsha|           21|            1|        15000|\n",
      "|      Shubham|           23|            2|        18000|\n",
      "|       Mahesh|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793b3a",
   "metadata": {},
   "source": [
    "Replace the Null values with the mean values for a particular column or for each column. This involves imputing the mean values. In this example I will replace the Null values in the 'Experience' column only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80476513-7ef9-4824-88eb-d637d52b564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast string columns to integer data type\n",
    "df_pyspark = df_pyspark.withColumn('Age', df_pyspark['Age'].cast('integer'))\n",
    "df_pyspark = df_pyspark.withColumn('Experience', df_pyspark['Experience'].cast('integer'))\n",
    "df_pyspark = df_pyspark.withColumn('Salary', df_pyspark['Salary'].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "576bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "730d629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9e99668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|         Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|        Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|     Sudhansh|  30|         8| 25000|         30|                 8|         25000|\n",
      "|        Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|         Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|       Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|      Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|       Mahesh|null|      null| 40000|         28|                 5|         40000|\n",
      "|Missing Value|  34|        10| 38000|         34|                10|         38000|\n",
      "|Missing Value|  36|      null|  null|         36|                 5|         25750|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b547669-8e23-4511-9879-6d1f297ad0ee",
   "metadata": {},
   "source": [
    "The same can be done with median values also. Note, the imputed values are created in entirely new columns with the average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f673025d-a4d7-4a14-bca8-7ccfa8b193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|         Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|        Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|     Sudhansh|  30|         8| 25000|         30|                 8|         25000|\n",
      "|        Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|         Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|       Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|      Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|       Mahesh|null|      null| 40000|         29|                 4|         40000|\n",
      "|Missing Value|  34|        10| 38000|         34|                10|         38000|\n",
      "|Missing Value|  36|      null|  null|         36|                 4|         20000|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]).setStrategy(\"median\")\n",
    "\n",
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dba97b-aa88-4b89-a346-f983efa35f3a",
   "metadata": {},
   "source": [
    "## PySpark DataFrames\n",
    "1. Filter Operations\n",
    "2. &, |, == (And, Or, Equals)\n",
    "3. ~ (Not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4873d9c-46c3-4ab9-98ae-b40c6e7a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0912678e-bb2b-48bf-9099-8ca5ae4d7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c835e1b1-1b9a-497f-881d-71f8241adb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94017cfe-b29c-46a9-a59b-f7523af84ed6",
   "metadata": {},
   "source": [
    "There are a number of filters which can be applied using Pandas dataframes also. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2d0f7-d731-4f37-8045-a1946092aa9b",
   "metadata": {},
   "source": [
    "## Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd32e7b1-763b-4260-9579-1d9638dbafe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# salary of people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e95ca753-df3a-4366-ac1f-b80580bf214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   Name|Age|\n",
      "+-------+---+\n",
      "|  Sunny| 29|\n",
      "|   Paul| 24|\n",
      "| Harsha| 21|\n",
      "|Shubham| 23|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e413d61-a50c-46c2-be79-8eb0fff16b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf83bbf-f4ef-4680-87d1-0dc5af9bc924",
   "metadata": {},
   "source": [
    "### The AND operand\n",
    "An important point to note here is that the comparison statements which are being evaluated, must be contained within parentheses (brackets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b8356b4-6482-4a16-8a70-226d5891dcbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|Age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) & (df_pyspark['Salary']>=18000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a962c-719c-4085-881f-37afa9e4ab25",
   "metadata": {},
   "source": [
    "### The OR operand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "85c799cd-9aa0-4901-b400-3ab45a33358b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) | (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688cde5-2521-4835-b655-a05f95c84f76",
   "metadata": {},
   "source": [
    "### The NOT operand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5fe6a3f5-6470-4d22-bad8-7444845dd0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbca948-bc14-46ef-802d-22ae1dbbadb6",
   "metadata": {},
   "source": [
    "## PySpark GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f675d99-08de-4cb2-a64d-11df6eae3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b4ec3bf3-e82b-4afd-94d6-06f5b6c2f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Agg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "52a9cf4d-c45a-4c42-a6ee-14228d2d01af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x27666a9f9a0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5bf46793-adf9-4a48-b8ba-8184b14352a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test3.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b05ee034-0370-48b7-a807-bb40123b8260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|Salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0a6a5f3d-56ef-4f5c-864c-91e889daef1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2db610-9e39-42eb-8d4f-ca11a328b015",
   "metadata": {},
   "source": [
    "So 'Name' is a string, 'Departments' is a string and 'Salary' is an integer data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44918445-c7da-4346-82d7-4a6ee3ea5a93",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "#### Grouped to Find the Maximum Salary\n",
    "Finding the mean salary group by Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87c1a2f0-69b8-462a-921e-9e62691c7b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x27668833820>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b350e-73be-48d7-8146-521b1d0a66d6",
   "metadata": {},
   "source": [
    "GroupBy and Aggregate functions work together, so once groupBy( ) is applied, then an agg( ) function can follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4f823903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, sum(Salary): bigint]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f7e82-c55f-42fd-9b84-3ae0dde65210",
   "metadata": {},
   "source": [
    "This time a SQL dataframe is returned.\n",
    "\n",
    "Let's show this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0ad1a833-f5c5-443d-bdf6-c0edb405a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(Salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e5fd8-39b1-4077-bdb9-ef109cd4e14d",
   "metadata": {},
   "source": [
    "Press dot or period and tab...this will provide a whole list of possible functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "307228ca-6bb1-4303-8412-57c6f962c19f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|sum(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|      15000|\n",
      "|    Big Data|      15000|\n",
      "|Data Science|      43000|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "56d9f84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(Salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "561c6b50-b133-4b0d-b651-3918a5108fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bf677-8442-4990-b550-c4a2cba8ba48",
   "metadata": {},
   "source": [
    "Try applying a direct aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "eee5ac69-bf39-4930-9b23-23ecea6e9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|      73000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "441918d5-d07f-40d4-8cd5-cd535aa9d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|max(Salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      20000|\n",
      "|    Sunny|      10000|\n",
      "|    Krish|      10000|\n",
      "|   Mahesh|       4000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e7140-5473-485a-b27f-7304119b8e4b",
   "metadata": {},
   "source": [
    "And also, to see the minimum value with respect to Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3766f42-8e0a-4251-beeb-dce654eba75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|min(Salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|       5000|\n",
      "|    Sunny|       2000|\n",
      "|    Krish|       4000|\n",
      "|   Mahesh|       3000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f98ef49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+\n",
      "|     Name|       avg(Salary)|\n",
      "+---------+------------------+\n",
      "|Sudhanshu|11666.666666666666|\n",
      "|    Sunny|            6000.0|\n",
      "|    Krish| 6333.333333333333|\n",
      "|   Mahesh|            3500.0|\n",
      "+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608f7d5-7974-4bd4-b380-b71fd5283e4a",
   "metadata": {},
   "source": [
    "## Machine Learning with DataFrame API and RDD API's\n",
    "### Examples of PySpark ML\n",
    "Using a Linear Regression example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ff8ee953-a115-4bf6-9d9a-486d4c7e05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "77168768-74d1-4150-92b0-73246f8c8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "training = spark.read.csv('test1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6bc8e9ac-c1c6-42d2-b954-b107eef9ad73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|Age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c8e4efb-ba81-47ae-81dd-42175fe886db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2b3e7b98-5ff6-455a-878c-942c94e7b8aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42461d-eb30-431a-b1d2-fb32efae5935",
   "metadata": {},
   "source": [
    "In sklearn we have a dataset which we would perform a train-test split on, providing us with independent and dependent variables. In PySpark it's slightly different.\n",
    "\n",
    "['Age','Experience'] ----> New Feature ----> Independent Feature\n",
    "\n",
    "Grouping the independent variable features together is performed using the 'VectorAssembler' module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78959fd2-b04a-4b28-8495-e65ae068bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=[\"Age\", \"Experience\"], outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "504bf1a5-117d-47eb-afb7-1c417b9b2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5f33261d-b42a-44f5-897b-3c2a90f00da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|Age|Experience|Salary|Independent Features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771528cb",
   "metadata": {},
   "source": [
    "Note the 'Independent Features' column has 2 values in each row listing Age and Experience. This is a type of feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "26b3e999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary', 'Independent Features']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d749136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent Features\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b90ef215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99890163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# train test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol=\"Independent Features\", labelCol=\"Salary\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "0346d4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-323.2867, 1696.8066])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c4fb78cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22295.299605312008"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "69ae82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "47ca336b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent Features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [21.0,1.0]| 15000|17203.085755292603|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9e3e375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2203.0857552926027, 4853586.845173177)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc6d69",
   "metadata": {},
   "source": [
    "## Multi-Linear Regression Problem\n",
    "There are several features included in the independent values. In Databricks, you can start on the Databricks icon top left then click on 'Import & Explore Data' in the middle. Or alternatively, go to the 'Data' icon on the left menu and select the 'tips.csv' file and upload it.\n",
    "\n",
    "You'll be presented with a choice of 'Create Table with UI' or 'Create Table in Notebook'. The file will be uplodaed into the 'DBFS' file system which is one of the tabs at the top. The file path will look something like: '/FileStore/tables/tips.csv'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1f42e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e974cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb25b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e887e5a-9281-4649-af50-72e669649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334b0bf-c0b6-4127-b62a-98b435cb5183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
