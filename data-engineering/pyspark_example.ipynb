{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63a1f44a-f3e5-4506-b891-a33576c6e95d",
   "metadata": {},
   "source": [
    "This accompanies the Youtube video entitled: \"The ONLY PySpark Tutorial You Will Ever Need\" by Moran Reznik.\n",
    "\n",
    "Please note that you need to have a running Spark cluster to execute these commands!\n",
    "\n",
    "Create a SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba4f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6deff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2bcb6d-e024-4c6d-ace1-359ae3d41dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62491745",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333b5e2-6e27-48ac-b124-87b01e80cc5c",
   "metadata": {},
   "source": [
    "Create a DataFrame from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c44382-d3c6-47cd-8be6-26c046b1e7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header','true').csv('heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell pyspark the type of the columns - saves time on large dataset. there are other ways to do this, but that's my favorite\n",
    "schema = 'Age INTEGER, Sex STRING, ChestPainType STRING'\n",
    "df = spark.read.csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/data-engineering/heart.csv', schema=schema, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e7b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let PySpark infer the schema\n",
    "df = spark.read.csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/data-engineering/heart.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3c117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace nulls with other value at reading time\n",
    "df = spark.read.csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/data-engineering/heart.csv', nullValue='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5350a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "df.write.format(\"csv\").save(\"heart_save.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33c5b400",
   "metadata": {},
   "source": [
    "Saving won't let you write over an existing file. To do that, you need to set the 'mode' to overwrite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c64e4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to overwrite the file\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(\"heart_save.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f7b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show head of table\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e881c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count number of rows\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show parts of the table\n",
    "df.select('Age').show(3)\n",
    "df.select(['Age', 'Sex']).show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c15ad8a4",
   "metadata": {},
   "source": [
    "## Caching\n",
    "every time you run a DAG, it will be re-computed from the beginning. that is, the results are not saved in memory. so, if we want to save a result so it won't have to be recomputed, we can use the cache command. note, that this will occupy space in the working node's memory - so be careful with the sizes of datasets you are caching! by default, the cached DF is stored to RAM, and is unserialized (not converted into a stream of bytes). you can change both of these - store data to hard disk, serialized it, or both!\n",
    "\n",
    "## Collecting\n",
    "even after caching a DataFrame, it still sits in the worker nodes memory. if you want to collect is pieces, assemble them and save them on the master node so you won't have to pull it every time, use the command for collecting. again, be very careful with this, since the collected file will have to fit in the master node memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70a38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7ebf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert PySpark DataFrame to Pandas DataFrame\n",
    "pd_df = df.toPandas()\n",
    "# convert it back\n",
    "spark_df = spark.createDataFrame(pd_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402eabee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show first three rows as three row objects, which is how spark represents single rows from a table.\n",
    "# we will learn more about it later\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9484e-17c9-4808-9c22-bc5eeb99753b",
   "metadata": {},
   "source": [
    "Print the DataFrame's schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329dfeb-cba8-43bf-94f7-9426518adf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type as columns\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bcec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# column dtypes as list of tuples\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb3e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast a column from one type to other\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "df = df.withColumn(\"Age\", df.Age.cast(FloatType()))\n",
    "df = df.withColumn(\"RestingBP\", df.Age.cast(FloatType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute summery statistics\n",
    "df.select(['Age', 'RestingBP']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ead48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column or replace existing one\n",
    "AgeFixed = df['Age'] + 1  # select alwayes returns a DataFrame object, and we need a column object\n",
    "df = df.withColumn('AgeFixed', AgeFixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a746bda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['AgeFixed', 'Age']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6869701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns\n",
    "df.drop('AgeFixed').show(1)  # add df = to get the new DataFrame into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4e7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename a column\n",
    "df.withColumnRenamed('Age', 'age').select('age').show(1)\n",
    "# to rename more than a single column, i would suggest a loop.\n",
    "name_pairs = [('Age', 'age'), ('Sex', 'sex')]\n",
    "for old_name, new_name in name_pairs:\n",
    "    df = df.withColumnRenamed(old_name, new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda09676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['age','sex']).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5164af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all rows that contain any NA\n",
    "df = df.na.drop()\n",
    "df.count()\n",
    "# drop all rows where all values are NA\n",
    "df = df.na.drop(how='all')\n",
    "# drop all rows where more at least 2 values are NOT NA\n",
    "df = df.na.drop(thresh=2)\n",
    "# drop all rows where any value at specific columns are NAs.\n",
    "df = df.na.drop(how='any', subset=['age', 'sex'])  # 'any' is the defult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d4554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing values in a specific column with a '?'\n",
    "df = df.na.fill(value='?', subset=['sex'])\n",
    "# replace NAs with mean of column\n",
    "from pyspark.ml.feature import Imputer  # In statistics, imputation is the process of\n",
    "# replacing missing data with substituted values\n",
    "imptr = Imputer(inputCols=['age', 'RestingBP'],\n",
    "                outputCols=['age', 'RestingBP']).setStrategy('mean')  # can also be 'median' and so on\n",
    "\n",
    "df = imptr.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb3a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to adults only and calculate mean\n",
    "df.filter('age > 18')\n",
    "df.where('age > 18')  # 'where' is an alias to 'filter'\n",
    "df.where(df['age'] > 18)  # third option\n",
    "# add another condition ('&' means and, '|' means or)\n",
    "df.where((df['age'] > 18) | (df['ChestPainType'] == 'ATA'))\n",
    "# take every record where the 'ChestPainType' is NOT 'ATA'\n",
    "df.filter(~(df['ChestPainType'] == 'ATA'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecf9c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter('age > 18').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e645d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a string expression into command\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "exp = 'age + 0.2 * AgeFixed'\n",
    "df.withColumn('new_col', expr(exp)).select('new_col').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by age\n",
    "disease_by_age = df.groupby('age').mean().select(['age', 'avg(HeartDisease)'])\n",
    "# sort values in desnding order\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "disease_by_age.orderBy(desc(\"age\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d836d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import asc\n",
    "\n",
    "disease_by_age = df.groupby('age').mean().select(['age', 'avg(HeartDisease)'])\n",
    "disease_by_age.orderBy(desc(\"age\")).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a687f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate to get several statistics for several columns\n",
    "# the available aggregate functions are avg, max, min, sum, count\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.agg(F.min(df['age']), F.max(df['age']), F.avg(df['sex'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07c1dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('HeartDisease').agg(F.min(df['age']), F.avg(df['sex'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e605bfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run an SQL query on the data\n",
    "df.createOrReplaceTempView(\n",
    "    \"df\")  # tell PySpark how the table will be called in the SQL query\n",
    "spark.sql(\"\"\"SELECT sex from df\"\"\").show(2)\n",
    "\n",
    "# we also choose columns using SQL sytnx, with a command that combins '.select()' and '.sql()'\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('age').pivot('sex', (\"M\", \"F\")).count().show(3)››› "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ddc163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot - expensive operation\n",
    "df.selectExpr(\"age >= 40 as older\", \"age\",'sex').groupBy(\"sex\")\\\n",
    "                    .pivot(\"older\", (\"true\", \"false\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(['age', 'MaxHR', 'Cholesterol']).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50674821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide dataset to training features and target\n",
    "X_column_names = ['Age', 'Cholesterol']\n",
    "target_colum_name = ['MaxHR']\n",
    "\n",
    "# convert feature columns into a columns where the vlues are feature vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "v_asmblr = VectorAssembler(inputCols=X_column_names, outputCol='Fvec')\n",
    "df = v_asmblr.transform(df)\n",
    "X = df.select(['Age', 'Cholesterol', 'Fvec', 'MaxHR'])\n",
    "X.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f06f169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# devide dataset into training and testing sets\n",
    "trainset, testset = X.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7cf4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 'RestingBP' using linear regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "model = LinearRegression(featuresCol='Fvec', labelCol='MaxHR')\n",
    "model = model.fit(trainset)\n",
    "print(model.coefficients)\n",
    "print(model.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model\n",
    "model.evaluate(testset).predictions.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa542af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handel categorical features with ordinal indexing\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indxr = StringIndexer(inputCol='ChestPainType', outputCol='ChestPainTypeInxed')\n",
    "indxr.fit(df).transform(df).select('ChestPainTypeInxed').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa17c3-790d-408a-abe8-50763dc48b6f",
   "metadata": {},
   "source": [
    "Stop the SparkSession."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cfbb42-d905-4f0b-b518-b7f4d685281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
