{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50f1d16",
   "metadata": {},
   "source": [
    "# ETL Using Python\n",
    "\n",
    "We will be ingesting data from the Microsoft AdventureWorks Sample Databases: https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver16&tabs=ssms.\n",
    "\n",
    "We can choose from either the OLTP, Data Warehouse or Lightweight '.bak' files. The Source is a database from SQL Server which is then processed using Extract-Load-Transform before reaching it's Destination as a PostgreSQL database.\n",
    "\n",
    "So, we're using SQL Server 'AdventureWorksDW2019.bak' as the database source, before loading it into PostgreSQL using Python:\n",
    "\n",
    "1. ELT is a fundamental workflow used in data engineering\n",
    "2. The data source can be an API, a db or a flatfile.\n",
    "3. The source data is extracted into a 'Staging Area', transformed into a product ready to be loaded into it's destination or target database (stored in a Data Lake or Data Warehouse).\n",
    "4. We'll use Python (Pandas library) to build the ETL pipelines.\n",
    "\n",
    "Extract: get data from a source.\n",
    "Transform: structure, format or clean the data.\n",
    "Load: write the data to an external target / destination.\n",
    "\n",
    "## Sources\n",
    "### Databases\n",
    "RDBMS / NoSQL\n",
    "### Files\n",
    "csv / json / xml\n",
    "### SaaS Applications\n",
    "REST API's\n",
    "### Application Events\n",
    "Webhook\n",
    "\n",
    "It's Extracted into the Staging Area where it's Transformed before being Loaded into a Data Warehouse to be Analyzed for BI, ML, Data Science and Analytics end products.\n",
    "\n",
    "Another important point to note is that there are several different formats, database drivers and types of structure which need to be learned.\n",
    "\n",
    "## PostgreSQL Set Up\n",
    "### SQL Query\n",
    "This query is in the pgAdmin4 (PostgreSQL db driver)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423eff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- Database: AdventureWorks\n",
    "\n",
    "-- DROP DATABASE \"AdventureWorks\";\n",
    "\n",
    "CREATE DATABASE \"AdventureWorks\"\n",
    "    WITH\n",
    "    OWNER = postgres\n",
    "    ENCODING = 'UTF8'\n",
    "    LC_COLLATE = 'English_United States.1252'\n",
    "    TABLESPACE = pg_default\n",
    "    CONNECTION LIMIT = -1;\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e707f188",
   "metadata": {},
   "source": [
    "This creates a database in PostgreSQL. \n",
    "* Refresh the databases file manager on the left hand side, open the 'Databases' folder to see the 'AdventureWorks' db.\n",
    "\n",
    "### Create a New ETL User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e2e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-- create etl user\n",
    "CREATE USER etl WITH PASSWORD 'demopass';\n",
    "-- grant connect\n",
    "GRANT CONNECT ON DATABASE \"AdventureWorks\" TO etl;\n",
    "-- grant table permissions\n",
    "GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO etl;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51411d",
   "metadata": {},
   "source": [
    "In file manager go to: 'Login / Group Roles'. You should see a new user called 'etl' under this folder, once the folders have been refreshed.\n",
    "\n",
    "Our PostgreSQL db has been set up!\n",
    "\n",
    "## SQL Server Management Studio Set Up\n",
    "### Create a Login File Called 'etl'\n",
    "This will be a '.sql' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acdf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "USE[master]\n",
    "GO\n",
    "CREATE LOGIN[etl] WITH PASSWORD = N'demopass',\n",
    "DEFAULT_DATABASE = [AdventureWorksDW2019],\n",
    "CHECK_EXPIRATION = OFF, CHECK_POLICY = OFF\n",
    "GO\n",
    "USE [AdventureWorksDW2019]\n",
    "GO\n",
    "CREATE USER [etl] FOR LOGIN [etl]\n",
    "GO\n",
    "USE [AdventureWorksDW2019]\n",
    "GO\n",
    "ALTER ROLE [db_datareader] ADD MEMBER [etl]\n",
    "GO\n",
    "USE [master]\n",
    "GO\n",
    "GRANT CONNECT SQL TO [etl]\n",
    "GO\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b143789",
   "metadata": {},
   "source": [
    "This creates an 'etl' user along with login. We're giving necessary permissions to connect and read data from the 'AdventureWorksDW2019' db.\n",
    "\n",
    "The login window will pop-up so hit 'Connect'!\n",
    "\n",
    "In the SSMS file manager in the folder called 'Security' and sub-folder called 'Users' you should see the new 'etl' user name file generated. So check the following path: 'Security' -> 'Logins' -> 'etl'.\n",
    "\n",
    "On the SQL Server web site at: https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver16&tabs=ssms there should be 3 different versions of the AdventureWorks database:\n",
    "\n",
    "i) OLTP\n",
    "ii) DW\n",
    "iii) Lightweight\n",
    "\n",
    "## ETL Pipeline Code\n",
    "Using a text editor or IDE:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378594ef",
   "metadata": {},
   "source": [
    "'sqlalchemy' is the module used to interact with PostgreSQL. \n",
    "\n",
    "'pyodbc' is the module used to query SQL Server. \n",
    "\n",
    "'pandas' is the module used to perform the data extraction / loading. \n",
    "\n",
    "'os' is the module used to retrieve the username and password which in this case is stored separately in the 'System -> Environment Variables -> System Variables (Lower window section)'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20a41b99",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "There are some important points to note here regarding authentication access to the database drivers.\n",
    "The user's credentials could be stored in a '.sh' , '.ps' or an '.xml' file. \n",
    "!! WARNING !! Do not hard code your credentials in the script. Store them separately (i.e. in 'Environment Variables') maybe as a shell script .sh file. If you store it in 'System Variables' and name it something like:\n",
    "\n",
    "Variable = %PGPASS%, Value = demopass.\n",
    "\n",
    "Hit OK!\n",
    "\n",
    "The aim is to protect the users credentials from being exposed in the 'etl' Python script. Define a variable to store the SQL Server driver.\n",
    "Note: you can use a configuration file and store it in 'System' -> 'Environment Variables'. Grab the password from the environment variable next.\n",
    "\n",
    "Grab the password from the environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get password from environment var\n",
    "pwd = os.environ['PGPASS']\n",
    "uid = os.environ['PGUID']\n",
    "#sql db details\n",
    "driver = \"{SQL Server Native Client 11.0}\"\n",
    "server = \"haq-PC\"\n",
    "database = \"AdventureWorksDW2019;\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe18114b",
   "metadata": {},
   "source": [
    "Pass in the key for the password. Similarly, you can get the User Id and store it in a local variable. Then define a varianle to store the SQL Server driver:\n",
    "\n",
    "# sql db details\n",
    "driver = \"{SQL Server Native Client 11.0}\"\n",
    "\n",
    "You may need to install this locally on your machine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745de89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data from sql server\n",
    "def extract():\n",
    "    try:\n",
    "        src_conn = pyodbc.connect('DRIVER=' + driver + ';SERVER=' + server + '\\SQLEXPRESS' + ';DATABASE=' + database + ';UID=' + uid + ';PWD=' + pwd)\n",
    "        src_cursor = src_conn.cursor()\n",
    "        # execute query\n",
    "        src_cursor.execute(\"\"\" select  t.name as table_name\n",
    "        from sys.tables t where t.name in ('DimProduct','DimProductSubcategory','DimProductSubcategory','DimProductCategory','DimSalesTerritory','FactInternetSales') \"\"\")\n",
    "        src_tables = src_cursor.fetchall()\n",
    "        for tbl in src_tables:\n",
    "            #query and load save data to dataframe\n",
    "            df = pd.read_sql_query(f'select * FROM {tbl[0]}', src_conn)\n",
    "            load(df, tbl[0])\n",
    "    except Exception as e:\n",
    "        print(\"Data extract error: \" + str(e))\n",
    "    finally:\n",
    "        src_conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e958c1",
   "metadata": {},
   "source": [
    "## Transform\n",
    "This is the phase where I need to check for missing values and generally clean the data by exploring the type of information present and determining the scale or units of measurement to produce a snapshot which can be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data to postgres\n",
    "def load(df, tbl):\n",
    "    try:\n",
    "        rows_imported = 0\n",
    "        engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:5432/AdventureWorks')\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(df)}... for table {tbl}')\n",
    "        # save df to postgres\n",
    "        df.to_sql(f'stg_{tbl}', engine, if_exists='replace', index=False)\n",
    "        rows_imported += len(df)\n",
    "        # add elapsed time to final print out\n",
    "        print(\"Data imported successful\")\n",
    "    except Exception as e:\n",
    "        print(\"Data load error: \" + str(e))\n",
    "\n",
    "try:\n",
    "    #call extract function\n",
    "    extract()\n",
    "except Exception as e:\n",
    "    print(\"Error while extracting data: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642a4a4",
   "metadata": {},
   "source": [
    "May need to provide a list of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df96c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dc316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21520db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
