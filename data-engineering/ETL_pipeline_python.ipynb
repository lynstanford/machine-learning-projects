{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a50f1d16",
   "metadata": {},
   "source": [
    "# ETL Using Python\n",
    "\n",
    "We will be ingesting data from the Microsoft AdventureWorks Sample Databases: https://learn.microsoft.com/en-us/sql/samples/adventureworks-install-configure?view=sql-server-ver16&tabs=ssms.\n",
    "\n",
    "We can choose from either the OLTP, Data Warehouse or Lightweight '.bak' files. The Source is a database from SQL Server which is then processed using Extract-Load-Transform before reaching it's Destination as a PostgreSQL database.\n",
    "\n",
    "So, we're using SQL Server 'AdventureWorksDW2019.bak' as the database source, before loading it into PostgreSQL using Python:\n",
    "\n",
    "1. ELT is a fundamental workflow used in data engineering\n",
    "2. The data source can be an API, a db or a flatfile.\n",
    "3. The source data is extracted into a 'Staging Area', transformed into a product ready to be loaded into it's destination or target database (stored in a Data Lake or Data Warehouse).\n",
    "4. We'll use Python (Pandas library) to build the ETL pipelines.\n",
    "\n",
    "Extract: get data from a source.\n",
    "Transform: structure, format or clean the data.\n",
    "Load: write the data to an external target / destination.\n",
    "\n",
    "## Sources\n",
    "\n",
    "### Databases\n",
    "RDBMS / NoSQL\n",
    "\n",
    "### Files\n",
    "csv / json / xml\n",
    "\n",
    "### SaaS Applications\n",
    "REST API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96f6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd730c3",
   "metadata": {},
   "source": [
    "'sqlalchemy' is the module used to interact with PostgreSQL. 'pyodbc' is the module used to query SQL Server. 'pandas' is the module used to perform the data extraction / loading. 'os' is the module used to retrieve the username and passwordwhich in this case is stored separately in the 'System -> Environment Variables -> System Variables (Lower window section)'. \n",
    "\n",
    "## Authentication\n",
    "There are some important points to note here regarding authentication access to the database drivers.\n",
    "The user's credentials could be stored in a '.sh' , '.ps' or an '.xml' file.\n",
    "\n",
    "Grab the password from the environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca82131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get password from environment var\n",
    "pwd = os.environ['PGPASS']\n",
    "uid = os.environ['PGUID']\n",
    "#sql db details\n",
    "driver = \"{SQL Server Native Client 11.0}\"\n",
    "server = \"haq-PC\"\n",
    "database = \"AdventureWorksDW2019;\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423344eb",
   "metadata": {},
   "source": [
    "Define a variable to store the SQL Server driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745de89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract data from sql server\n",
    "def extract():\n",
    "    try:\n",
    "        src_conn = pyodbc.connect('DRIVER=' + driver + ';SERVER=' + server + '\\SQLEXPRESS' + ';DATABASE=' + database + ';UID=' + uid + ';PWD=' + pwd)\n",
    "        src_cursor = src_conn.cursor()\n",
    "        # execute query\n",
    "        src_cursor.execute(\"\"\" select  t.name as table_name\n",
    "        from sys.tables t where t.name in ('DimProduct','DimProductSubcategory','DimProductSubcategory','DimProductCategory','DimSalesTerritory','FactInternetSales') \"\"\")\n",
    "        src_tables = src_cursor.fetchall()\n",
    "        for tbl in src_tables:\n",
    "            #query and load save data to dataframe\n",
    "            df = pd.read_sql_query(f'select * FROM {tbl[0]}', src_conn)\n",
    "            load(df, tbl[0])\n",
    "    except Exception as e:\n",
    "        print(\"Data extract error: \" + str(e))\n",
    "    finally:\n",
    "        src_conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e958c1",
   "metadata": {},
   "source": [
    "## Transform\n",
    "This is the phase where I need to check for missing values and generally clean the data by exploring the type of information present and determining the scale or units of measurement to produce a snapshot which can be used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4f880e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data to postgres\n",
    "def load(df, tbl):\n",
    "    try:\n",
    "        rows_imported = 0\n",
    "        engine = create_engine(f'postgresql://{uid}:{pwd}@{server}:5432/AdventureWorks')\n",
    "        print(f'importing rows {rows_imported} to {rows_imported + len(df)}... for table {tbl}')\n",
    "        # save df to postgres\n",
    "        df.to_sql(f'stg_{tbl}', engine, if_exists='replace', index=False)\n",
    "        rows_imported += len(df)\n",
    "        # add elapsed time to final print out\n",
    "        print(\"Data imported successful\")\n",
    "    except Exception as e:\n",
    "        print(\"Data load error: \" + str(e))\n",
    "\n",
    "try:\n",
    "    #call extract function\n",
    "    extract()\n",
    "except Exception as e:\n",
    "    print(\"Error while extracting data: \" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642a4a4",
   "metadata": {},
   "source": [
    "May need to provide a list of dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df96c7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6dc316",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21520db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
