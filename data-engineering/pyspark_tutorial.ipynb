{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e36bf-97d6-4785-84e4-ddfb0fe2fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e02ef-d029-4069-b940-427b9a2eeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa8105-da77-4959-9e63-cd1fe96d74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b031757-3522-4490-9b07-92fa8e0328f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.read_csv('test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f9e83-c4d0-483c-af0d-7a5df734eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea983",
   "metadata": {},
   "source": [
    "This starts the spark session and enables us to run it in a single-node cluster called the 'Master' node, or host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62403c9-9ce1-4d7b-bec8-2d719b51eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974a605-8477-4cf4-ad82-6ce14634ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a2a4a-7298-4024-89d0-0b93dcdd0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf280f-1fb0-4fb5-9acb-80365454729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dabe54-3a79-4264-897d-98c57f6ac5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159cc0d-1d2c-4aaf-9ab1-e91d918ec64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option('header','true').csv('test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3c242-6577-437a-9f6b-05e5240c9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91131f54-6b4d-4193-85b8-1e85dc84bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac53d87-7e00-45e8-8566-741efcd1df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3343069-d437-4c54-8ebc-3c25f9febdef",
   "metadata": {},
   "source": [
    "This is a SQL dataframe, similar to a Pandas dataframe (if you want to convert to a Pandas dataframe simply apply the .toPandas() method). Let's check to see if we can read the first few rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527246b2-df1e-43e6-b785-cb0b7c9804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3969f7",
   "metadata": {},
   "source": [
    "The 'select()' method must be used to identify a column name to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84501d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a618cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b693c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d661",
   "metadata": {},
   "source": [
    "Selecting more than one column to reveal the row entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da19052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d97264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7448e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f107e",
   "metadata": {},
   "source": [
    "Obviously no numeric values can be used for the string 'Name' variable. The min and max values for the 'Name' variable have been determined by the index number values which happen to be lowest for Krish and highest for Sunny.\n",
    "\n",
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fee0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b26a4",
   "metadata": {},
   "source": [
    "In order for this 'withColumn' method to be reflected it must be assigned to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdceee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2262f",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.drop('Experience After 2 Years').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4105978",
   "metadata": {},
   "source": [
    "Once again, assign this method to a variable, so in order to see that the column has been dropped assign it to the df_pyspark variable once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39475b",
   "metadata": {},
   "source": [
    "## Re-naming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8377af",
   "metadata": {},
   "source": [
    "## PySpark Handling Missing Values\n",
    "1. Dropping Columns\n",
    "2. Dropping Rows\n",
    "3. Various Parameter in Dropping Functionalities\n",
    "4. Handling Missing Values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7834a1",
   "metadata": {},
   "source": [
    "To see the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbcf40",
   "metadata": {},
   "source": [
    "Save the dataset as a dataframe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58ef7f",
   "metadata": {},
   "source": [
    "This could also be achieved by applying the .toDF() method to the spark dataset, then assigning it to the 'df_pyspark' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b45992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb1ef",
   "metadata": {},
   "source": [
    "## Dropping the Columns (Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b83760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b3b44",
   "metadata": {},
   "source": [
    "To reset the dataframe simply use the show() method again. The 'Name' column will only be dropped permanently if the value of the expression is assigned. It could be assigned once again to 'df_pyspark', or to a completely different variable name such as 'df', or perhaps something more easily remembered like 'no_name' but the important point to remember is that until the expression is assigned it will not be stored in local memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c79102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeccf11",
   "metadata": {},
   "source": [
    "## Dropping Specific Rows\n",
    "\n",
    "This will drop any rows with Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe272ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ff4e2",
   "metadata": {},
   "source": [
    "### Drop Function\n",
    "\n",
    "Looking at the arguments in the drop( ) function we have: 'how', 'thresh' and 'subset'. I can view these simply by placing the cursor at the function parentheses and typing Shift-Tab. This will show a little drop down comment bubble explaining configuration options for each function argument. (Actually, it's also important to note that key-value args always follow positional args in the order).\n",
    "\n",
    "Hitting the '+' icon in the top right of the dropdown bubble expands the explanation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how argument\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d6f69",
   "metadata": {},
   "source": [
    "In this example dataset the drop() method has removed 'any' instance which contains a value of null. If we set the 'how' arg to 'all', then it would only remove an instance or row in the dataset if all values were null within that instance or row. See below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5aa499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how argument set to 'all'\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232095",
   "metadata": {},
   "source": [
    "So the how argument doesn't remove any of the instances this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh\n",
    "df_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b00797",
   "metadata": {},
   "source": [
    "The 'threshold' argument means that the row or instance will only be dropped if there are more non-null values than the threshold specified! If set to two then there must be at least 3 non-null values in the row before it's dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset\n",
    "df_pyspark.na.drop(how='any', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa154d",
   "metadata": {},
   "source": [
    "Only those null values which appear in the Experience column will have their rows dropped. This is a form of dataset slicing in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b51cb1",
   "metadata": {},
   "source": [
    "## Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6e876-46ef-4cc5-8650-76c18dae68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.fill('Missing').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db765f",
   "metadata": {},
   "source": [
    "This doesn't seem to be working! Why not? The fill method only appears to be changing the values in one column, the 'Name' column. It should be changing the null values in all columns. \n",
    "\n",
    "According to the PySpark documentation there are two main arguments to address which are the positional replacement 'value' (a string, number, or simply \" \") for the Null items in your dataset and the 'subset' key-value argument which can be set to None, or a list [], or tuple () of 'column_name' values. It can represent only one column's values, or several column values if desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff497b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.fill('Missing', subset=None).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58706cb",
   "metadata": {},
   "source": [
    "Try setting fill(value=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31508928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.fill(value=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7419d4",
   "metadata": {},
   "source": [
    "This hasn't worked either which tells me the problem lies in the different datatypes associated with each attribute or column. The 'Name' column is the only type which is a String. All the other columns are Integer, so this is preventing the operation from occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.fill(\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589df6b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f996eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8958f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f52bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e887e5a-9281-4649-af50-72e669649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
