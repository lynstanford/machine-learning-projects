{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e36bf-97d6-4785-84e4-ddfb0fe2fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e02ef-d029-4069-b940-427b9a2eeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa8105-da77-4959-9e63-cd1fe96d74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b031757-3522-4490-9b07-92fa8e0328f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.read_csv('test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f9e83-c4d0-483c-af0d-7a5df734eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea983",
   "metadata": {},
   "source": [
    "This starts the spark session and enables us to run it in a single-node cluster called the 'Master' node, or host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62403c9-9ce1-4d7b-bec8-2d719b51eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974a605-8477-4cf4-ad82-6ce14634ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a2a4a-7298-4024-89d0-0b93dcdd0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf280f-1fb0-4fb5-9acb-80365454729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dabe54-3a79-4264-897d-98c57f6ac5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159cc0d-1d2c-4aaf-9ab1-e91d918ec64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option('header','true').csv('test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3c242-6577-437a-9f6b-05e5240c9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91131f54-6b4d-4193-85b8-1e85dc84bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac53d87-7e00-45e8-8566-741efcd1df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3343069-d437-4c54-8ebc-3c25f9febdef",
   "metadata": {},
   "source": [
    "This is a SQL dataframe, similar to a Pandas dataframe (if you want to convert to a Pandas dataframe simply apply the .toPandas() method). Let's check to see if we can read the first few rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527246b2-df1e-43e6-b785-cb0b7c9804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3969f7",
   "metadata": {},
   "source": [
    "The 'select( )' method must be used to identify a column name to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84501d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a618cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b693c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d661",
   "metadata": {},
   "source": [
    "Selecting more than one column to reveal the row entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da19052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d97264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7448e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f107e",
   "metadata": {},
   "source": [
    "Obviously no numeric values can be used for the string 'Name' variable. The min and max values for the 'Name' variable have been determined by the index number values which happen to be lowest for Krish and highest for Sunny.\n",
    "\n",
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fee0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b26a4",
   "metadata": {},
   "source": [
    "In order for this 'withColumn' method to be reflected it must be assigned to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdceee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2262f",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.drop('Experience After 2 Years').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4105978",
   "metadata": {},
   "source": [
    "Once again, assign this method to a variable, so in order to see that the column has been dropped assign it to the df_pyspark variable once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39475b",
   "metadata": {},
   "source": [
    "## Re-naming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8377af",
   "metadata": {},
   "source": [
    "## PySpark Handling Missing Values\n",
    "1. Dropping Columns\n",
    "2. Dropping Rows\n",
    "3. Various Parameter in Dropping Functionalities\n",
    "4. Handling Missing Values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7834a1",
   "metadata": {},
   "source": [
    "To see the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f5672",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbcf40",
   "metadata": {},
   "source": [
    "Save the dataset as a dataframe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58ef7f",
   "metadata": {},
   "source": [
    "This could also be achieved by applying the .toDF() method to the spark dataset, then assigning it to the 'df_pyspark' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b45992",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593fb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f620ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb1ef",
   "metadata": {},
   "source": [
    "## Dropping the Columns (Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b83760",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b3b44",
   "metadata": {},
   "source": [
    "To reset the dataframe simply use the show( ) method again. The 'Name' column will only be dropped permanently if the value of the expression is assigned. It could be assigned once again to 'df_pyspark', or to a completely different variable name such as 'df', or perhaps something more easily remembered like 'no_name' but the important point to remember is that until the expression is assigned it will not be stored in local memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c79102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeccf11",
   "metadata": {},
   "source": [
    "## Dropping Specific Rows\n",
    "\n",
    "This will drop any rows with Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe272ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ff4e2",
   "metadata": {},
   "source": [
    "### Drop Function\n",
    "\n",
    "Looking at the arguments in the drop( ) function we have: 'how', 'thresh' and 'subset'. I can view these simply by placing the cursor at the function parentheses and typing Shift-Tab. This will show a little drop down comment bubble explaining configuration options for each function argument. (Actually, it's also important to note that key-value args always follow positional args in the order).\n",
    "\n",
    "Hitting the '+' icon in the top right of the dropdown bubble expands the explanation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1c4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how argument\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d6f69",
   "metadata": {},
   "source": [
    "In this example dataset the drop( ) method has removed 'any' instance which contains a value of null. If we set the 'how' arg to 'all', then it would only remove an instance or row in the dataset if all values were null within that instance or row. See below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5aa499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how argument set to 'all'\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232095",
   "metadata": {},
   "source": [
    "So the how argument doesn't remove any of the instances this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78df6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh\n",
    "df_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b00797",
   "metadata": {},
   "source": [
    "The 'threshold' argument means that the row or instance will only be dropped if there are more non-null values than the threshold specified! If set to two then there must be at least 3 non-null values in the row before it's dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cbc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset\n",
    "df_pyspark.na.drop(how='any', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa154d",
   "metadata": {},
   "source": [
    "Only those null values which appear in the Experience column will have their rows dropped. This is a form of dataset slicing in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b51cb1",
   "metadata": {},
   "source": [
    "## Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6e876-46ef-4cc5-8650-76c18dae68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db765f",
   "metadata": {},
   "source": [
    "This doesn't seem to be working! Why not? The fill method only appears to be changing the values in one column, the 'Name' column. It should be changing the null values in all columns. \n",
    "\n",
    "According to the PySpark documentation there are two main arguments to address which are the positional replacement 'value' (a string, number, or simply \" \") for the Null items in your dataset and the 'subset' key-value argument which can be set to None, or a list [], or tuple () of 'column_name' values. It can represent only one column's values, or several column values if desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff497b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.fill(value='Missing', subset=None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.fill(value='Missing', subset=['Name','Age','Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58706cb",
   "metadata": {},
   "source": [
    "Try setting fill(value=0) for all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31508928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.fill(value=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7419d4",
   "metadata": {},
   "source": [
    "This hasn't worked either which tells me the problem lies in the different datatypes associated with each attribute or column. The 'Name' column is the only type which is a String. All the other columns are Integer, so this is preventing the operation from occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef37fe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.na.fill(\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589df6b",
   "metadata": {},
   "source": [
    "Storing the na.fill( ) method in a variable doesn't seem to work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e2ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file using PySpark\n",
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Cast integer columns to string data type\n",
    "df_pyspark = df_pyspark.withColumn('Age', df_pyspark['Age'].cast('string'))\n",
    "df_pyspark = df_pyspark.withColumn('Experience', df_pyspark['Experience'].cast('string'))\n",
    "df_pyspark = df_pyspark.withColumn('Salary', df_pyspark['Salary'].cast('string'))\n",
    "\n",
    "# Replace all null values with 'Missing Value'\n",
    "df_pyspark = df_pyspark.fillna('Missing Value')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.na.fill('Missing Value', ['Experience','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b1839",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793b3a",
   "metadata": {},
   "source": [
    "Replace the Null values with the mean values for a particular column or for each column. This involves imputing the mean values. In this example I will replace the Null values in the 'Experience' column only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80476513-7ef9-4824-88eb-d637d52b564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast string columns to integer data type\n",
    "df_pyspark = df_pyspark.withColumn('Age', df_pyspark['Age'].cast('integer'))\n",
    "df_pyspark = df_pyspark.withColumn('Experience', df_pyspark['Experience'].cast('integer'))\n",
    "df_pyspark = df_pyspark.withColumn('Salary', df_pyspark['Salary'].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e99668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b547669-8e23-4511-9879-6d1f297ad0ee",
   "metadata": {},
   "source": [
    "The same can be done with median values also. Note, the imputed values are created in entirely new columns with the average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673025d-a4d7-4a14-bca8-7ccfa8b193f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]).setStrategy(\"median\")\n",
    "\n",
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dba97b-aa88-4b89-a346-f983efa35f3a",
   "metadata": {},
   "source": [
    "## PySpark DataFrames\n",
    "1. Filter Operations\n",
    "2. &, |, == (And, Or, Equals)\n",
    "3. ~ (Not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4873d9c-46c3-4ab9-98ae-b40c6e7a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912678e-bb2b-48bf-9099-8ca5ae4d7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c835e1b1-1b9a-497f-881d-71f8241adb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94017cfe-b29c-46a9-a59b-f7523af84ed6",
   "metadata": {},
   "source": [
    "There are a number of filters which can be applied using Pandas dataframes also. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2d0f7-d731-4f37-8045-a1946092aa9b",
   "metadata": {},
   "source": [
    "## Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32e7b1-763b-4260-9579-1d9638dbafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# salary of people less than or equal to 20000\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ca753-df3a-4366-ac1f-b80580bf214f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e413d61-a50c-46c2-be79-8eb0fff16b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<=20000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf83bbf-f4ef-4680-87d1-0dc5af9bc924",
   "metadata": {},
   "source": [
    "### The AND operand\n",
    "An important point to note here is that the comparison statements which are being evaluated, must be contained within parentheses (brackets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8356b4-6482-4a16-8a70-226d5891dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) & (df_pyspark['Salary']>=18000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a962c-719c-4085-881f-37afa9e4ab25",
   "metadata": {},
   "source": [
    "### The OR operand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c799cd-9aa0-4901-b400-3ab45a33358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter((df_pyspark['Salary']<=20000) | (df_pyspark['Salary']>=15000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688cde5-2521-4835-b655-a05f95c84f76",
   "metadata": {},
   "source": [
    "### The NOT operand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6a3f5-6470-4d22-bad8-7444845dd0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.filter(~(df_pyspark['Salary']<=20000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbca948-bc14-46ef-802d-22ae1dbbadb6",
   "metadata": {},
   "source": [
    "## PySpark GroupBy and Aggregate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f675d99-08de-4cb2-a64d-11df6eae3b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec3bf3-e82b-4afd-94d6-06f5b6c2f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Agg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9cf4d-c45a-4c42-a6ee-14228d2d01af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf46793-adf9-4a48-b8ba-8184b14352a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test3.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05ee034-0370-48b7-a807-bb40123b8260",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a5f3d-56ef-4f5c-864c-91e889daef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2db610-9e39-42eb-8d4f-ca11a328b015",
   "metadata": {},
   "source": [
    "So 'Name' is a string, 'Departments' is a string and 'Salary' is an integer data type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44918445-c7da-4346-82d7-4a6ee3ea5a93",
   "metadata": {},
   "source": [
    "### Groupby\n",
    "#### Grouped to Find the Maximum Salary\n",
    "Finding the mean salary group by Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c1a2f0-69b8-462a-921e-9e62691c7b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b350e-73be-48d7-8146-521b1d0a66d6",
   "metadata": {},
   "source": [
    "GroupBy and Aggregate functions work together, so once groupBy( ) is applied, then an agg( ) function can follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f823903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f7e82-c55f-42fd-9b84-3ae0dde65210",
   "metadata": {},
   "source": [
    "This time a SQL dataframe is returned.\n",
    "\n",
    "Let's show this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad1a833-f5c5-443d-bdf6-c0edb405a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e5fd8-39b1-4077-bdb9-ef109cd4e14d",
   "metadata": {},
   "source": [
    "Press dot or period and tab...this will provide a whole list of possible functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307228ca-6bb1-4303-8412-57c6f962c19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Departments').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9f84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c6b50-b133-4b0d-b651-3918a5108fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bf677-8442-4990-b550-c4a2cba8ba48",
   "metadata": {},
   "source": [
    "Try applying a direct aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee5ac69-bf39-4930-9b23-23ecea6e9c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441918d5-d07f-40d4-8cd5-cd535aa9d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87e7140-5473-485a-b27f-7304119b8e4b",
   "metadata": {},
   "source": [
    "And also, to see the minimum value with respect to Name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3766f42-8e0a-4251-beeb-dce654eba75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').min().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98ef49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.groupBy('Name').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7608f7d5-7974-4bd4-b380-b71fd5283e4a",
   "metadata": {},
   "source": [
    "## Machine Learning with DataFrame API and RDD API's\n",
    "### Examples of PySpark ML\n",
    "Using a Linear Regression example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ee953-a115-4bf6-9d9a-486d4c7e05d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Missing').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77168768-74d1-4150-92b0-73246f8c8817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the dataset\n",
    "training = spark.read.csv('test1.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc8e9ac-c1c6-42d2-b954-b107eef9ad73",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e4efb-ba81-47ae-81dd-42175fe886db",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e7b98-5ff6-455a-878c-942c94e7b8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42461d-eb30-431a-b1d2-fb32efae5935",
   "metadata": {},
   "source": [
    "In sklearn we have a dataset which we would perform a train-test split on, providing us with independent and dependent variables. In PySpark it's slightly different.\n",
    "\n",
    "['Age','Experience'] ----> New Feature ----> Independent Feature\n",
    "\n",
    "Grouping the independent variable features together is performed using the 'VectorAssembler' module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78959fd2-b04a-4b28-8495-e65ae068bd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=[\"Age\", \"Experience\"], outputCol=\"Independent Features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504bf1a5-117d-47eb-afb7-1c417b9b2064",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureassembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f33261d-b42a-44f5-897b-3c2a90f00da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771528cb",
   "metadata": {},
   "source": [
    "Note the 'Independent Features' column has 2 values in each row listing Age and Experience. This is a type of feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b3e999",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d749136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"Independent Features\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90ef215",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99890163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# train test split\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25])\n",
    "regressor = LinearRegression(featuresCol=\"Independent Features\", labelCol=\"Salary\")\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0346d4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients\n",
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fb78cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intercepts\n",
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae82c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3e375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results.meanAbsoluteError, pred_results.meanSquaredError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98dc6d69",
   "metadata": {},
   "source": [
    "## Multi-Linear Regression Problem\n",
    "Follow these steps in Databricks:\n",
    "1. Create a cluster in the 'Clusters' option, or the 'Compute' option in the left-hand menu. Create a name for the cluster, select databricks runtime version (perhaps the latest), driver type or availability zone if required, then click 'Create Cluster'.\n",
    "2. You can choose 'Libraries' from the tabs at the top, select 'Install' and choose from a variety of libraries like PyPi (Python) or Maven (Java).\n",
    "3. Once the cluster is up and running go to the Home icon, top left.\n",
    "4. Create a blank notebook and call it Linear Regression for this example.\n",
    "5. There are several features included in the independent values. In Databricks, you can start on the Databricks icon top left then click on 'Import & Explore Data' in the middle. Or alternatively, go to the 'Data' icon on the left menu and select the 'tips.csv' file and upload it. You'll be presented with a choice of 'Create Table with UI' or 'Create Table in Notebook'. The file will be uploaded into the 'DBFS' file system which is one of the tabs at the top. The file path will look something like: '/FileStore/tables/tips.csv'.\n",
    "6. The notebook should appear as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ebaee7",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. DBFS is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n",
    "\n",
    "This notebook is written in Python so the default cell type is Python. However, you can use different languages by using the %LANGUAGE syntax. Python, Scala, SQL, and R are all supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d1f42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "file_location = \"/FileStore/tables/tips.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.csv(file_location, header=True, inferSchema=True)\n",
    "df.show()\n",
    "\n",
    "# Or alternatively\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd959a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a048c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view or table\n",
    "\n",
    "temp_table_name = \"tips_csv\"\n",
    "\n",
    "df.createOrReplaceTempView(temp_table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3918495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbc8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e974cfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb25b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e887e5a-9281-4649-af50-72e669649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334b0bf-c0b6-4127-b62a-98b435cb5183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
