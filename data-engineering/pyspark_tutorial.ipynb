{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "042e36bf-97d6-4785-84e4-ddfb0fe2fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2e02ef-d029-4069-b940-427b9a2eeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10fa8105-da77-4959-9e63-cd1fe96d74d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Krish</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sudhansh</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Name  Age  Experience\n",
       "0     Krish   31          10\n",
       "1  Sudhansh   30           8\n",
       "2     Sunny   29           4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b031757-3522-4490-9b07-92fa8e0328f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pd.read_csv('test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f10f9e83-c4d0-483c-af0d-7a5df734eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea983",
   "metadata": {},
   "source": [
    "This starts the spark session and enables us to run it in a single-node cluster called the 'Master' node, or host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62403c9-9ce1-4d7b-bec8-2d719b51eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7974a605-8477-4cf4-ad82-6ce14634ea77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1a9c7cfa260>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "123a2a4a-7298-4024-89d0-0b93dcdd0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92bf280f-1fb0-4fb5-9acb-80365454729d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|     _c0|_c1|       _c2|\n",
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2dabe54-3a79-4264-897d-98c57f6ac5f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3159cc0d-1d2c-4aaf-9ab1-e91d918ec64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da3c242-6577-437a-9f6b-05e5240c9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91131f54-6b4d-4193-85b8-1e85dc84bd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ac53d87-7e00-45e8-8566-741efcd1df13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3343069-d437-4c54-8ebc-3c25f9febdef",
   "metadata": {},
   "source": [
    "This is a SQL dataframe, similar to a Pandas dataframe (if you want to convert to a Pandas dataframe simply apply the .toPandas() method). Let's check to see if we can read the first few rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "527246b2-df1e-43e6-b785-cb0b7c9804d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Krish', Age='31', Experience='10'),\n",
       " Row(Name='Sudhansh', Age='30', Experience='8'),\n",
       " Row(Name='Sunny', Age='29', Experience='4')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c072adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3969f7",
   "metadata": {},
   "source": [
    "The 'select( )' method must be used to identify a column name to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e84501d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a618cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    Name|\n",
      "+--------+\n",
      "|   Krish|\n",
      "|Sudhansh|\n",
      "|   Sunny|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b693c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d661",
   "metadata": {},
   "source": [
    "Selecting more than one column to reveal the row entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "becdc1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|    Name|Experience|\n",
      "+--------+----------+\n",
      "|   Krish|        10|\n",
      "|Sudhansh|         8|\n",
      "|   Sunny|         4|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da19052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9e0a68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'string'), ('Experience', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d97264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Age: string, Experience: string]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7448e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+-----------------+\n",
      "|summary| Name| Age|       Experience|\n",
      "+-------+-----+----+-----------------+\n",
      "|  count|    3|   3|                3|\n",
      "|   mean| null|30.0|7.333333333333333|\n",
      "| stddev| null| 1.0|3.055050463303893|\n",
      "|    min|Krish|  29|               10|\n",
      "|    max|Sunny|  31|                8|\n",
      "+-------+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f107e",
   "metadata": {},
   "source": [
    "Obviously no numeric values can be used for the string 'Name' variable. The min and max values for the 'Name' variable have been determined by the index number values which happen to be lowest for Krish and highest for Sunny.\n",
    "\n",
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9fee0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string, Experience After 2 Years: double]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b26a4",
   "metadata": {},
   "source": [
    "In order for this 'withColumn' method to be reflected it must be assigned to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcc3e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6efbd7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string, Experience After 2 Years: double]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bdceee0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------------------------+\n",
      "|    Name|Age|Experience|Experience After 2 Years|\n",
      "+--------+---+----------+------------------------+\n",
      "|   Krish| 31|        10|                    12.0|\n",
      "|Sudhansh| 30|         8|                    10.0|\n",
      "|   Sunny| 29|         4|                     6.0|\n",
      "+--------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2262f",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e6efa087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Experience After 2 Years').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4105978",
   "metadata": {},
   "source": [
    "Once again, assign this method to a variable, so in order to see that the column has been dropped assign it to the df_pyspark variable once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5e2b142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39475b",
   "metadata": {},
   "source": [
    "## Re-naming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e8dbec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8377af",
   "metadata": {},
   "source": [
    "## PySpark Handling Missing Values\n",
    "1. Dropping Columns\n",
    "2. Dropping Rows\n",
    "3. Various Parameter in Dropping Functionalities\n",
    "4. Handling Missing Values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "546b5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "faf030bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7834a1",
   "metadata": {},
   "source": [
    "To see the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b34f5672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbcf40",
   "metadata": {},
   "source": [
    "Save the dataset as a dataframe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c77e73da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58ef7f",
   "metadata": {},
   "source": [
    "This could also be achieved by applying the .toDF() method to the spark dataset, then assigning it to the 'df_pyspark' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5b45992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f593fb4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f620ef42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7eb1ef",
   "metadata": {},
   "source": [
    "## Dropping the Columns (Again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22b83760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  31|        10| 30000|\n",
      "|  30|         8| 25000|\n",
      "|  29|         4| 20000|\n",
      "|  24|         3| 20000|\n",
      "|  21|         1| 15000|\n",
      "|  23|         2| 18000|\n",
      "|null|      null| 40000|\n",
      "|  34|        10| 38000|\n",
      "|  36|      null|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39b3b44",
   "metadata": {},
   "source": [
    "To reset the dataframe simply use the show( ) method again. The 'Name' column will only be dropped permanently if the value of the expression is assigned. It could be assigned once again to 'df_pyspark', or to a completely different variable name such as 'df', or perhaps something more easily remembered like 'no_name' but the important point to remember is that until the expression is assigned it will not be stored in local memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0c79102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeccf11",
   "metadata": {},
   "source": [
    "## Dropping Specific Rows\n",
    "\n",
    "This will drop any rows with Null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe272ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ff4e2",
   "metadata": {},
   "source": [
    "### Drop Function\n",
    "\n",
    "Looking at the arguments in the drop( ) function we have: 'how', 'thresh' and 'subset'. I can view these simply by placing the cursor at the function parentheses and typing Shift-Tab. This will show a little drop down comment bubble explaining configuration options for each function argument. (Actually, it's also important to note that key-value args always follow positional args in the order).\n",
    "\n",
    "Hitting the '+' icon in the top right of the dropdown bubble expands the explanation options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e1c4202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how argument\n",
    "df_pyspark.na.drop(how='any').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7d6f69",
   "metadata": {},
   "source": [
    "In this example dataset the drop( ) method has removed 'any' instance which contains a value of null. If we set the 'how' arg to 'all', then it would only remove an instance or row in the dataset if all values were null within that instance or row. See below..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dd5aa499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# how argument set to 'all'\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c232095",
   "metadata": {},
   "source": [
    "So the how argument doesn't remove any of the instances this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78df6ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh\n",
    "df_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b00797",
   "metadata": {},
   "source": [
    "The 'threshold' argument means that the row or instance will only be dropped if there are more non-null values than the threshold specified! If set to two then there must be at least 3 non-null values in the row before it's dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03cbc80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "|    null| 34|        10| 38000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset\n",
    "df_pyspark.na.drop(how='any', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa154d",
   "metadata": {},
   "source": [
    "Only those null values which appear in the Experience column will have their rows dropped. This is a form of dataset slicing in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f902bb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|    null|  34|        10| 38000|\n",
      "|    null|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b51cb1",
   "metadata": {},
   "source": [
    "## Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ec6e876-46ef-4cc5-8650-76c18dae68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+------+\n",
      "|          Name| Age|Experience|Salary|\n",
      "+--------------+----+----------+------+\n",
      "|         Krish|  31|        10| 30000|\n",
      "|      Sudhansh|  30|         8| 25000|\n",
      "|         Sunny|  29|         4| 20000|\n",
      "|          Paul|  24|         3| 20000|\n",
      "|        Harsha|  21|         1| 15000|\n",
      "|       Shubham|  23|         2| 18000|\n",
      "|        Mahesh|null|      null| 40000|\n",
      "|Missing Values|  34|        10| 38000|\n",
      "|Missing Values|  36|      null|  null|\n",
      "+--------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Values').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db765f",
   "metadata": {},
   "source": [
    "This doesn't seem to be working! Why not? The fill method only appears to be changing the values in one column, the 'Name' column. It should be changing the null values in all columns. \n",
    "\n",
    "According to the PySpark documentation there are two main arguments to address which are the positional replacement 'value' (a string, number, or simply \" \") for the Null items in your dataset and the 'subset' key-value argument which can be set to None, or a list [], or tuple () of 'column_name' values. It can represent only one column's values, or several column values if desired. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bff497b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "| Missing|  34|        10| 38000|\n",
      "| Missing|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value='Missing', subset=None).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b44f7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "| Missing|  34|        10| 38000|\n",
      "| Missing|  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value='Missing', subset=['Name','Age','Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58706cb",
   "metadata": {},
   "source": [
    "Try setting fill(value=0) for all null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31508928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Krish| 31|        10| 30000|\n",
      "|Sudhansh| 30|         8| 25000|\n",
      "|   Sunny| 29|         4| 20000|\n",
      "|    Paul| 24|         3| 20000|\n",
      "|  Harsha| 21|         1| 15000|\n",
      "| Shubham| 23|         2| 18000|\n",
      "|  Mahesh|  0|         0| 40000|\n",
      "|    null| 34|        10| 38000|\n",
      "|    null| 36|         0|     0|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(value=0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7419d4",
   "metadata": {},
   "source": [
    "This hasn't worked either which tells me the problem lies in the different datatypes associated with each attribute or column. The 'Name' column is the only type which is a String. All the other columns are Integer, so this is preventing the operation from occurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef37fe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|    Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|   Krish|  31|        10| 30000|\n",
      "|Sudhansh|  30|         8| 25000|\n",
      "|   Sunny|  29|         4| 20000|\n",
      "|    Paul|  24|         3| 20000|\n",
      "|  Harsha|  21|         1| 15000|\n",
      "| Shubham|  23|         2| 18000|\n",
      "|  Mahesh|null|      null| 40000|\n",
      "|        |  34|        10| 38000|\n",
      "|        |  36|      null|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.na.fill(\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4589df6b",
   "metadata": {},
   "source": [
    "Storing the na.fill( ) method in a variable doesn't seem to work either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2e2ec03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Krish|           31|           10|        30000|\n",
      "|     Sudhansh|           30|            8|        25000|\n",
      "|        Sunny|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|       Harsha|           21|            1|        15000|\n",
      "|      Shubham|           23|            2|        18000|\n",
      "|       Mahesh|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file using PySpark\n",
    "df_pyspark = spark.read.csv('test2.csv', header=True, inferSchema=True)\n",
    "\n",
    "# Cast integer columns to string data type\n",
    "df_pyspark = df_pyspark.withColumn('Age', df_pyspark['Age'].cast('string'))\n",
    "df_pyspark = df_pyspark.withColumn('Experience', df_pyspark['Experience'].cast('string'))\n",
    "df_pyspark = df_pyspark.withColumn('Salary', df_pyspark['Salary'].cast('string'))\n",
    "\n",
    "# Replace all null values with 'Missing Value'\n",
    "df_pyspark = df_pyspark.fillna('Missing Value')\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ac9525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Krish|           31|           10|        30000|\n",
      "|     Sudhansh|           30|            8|        25000|\n",
      "|        Sunny|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|       Harsha|           21|            1|        15000|\n",
      "|      Shubham|           23|            2|        18000|\n",
      "|       Mahesh|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill('Missing Value', ['Experience','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bd8b1839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-------------+-------------+\n",
      "|         Name|          Age|   Experience|       Salary|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "|        Krish|           31|           10|        30000|\n",
      "|     Sudhansh|           30|            8|        25000|\n",
      "|        Sunny|           29|            4|        20000|\n",
      "|         Paul|           24|            3|        20000|\n",
      "|       Harsha|           21|            1|        15000|\n",
      "|      Shubham|           23|            2|        18000|\n",
      "|       Mahesh|Missing Value|Missing Value|        40000|\n",
      "|Missing Value|           34|           10|        38000|\n",
      "|Missing Value|           36|Missing Value|Missing Value|\n",
      "+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb793b3a",
   "metadata": {},
   "source": [
    "Replace the Null values with the mean values for a particular column or for each column. This involves imputing the mean values. In this example I will replace the Null values in the 'Experience' column only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80476513-7ef9-4824-88eb-d637d52b564a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast string columns to integer data type\n",
    "df_pyspark = df_pyspark.withColumn('Age', df_pyspark['Age'].cast('integer'))\n",
    "df_pyspark = df_pyspark.withColumn('Experience', df_pyspark['Experience'].cast('integer'))\n",
    "df_pyspark = df_pyspark.withColumn('Salary', df_pyspark['Salary'].cast('integer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "576bf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "730d629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9e99668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|         Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|        Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|     Sudhansh|  30|         8| 25000|         30|                 8|         25000|\n",
      "|        Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|         Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|       Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|      Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|       Mahesh|null|      null| 40000|         28|                 5|         40000|\n",
      "|Missing Value|  34|        10| 38000|         34|                10|         38000|\n",
      "|Missing Value|  36|      null|  null|         36|                 5|         25750|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b547669-8e23-4511-9879-6d1f297ad0ee",
   "metadata": {},
   "source": [
    "The same can be done with median values also. Note, the imputed values are created in entirely new columns with the average values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f673025d-a4d7-4a14-bca8-7ccfa8b193f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|         Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "|        Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|     Sudhansh|  30|         8| 25000|         30|                 8|         25000|\n",
      "|        Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|         Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|       Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|      Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|       Mahesh|null|      null| 40000|         29|                 4|         40000|\n",
      "|Missing Value|  34|        10| 38000|         34|                10|         38000|\n",
      "|Missing Value|  36|      null|  null|         36|                 4|         20000|\n",
      "+-------------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer = Imputer(\n",
    "    inputCols=['Age','Experience','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Experience','Salary']]).setStrategy(\"median\")\n",
    "\n",
    "# Add imputation cols to df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dba97b-aa88-4b89-a346-f983efa35f3a",
   "metadata": {},
   "source": [
    "## PySpark DataFrames - Using Filter Operations\n",
    "2. &, |, ==\n",
    "3. ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4873d9c-46c3-4ab9-98ae-b40c6e7a4bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0912678e-bb2b-48bf-9099-8ca5ae4d7c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c835e1b1-1b9a-497f-881d-71f8241adb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|    Name|Age|Experience|\n",
      "+--------+---+----------+\n",
      "|   Krish| 31|        10|\n",
      "|Sudhansh| 30|         8|\n",
      "|   Sunny| 29|         4|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94017cfe-b29c-46a9-a59b-f7523af84ed6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2938c80-7cf9-492a-a49d-ae4ec2e967c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32e7b1-763b-4260-9579-1d9638dbafe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95ca753-df3a-4366-ac1f-b80580bf214f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e413d61-a50c-46c2-be79-8eb0fff16b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8356b4-6482-4a16-8a70-226d5891dcbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c799cd-9aa0-4901-b400-3ab45a33358b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe6a3f5-6470-4d22-bad8-7444845dd0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df118769-f1cb-4c4a-8731-74c28400891d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f675d99-08de-4cb2-a64d-11df6eae3b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec3bf3-e82b-4afd-94d6-06f5b6c2f2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9cf4d-c45a-4c42-a6ee-14228d2d01af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f823903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e887e5a-9281-4649-af50-72e669649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
