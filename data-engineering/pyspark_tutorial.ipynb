{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e36bf-97d6-4785-84e4-ddfb0fe2fe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e02ef-d029-4069-b940-427b9a2eeff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa8105-da77-4959-9e63-cd1fe96d74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b031757-3522-4490-9b07-92fa8e0328f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(pd.read_csv('test1.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10f9e83-c4d0-483c-af0d-7a5df734eaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dea983",
   "metadata": {},
   "source": [
    "This starts the spark session and enables us to run it in a single-node cluster called the 'Master' node, or host."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62403c9-9ce1-4d7b-bec8-2d719b51eb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7974a605-8477-4cf4-ad82-6ce14634ea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a2a4a-7298-4024-89d0-0b93dcdd0963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bf280f-1fb0-4fb5-9acb-80365454729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2dabe54-3a79-4264-897d-98c57f6ac5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159cc0d-1d2c-4aaf-9ab1-e91d918ec64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.option('header','true').csv('test1.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3c242-6577-437a-9f6b-05e5240c9afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91131f54-6b4d-4193-85b8-1e85dc84bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac53d87-7e00-45e8-8566-741efcd1df13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3343069-d437-4c54-8ebc-3c25f9febdef",
   "metadata": {},
   "source": [
    "This is a SQL dataframe, similar to a Pandas dataframe. Let's check to see if we can read the first few rows?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527246b2-df1e-43e6-b785-cb0b7c9804d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072adfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3969f7",
   "metadata": {},
   "source": [
    "The 'select()' function must be used to identify a column name to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84501d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a618cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b693c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de29d661",
   "metadata": {},
   "source": [
    "Selecting more than one column to reveal the row entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becdc1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da19052",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0a68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d97264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7448e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5f107e",
   "metadata": {},
   "source": [
    "Obviously no numeric values can be used for the string 'Name' variable. The min and max values for the 'Name' variable have been determined by the index number values which happen to be lowest for Krish and highest for Sunny.\n",
    "\n",
    "## Adding a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fee0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b26a4",
   "metadata": {},
   "source": [
    "In order for this 'withColumn' method to be reflected it must be assigned to a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc3e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Experience After 2 Years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6efbd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdceee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f2262f",
   "metadata": {},
   "source": [
    "## Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efa087",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.drop('Experience After 2 Years').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4105978",
   "metadata": {},
   "source": [
    "Once again, assign this method to a variable, so in order to see that the column has been dropped assign it to the df_pyspark variable once again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e2b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 Years')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd39475b",
   "metadata": {},
   "source": [
    "## Re-naming a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8dbec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e887e5a-9281-4649-af50-72e669649e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8377af",
   "metadata": {},
   "source": [
    "## PySpark Handling Missing Values\n",
    "1. Dropping Columns\n",
    "2. Dropping Rows\n",
    "3. Various Parameter in Dropping Functionalities\n",
    "4. Handling Missing Values by Mean, Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546b5670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf030bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv('test2.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7a3108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34f5672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c34216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec6e876-46ef-4cc5-8650-76c18dae68f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
