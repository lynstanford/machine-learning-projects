{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression\n",
    "So the relationship between the dependent and independent variables appear to be strongly linear in relationship although the connection between the 'Volume' of daily bitcoin bought and sold has less association with the daily 'Close' price. Fitting a Linear Regression line to the data may be accurate in this case, with an R2 value of 0.9991392014437468 and RMSE of 689.1925598643533. However, out of curiosity I decided to see if a Polynomial function could fit the line slightly better to capture some of the variance in this seemingly linear relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import data\n",
    "bitcoin = pd.read_csv(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "df = pd.DataFrame(bitcoin).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Adj Close    float64\n",
       "Volume       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Selection\n",
    "Assign the (dependant) y variable and (independent) X variables for the modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for modeling\n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
    "y = df[\"Close\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "Using this data for the Polynomial Features model and splitting it into training and test sets with a 70-30 split. Make a copy of the dataframe first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "The following method is one way for training and testing the polynomial function, but first import the relevant libraries and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# initialize the model for a given degree\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False)\n",
    "  \n",
    "# transforms the existing features to higher degree features.\n",
    "X_train_poly = poly_features.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using indexation to return any value in X, say the 1st value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9718.07</td>\n",
       "      <td>9838.33</td>\n",
       "      <td>9728.25</td>\n",
       "      <td>4.624843e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Open     High      Low        Volume\n",
       "0  9718.07  9838.33  9728.25  4.624843e+10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeating for the data contained in the 'X_train_poly' set and we can see that there are 14 values returned so it has created an array with 10 new features for a total of 14 features. This includes element-wise dot product values and some squared values (without going into too much detail). Both the original feature values for 'X1' to 'Xn' and the feature squared value from 'X_train_poly' are returned in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.44019200e+04, 1.45237600e+04, 1.42865400e+04, 3.47848792e+10,\n",
       "        2.07415300e+08, 2.09170030e+08, 2.05753606e+08, 5.00969048e+14,\n",
       "        2.10939605e+08, 2.07494278e+08, 5.05207238e+14, 2.04105225e+08,\n",
       "        4.96955569e+14, 1.20998782e+21]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_poly[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the current dataframe to a CSV formatted file (or an Excel file) for preservation to view the degree of newly expanded features in a fresh table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/X_train_poly.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first set of metrics show more variance for the training set but a higher degree of fit with the polynomial features model than the test set.\n",
    "\n",
    "Also, the bias term (intercept) and coefficients are both attributes of the LinearRegression() model so I can examine these from the independent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.04746753e-11,  1.34619966e-08,  7.63252662e-11,  3.95073614e-08,\n",
       "       -6.83473603e-06, -5.82358084e-07, -8.40449336e-07, -1.49894184e-12,\n",
       "        5.83557909e-06,  5.55122986e-06,  2.54559597e-12,  5.15929020e-06,\n",
       "        2.50892933e-12, -4.61371567e-19])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly_model.intercept_\n",
    "poly_model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Regularization\n",
    "To avoid overfitting regularization can be applied, but will require tuning hyperparameters manually to an extent. To see if I can improve on the linear regression model's ability to predict the target variable, I have decided to use Ridge Regression.\n",
    "The regularization term basically employs the use of a penalization method by summing the squared values of each coefficient (whether positive or negative) and helps reduce the parameter weights overall but this term is only used during the training phase so I need to remove it for the purpose of testing and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# import data\n",
    "bitcoin = pd.read_csv(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "df = pd.DataFrame(bitcoin).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the first 2 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the 'Close' prices to the dependent (target) variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"Close\"]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features \n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Data\n",
    "Using a 70-30 split for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the shape of the training sets gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the shape of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "### Ridge Regression\n",
    "Trying a slightly different type of regression model using the alpha learning rate hyperparameter of 1.0 to see if I can reduce the (rmse) error value and increase the (r2) accuracy score. The purpose of using a ridge regression model is to try to reduce or eliminate the coefficient values of all the various features (especially those with high multi-colinearity between predictors) and will increase bias slightly but should decrease variance significantly. Ridge regression achieves this by assigning equal weights to those coefficient values which have high colinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "ridge_regression = linear_model.Ridge(alpha=1.0)\n",
    "\n",
    "# fit model\n",
    "ridge_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization term should only be added to the cost function during the training phase. Now the 'training' data has been fit, it becomes important to discover the performance measure on the unregularized test set. Making a prediction on the first 5 values in the test set first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_predictions = ridge_regression.predict(X_test)\n",
    "print(\"Predictions: \", ridge_regression.predict(X_test.iloc[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try a prediction by imputing my own values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting price based on Open = C$35,000, High = C$40,000, Low = C$32,000 and Volume = 100bn\n",
    "ridge_regression.predict([[35000, 40000, 32000, 100000000000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells me the prediction for the target output variable, y, based on the input variables specified and using ridge regression giving the value of C$37,990.99.\n",
    "\n",
    "## Model Validation MetricsÂ¶\n",
    "Now to measure the error score and accuracy of the line of fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new data matrix containing the additional features with the squared values has been created by expanding the number of features and the parameter weights (or coefficients) which represent a quadratic equation. The linear regression model should now be applied again to this newly expanded dataframe containing the polynomial features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model performance for the training set\n",
      "-------------------------------------------\n",
      "RMSE of training set is 3087.9523217240453\n",
      "R2 score of training set is 0.9809084291955809\n",
      "\n",
      "\n",
      "The model performance for the test set\n",
      "-------------------------------------------\n",
      "RMSE of test set is 2941.2787067558293\n",
      "R2 score of test set is 0.9789579261471753\n"
     ]
    }
   ],
   "source": [
    "# fit the transformed features to Linear Regression\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y_train)\n",
    "  \n",
    "# predicting on training data-set\n",
    "y_train_pred = poly_model.predict(X_train_poly)\n",
    "  \n",
    "# predicting on test data-set\n",
    "y_test_pred = poly_model.predict(poly_features.fit_transform(X_test))\n",
    "  \n",
    "# evaluating the model on training dataset\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "  \n",
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "  \n",
    "print(\"The model performance for the training set\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))\n",
    "  \n",
    "print(\"\\n\")\n",
    "  \n",
    "print(\"The model performance for the test set\")\n",
    "print(\"-------------------------------------------\")\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-squared: \", ridge_regression.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing a scatter plot of the data points to display actual vs predicted prices and the regression line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, price_predictions)\n",
    "print(mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n",
    "\n",
    "# set the width and height of the plot\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# visualizing the relationship between actual and predicted values for y\n",
    "plt.scatter(y_test, price_predictions)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual Prices vs Predicted Prices\")\n",
    "\n",
    "# fit a regression line between high and low values to show linear nature\n",
    "sns.regplot(x=y_test, y=price_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge_regression.intercept_)\n",
    "print(ridge_regression.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Pipeline\n",
    "The next method involves placing the expanded polynomial features and linear regression of these within a pipeline which can be trained and used to predict the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14451.8497609  23645.4431646  43301.74970473 30554.99600358\n",
      " 15029.58660573 60042.6849235  68522.7234905  70031.13742336\n",
      " 12498.2880599   9939.39361695 13657.77434716 11749.97202266\n",
      " 43800.25035275 76050.50530402 24758.5564535  45121.02936203\n",
      " 13818.00130113 68906.63435158 46736.12879176 36745.01394664\n",
      " 12467.52975303 74031.13180218 12375.78903863 43236.84905855\n",
      " 14784.26699683 12410.68817201 23710.75823537 22451.13791809\n",
      " 78792.37811112 60853.4173909  72334.44551747 15544.24736262\n",
      " 12988.90680838 71691.97288227 62558.40694316 12321.08862602\n",
      " 13270.73561795 73851.90407614 14015.30226863 27043.84261296\n",
      " 13345.17751371 49408.93141789 73175.32006717 59976.02708372\n",
      " 65084.18982501 13787.60716279 49952.71031336 15063.04015456\n",
      " 14231.86121981 14136.57949379 13380.78833418 46467.65757751\n",
      " 14988.0839077  29755.92348715 35289.7715675  12190.98039995\n",
      " 24786.27773596 24183.61320924 23619.1359942  23957.79504429\n",
      " 13112.54649684 76766.97433793 15125.13946685 12746.8912965\n",
      " 16082.55847323 14182.73188852 31551.97745281 24580.0800544\n",
      " 13705.20613771 41529.19476119 12547.81552201 19878.63371064\n",
      " 12517.39938705 25142.329573   66689.18697223 71515.66288443\n",
      " 78941.49763198 14546.35624356 34038.07309611 14514.37669939\n",
      " 12549.04150053 17947.57251857 70540.74373431 16162.33104179\n",
      " 21029.97309661 24312.93387234 70675.87052838 15636.44167066\n",
      " 14459.16881198 14284.68859153 14378.7335072  13573.1222617\n",
      " 24859.36012585 12173.88298974 15768.99467429 60321.08165174\n",
      " 21113.73989042 35034.73014599 13201.69917152 78600.20074274\n",
      " 10677.51979314 13308.22750617 42915.16988201 12720.63678574\n",
      " 12343.44029904 48463.27127965 18506.01105739 13993.17548399\n",
      " 25255.26941608 75894.60729054 29189.12273961 69217.33246454\n",
      " 13072.83892524 15118.35688985 12425.20497339 15679.14366284\n",
      " 41104.73993435 10587.17408206 59016.61362072 15532.9641695\n",
      " 13355.44886006 12609.67448049 12471.82841257 17922.56287149\n",
      " 17337.84064698 69159.56018363 43980.19077715 74694.4621488\n",
      " 12436.8552032  60435.77289778 63477.75466725 16987.86165202\n",
      " 70951.79640012 12429.99246499 12768.27698545 10303.73211193\n",
      " 12788.49489251 15258.75272786 44842.6008331  12300.68705562\n",
      " 12523.70267843 12536.16461829 73648.98079842 12687.57551084\n",
      " 13456.49457565 12681.69337231 13814.81319002 13163.44679974\n",
      " 23458.13135573 70775.31823058 45365.92357585 61742.8908693\n",
      " 15281.29414255 24610.71351157 12957.00053664 40913.13815899\n",
      " 15146.40201643 17654.39025813 13598.38185778 12942.7766025\n",
      "  9763.13796947 20793.74726821 62785.24174955 14535.07444909\n",
      " 15407.80042468 41423.53045958 12757.31368259 70023.73241426\n",
      " 12943.59473076 61259.54145859 14176.38946087 12441.57924085\n",
      " 12999.65424966 23011.92869242 39859.49029491 24739.83820405\n",
      " 14449.57275126 13584.81282738 16960.51320689 15330.60013784\n",
      " 70727.88075937 69764.16252876 44725.15710169 21568.4218748\n",
      " 12530.15437871 15699.01777396 56909.39110706 71711.44296225\n",
      " 12553.98578686 12554.58832782 46991.06241963 15971.64938241\n",
      " 58850.94550216 12675.29063281 45291.78300256 70586.51532473\n",
      " 12962.4747692  15114.59991934 40662.74377864 64812.39952816\n",
      " 13016.39254315 12779.42718387 61319.87283826 14850.62541613\n",
      " 58816.78943779 13521.28942754 17273.15796748 14151.3138484\n",
      " 14665.02642752 30270.810001   21220.36218298 12826.09348835\n",
      " 12916.48964372 15095.47426069 17352.91938122 65331.01611844\n",
      " 51207.93339905 70694.50414155 14317.75531234 47356.16419736\n",
      " 74199.06276817 15164.84565871 14946.57243283 19411.39101648\n",
      " 30155.37813122 16000.99526935 49552.77134783 12296.11023478\n",
      " 69152.97122158 13354.01672965 14086.25848818 70520.89478045\n",
      " 73328.47124904 15204.44005325 72807.31318242 13198.72265469\n",
      " 18111.41225568 62738.2674014  12408.68271008 12443.39457938\n",
      " 12369.52870404 57294.024991   30295.26685835 40538.80996297\n",
      " 24799.83723208 12605.1319679  20206.08571318 36886.64766673\n",
      " 76000.76834894 17929.19819415 15724.34196841 12830.2820835\n",
      " 15011.56490132]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "print(y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I wanted to find the shape of the matrices involved and now these datasets have been expanded by the polynomial model I want to make sure they have the same dimensions (m.n) otherwise the polynomial regression won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of training set is 600.3385643743142\n",
      "R2 score of training set is 0.9992784058980032\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model on training dataset\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics from using the 2nd order quadratic coefficients and terms on the training set, a significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17778.77021386 40990.59151141 13213.10020101 13576.72568197\n",
      " 22115.23563654 21459.50220266 73861.87152871 14482.66725764\n",
      " 15325.35778916 10921.55020283 14274.74834326 14438.43409003\n",
      " 18386.97114914 12783.33172331 15507.84960687 13768.14202956\n",
      " 62068.22550618 61523.26190959 18361.33212892 13799.51185826\n",
      " 42156.59079595 24124.95329311 70694.4849419  64366.74010853\n",
      " 23930.90300245 34098.65495034 12485.03323    29102.95124855\n",
      " 19805.8878612  49418.14231437 15663.74751615 50940.73940904\n",
      " 12929.15492768 12956.75113655 12923.50518806 10831.35739012\n",
      " 12813.71474303 77223.50140964 65394.44079378 47005.1660213\n",
      " 24362.35252415 15492.08697297 71587.49574098 14875.08844024\n",
      " 12236.98139105 12649.13911789 12504.66919562 75497.99349968\n",
      " 24610.69635306 12861.14651084 20401.00988547 15076.34799577\n",
      " 12548.09785747 24769.10556438 12427.4554853  12911.48145233\n",
      " 12624.16070844 74538.2174557  12543.91326383 30184.58493709\n",
      " 65543.99852979 10963.76317933 22591.56408213 23293.72873672\n",
      " 14522.89529343 12953.93596608 15703.3046321  14188.25640479\n",
      " 73366.87962542 22801.70182585 14339.24911079 15844.87332176\n",
      " 19944.19521432 13105.09721811 49236.57361013 37139.84515487\n",
      " 73554.25396323 12792.12964719 13828.89578198 12303.4555896\n",
      " 13524.17480919 48311.89333501 13214.31560954 39209.96837002\n",
      " 13550.50890083 17085.41333254 47026.35100097 15500.67148035\n",
      " 15282.44706091 12777.52478063 15113.48704231 12755.50699347\n",
      " 13973.94811818 15137.66481396 15242.51436593 12679.86417534\n",
      " 15363.71679062 12809.84287214 15367.6234858  40231.3406182\n",
      " 60768.47004469 71363.51306921 13644.19844059 73784.66273069\n",
      " 14825.33265919 43196.09146128 23085.25326283 24486.63859507\n",
      " 15323.86968792]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_test, y_test)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "print(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics from generalizing to the test set data for the quadratic equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of test set is 358.82848984421446\n",
      "R2 score of test set is 0.999686822886171\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14479.41696141 23819.02394734 43655.12187891 30396.14781215\n",
      " 15035.22662708 60189.00993985 68511.85216449 70247.29157628\n",
      " 12560.78809674 10099.53593345 13482.88754155 11888.85501207\n",
      " 44075.1148766  76435.57188812 24963.162367   44189.78842616\n",
      " 13469.08234185 68784.97140446 46661.39198147 36715.76453925\n",
      " 12499.96718594 73948.31263382 12449.53953509 43600.57913168\n",
      " 14945.02546396 12472.72113217 23630.8851514  22235.02016008\n",
      " 79885.01627185 60939.35012036 72449.25051133 15522.93750821\n",
      " 13034.4647546  71017.50383388 62032.5181194  12343.39844425\n",
      " 13256.47144793 73868.00592164 14052.45186174 26860.76385027\n",
      " 13347.46665167 48717.26976134 72925.00054193 59921.89277069\n",
      " 64459.3874636  13779.11144849 50724.43876846 15074.83651556\n",
      " 14150.85721539 14176.32823545 13421.15125324 45895.63598491\n",
      " 15000.02074448 29803.06996917 35726.82401959 12238.36312613\n",
      " 24634.79429473 24171.98879771 23563.36512013 23873.32699919\n",
      " 13172.18287058 76622.26844757 15073.22177655 12814.10965542\n",
      " 16044.45228986 14169.23588266 31488.45699764 24561.29969753\n",
      " 13545.07421461 41419.10192416 12625.65939076 19841.04205255\n",
      " 12505.92208137 25046.31837263 68112.55873472 71253.54690062\n",
      " 78754.74914833 14551.55954621 33871.95756982 14502.675924\n",
      " 12630.7861691  17932.37074834 69841.39345074 16147.67000598\n",
      " 20979.58790455 23577.40691058 70934.40417995 15611.96280568\n",
      " 14420.35789759 14317.51268267 14403.55866126 13484.31983742\n",
      " 24504.93563726 12224.08458749 15672.4736793  60504.4523768\n",
      " 21056.11143634 35026.08238027 13251.51863691 78102.98053556\n",
      " 10727.5062062  13267.15997873 42521.65329181 12759.58478974\n",
      " 12407.16421435 48613.76207232 18504.00231296 14026.77845258\n",
      " 25316.31816635 76042.14137539 29066.50921043 69190.00970247\n",
      " 13101.53153637 15057.13700447 12246.89533774 15656.41719889\n",
      " 41196.28252089 10650.16971079 59027.87542411 15514.97511091\n",
      " 13207.21439699 12590.12892714 12541.755315   17846.68538183\n",
      " 17278.35243163 68763.9661371  44001.74708872 74015.12166192\n",
      " 12691.24998689 59897.21283197 62877.66927941 16968.60940316\n",
      " 70722.76460804 12501.00251645 12788.75070683 10383.16381973\n",
      " 12707.85790524 15113.9744333  44843.04694638 12373.7293479\n",
      " 12579.27790384 12665.59743044 73807.84576786 12762.89658629\n",
      " 13660.16117577 12550.72731846 13833.81182875 13219.29574818\n",
      " 23434.63862151 71426.04483656 45391.86112114 62110.51721864\n",
      " 15267.55553358 24584.42249399 12960.97399869 39544.98974905\n",
      " 15148.97306581 17648.34550615 13369.87104485 13004.4205827\n",
      "  9823.65904417 20627.48873033 62122.41986696 14562.96970455\n",
      " 15373.67950622 42041.60771855 12810.67550825 70077.5610178\n",
      " 12935.2462471  62078.1444556  14214.72331415 12483.49406521\n",
      " 13065.39733874 22990.75592474 39885.63403397 24677.02555085\n",
      " 14500.47816494 13460.84202117 17218.82287574 15272.85397803\n",
      " 71218.15555395 69741.14092765 45213.00001284 21510.66952407\n",
      " 12598.68271947 15675.02480728 57011.66993515 72118.73458231\n",
      " 12559.48930512 12584.47563319 47250.03433544 15964.76350297\n",
      " 58996.11968935 12743.39181774 45442.87259977 70217.33302136\n",
      " 12854.65382907 15099.06552446 42002.50420593 64702.42366159\n",
      " 13026.94544854 12852.22250547 61662.30424795 14855.72674941\n",
      " 58866.30526667 13468.51988296 17218.95881116 14177.39902847\n",
      " 14659.12240298 30261.03060554 21167.43834947 12899.92863831\n",
      " 12975.59517346 15092.01529563 17336.35244408 65398.7986121\n",
      " 51135.65534703 70168.89135174 14362.91798242 47457.59739869\n",
      " 74399.49554994 15135.72621203 14921.24505129 19271.78676312\n",
      " 30222.88655025 15922.14285277 49722.96190342 12356.65879057\n",
      " 69262.67110448 13404.10956442 14119.60076411 70466.59258476\n",
      " 72901.38736527 15199.77674053 73041.26291201 13287.0268847\n",
      " 18082.09718438 62568.90491937 12418.99406634 12509.02976083\n",
      " 12419.40585854 57845.34306555 30305.85143893 40762.80766131\n",
      " 24735.09397875 12467.5581637  20267.85575034 36953.46432034\n",
      " 76063.83616389 17882.45412    15717.58910381 12888.71955866\n",
      " 14994.23718192]\n",
      "\n",
      "\n",
      "RMSE of training set is 531.6845724438979\n",
      "R2 score of training set is 0.9994340101957949\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "print(y_train_pred)\n",
    "\n",
    "# evaluating the model on training dataset\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17999.87314506 41036.3726139  13056.40121838 13701.67317722\n",
      " 22104.10186216 21714.3055008  73907.70050981 14422.50682\n",
      " 15431.55230628 10866.27647892 14352.4288145  14352.70405383\n",
      " 18384.19533799 12735.49927782 15701.27835738 13911.83559148\n",
      " 62393.71791759 61668.81693837 18559.98004196 13732.5156679\n",
      " 42587.85956154 24442.33674872 70804.13719409 64096.27929078\n",
      " 24011.81019671 34328.62649579 12545.3336179  29153.90783245\n",
      " 20217.50342343 48763.65288448 15675.04614768 50771.27927526\n",
      " 13092.30456592 13055.79483562 12701.64023711 10779.658345\n",
      " 12585.23232223 77044.25461137 65833.6725336  47346.27508013\n",
      " 24519.62352237 15380.70780456 71901.52569645 14866.88567717\n",
      " 12030.49586924 12450.2152917  12392.79375211 75513.34479543\n",
      " 24737.82662495 12669.71064957 20397.82633078 15235.04099928\n",
      " 12781.84989068 24784.40035776 12447.43422649 12898.16822852\n",
      " 12419.72011144 74452.24768999 12424.16441931 30325.84505421\n",
      " 66302.90614064 10765.99838817 22746.28375906 23522.44763902\n",
      " 14360.30785163 12896.10323269 15616.14915448 14030.35011639\n",
      " 72677.31950631 22945.18216521 14284.4183877  15917.28732698\n",
      " 20016.91045766 12946.49169126 49547.989085   37094.28641718\n",
      " 73747.03898656 12768.07099282 13806.74238361 12413.19431719\n",
      " 13487.0805863  47392.20564016 13059.25802335 38919.59759798\n",
      " 13442.3270521  17183.17548453 46421.67414719 15742.9250234\n",
      " 15643.28098513 12955.23747301 14909.44162974 12773.79280485\n",
      " 14080.92334391 14927.38118047 15190.14830638 12528.42062619\n",
      " 15253.4950738  12846.28851361 15293.7687168  40935.60444222\n",
      " 60787.02754365 71528.53340897 13694.83350464 73687.02352118\n",
      " 15109.44095897 41369.08895037 23194.78995177 24664.92323773\n",
      " 14933.69645243]\n",
      "\n",
      "\n",
      "RMSE of test set is 199.2295604183635\n",
      "R2 score of test set is 0.9999034563450762\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_test, y_test)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "print(y_test_pred)\n",
    "\n",
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metrics from using an equation with 3rd order cubic coefficients and terms on the test set produced the best overall scores so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "So the fit of the regression forecast line to the data has increased in its accuracy marginally with the degree of fit (or expansion of terms) and the RMSE has reduced fairly considerably. The R-squared accuracy appears to increase with smaller sets of data such as the test sets.\n",
    "\n",
    "I have decided to see if I can improve the model's predictive power by electing to use a Decision Tree Regression model: \"https://github.com/lynstanford/machine-learning-projects/tree/master/machine-learning/decision_tree.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
