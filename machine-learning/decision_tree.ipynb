{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Models\n",
    "The Decision Tree model can be used to discover complex linear relationships between variables for either prediction, binary classification or multi-output classification. Obviously in this case I am looking for price prediction given a relatively small number of features.\n",
    "\n",
    "Importing the dataset and dependancies is the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# import data\n",
    "bitcoin = pd.read_csv(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "\n",
    "# select data subset\n",
    "df = pd.DataFrame(bitcoin).dropna(axis=0)\n",
    "\n",
    "# select data for modeling\n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
    "y = df[\"Close\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the linear model and fitting the regression line to the entire dataset based on predictors and labelled data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "lin_reg = linear_model.LinearRegression()\n",
    "\n",
    "# fit model\n",
    "lin_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.32747159e-01,  9.17019324e-01,  5.08535341e-01, -1.62880798e-09])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104.14703757539974"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the entire dataset needs to be split and trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = lin_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying a prediction on the working linear model (first 5 values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14398.87824859 23615.43973771 43221.96880896 30312.88923232\n",
      " 14945.24131451]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14398.87824859 23615.43973771 43221.96880896 30312.88923232\n",
      " 14945.24131451]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the RMSE and r-squared score for the linear model (based on training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "677.2239361854425\n",
      "0.9990817412202545\n"
     ]
    }
   ],
   "source": [
    "y_pred = lin_reg.predict(X_train)\n",
    "lin_mse = mean_squared_error(y_train, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred)\n",
    "print(r2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "I think it's really important to view this relationship in graphical format and have elected to use the Seaborn and Matplotlib visualization packages, firstly to display time-series price data but also to fit a regression line over the plot so I can see just how the prediction line fits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file path\n",
    "bitcoin_filepath = \"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\"\n",
    "\n",
    "# Read the file into a 'bitcoin' variable\n",
    "bitcoin = pd.read_csv(bitcoin_filepath, index_col=\"Date\", parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again setting the width and height of the figure\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Daily Bitcoin Close Price in $CAD\")\n",
    "\n",
    "# Line chart showing daily 'Close'\n",
    "sns.lineplot(data=bitcoin[\"Close\"], label=\"Close\")\n",
    "\n",
    "# Add label for horizontal axis\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, include the packages for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the width and height of the plot\n",
    "plt.figure(figsize=(14,6))\n",
    "# line chart of daily Bitcoin prices\n",
    "sns.lineplot(data=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a regression line will help make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "#sns.scatterplot(x=bitcoin[\"Open\"], y=bitcoin[\"Close\"])\n",
    "sns.lineplot(data=X)\n",
    "sns.regplot(x=bitcoin[\"Date\"], y=X)\n",
    "#sns.regplot(x=bitcoin[\"Open\"], y=bitcoin[\"Close\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So based on all the data contained within the matrix of values in 'X', how can I predict values in the vector 'y'? Once I have established the linear relationship between the dependent and independent variables I can summarize them in the following equation:\n",
    "\n",
    "y = a + b1x1 + b2x2 + b3x3 + b4x4 + e\n",
    "\n",
    "a = 104.14703757539974\n",
    "\n",
    "b1 = -4.32747159e-01\n",
    "x1 = \n",
    "\n",
    "b2 = 9.17019324e-01\n",
    "\n",
    "b3 = 5.08535341e-01\n",
    "\n",
    "b4 = -1.62880798e-09\n",
    "\n",
    "e = 677.2239361854425\n",
    "\n",
    "So, calculating a prediction of y based on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lin_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting price based on Open = C$35,000, High = C$40,000, Low = C$32,000 and Volume = 100bn\n",
    "lin_reg.predict([[35000, 40000, 32000, 100000000000]])\n",
    "\n",
    "y = lin_reg.predict(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once again finding the RMSE and r-squared for the linear model (based on the test set) this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lin_reg.predict(X_test)\n",
    "lin_mse = mean_squared_error(y_test, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "    \n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "print(r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this linear model appears to generalize well to the unseen (test) data having reduced the overall variance improving accuracy.\n",
    "\n",
    "# Decision Tree Model Selection\n",
    "Next, it's time to apply a Decision Tree model to the entire dataset before seeking further improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = tree_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying a prediction on the working linear model (first 5 values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the RMSE and r-squared score for the linear model (based on training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree_reg.predict(X_train)\n",
    "lin_mse = mean_squared_error(y_train, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "    \n",
    "r2_train = r2_score(y_train, y_pred)\n",
    "print(r2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This definitely appears to be overfitting with perfect scores for both RMSE and r-squared. Let's see if there is a different outcome for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tree_reg.predict(X_test)\n",
    "lin_mse = mean_squared_error(y_test, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "    \n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "print(r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears unlikely also. In order to establish a more likely outcome I will try dividing the dataframe into several smaller training and validation sets and perform the decision tree analysis on each. This is done using K-Fold Cross Validation.\n",
    "\n",
    "## Cross Validation\n",
    "This method will evaluate the Decsion Tree model by splitting the training set into several smaller training and validation sets for training and evaluation separately. This is achieved by using the K-fold cross validation technique and I have split the data into 10 separate folds, cv=10 (which can be changed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "                         \n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean\", scores.mean())\n",
    "    print(\"Standard Deviation\", scores.std())\n",
    "    print(\"R-Squared:\", r2_test)\n",
    "          \n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the scores from cross validation to those from the linear regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model Selection\n",
    "Next I will try the Random Forest Regressor model to try and improve on these scores and their accuracy. Using a Random Forest model should provide a more accurate prediction because it's an aggregate of several individual decision tree models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "forest_reg = RandomForestRegressor()\n",
    "forest_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = forest_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying a prediction on the working linear model (first 5 values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring the RMSE and r-squared score for the linear model (based on training set):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest_reg.predict(X_train)\n",
    "lin_mse = mean_squared_error(y_train, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "\n",
    "r2_train = r2_score(y_train, y_pred)\n",
    "print(r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = forest_reg.predict(X_test)\n",
    "lin_mse = mean_squared_error(y_test, y_pred)\n",
    "lin_rmse = np.sqrt(lin_mse)\n",
    "print(lin_rmse)\n",
    "\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "print(r2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalizes well with the test set data but I aim to use the cross-validation method one more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(forest_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "tree_rmse_scores = np.sqrt(-scores)\n",
    "r2_test = r2_score(y_test, y_pred)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores:\", scores)\n",
    "    print(\"Mean\", scores.mean())\n",
    "    print(\"Standard Deviation\", scores.std())\n",
    "    print(\"R-Squared:\", r2_test)\n",
    "          \n",
    "display_scores(tree_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the cross validation appears to have reduced the standard deviation considerably using the random forest ensemble method. Once again, I am comparing the scores from cross validation to those from the linear regression model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_scores = cross_val_score(lin_reg, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "lin_rmse_scores = np.sqrt(-lin_scores)\n",
    "display_scores(lin_rmse_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So evaluating each of the 10 subsets using K-Folds Cross Validation has produced the most accurate score and lowest margin of error so far.\n",
    "\n",
    "Saving the file as a pickle file will ensure some consistency when comparing scores, parameters and hyperparameters and enable me to start where I left off!\n",
    "\n",
    "I first need import pickle and joblib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('decision_tree.pickle','wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "    \n",
    "pickle_in = open('decision_tree.pickle','rb')\n",
    "clf = pickle.load(pickle_in)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
