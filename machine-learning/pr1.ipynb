{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import data\n",
    "bitcoin = pd.read_csv(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "df = pd.DataFrame(bitcoin).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'], dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          object\n",
       "Open         float64\n",
       "High         float64\n",
       "Low          float64\n",
       "Close        float64\n",
       "Adj Close    float64\n",
       "Volume       float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Selection\n",
    "Assign the (dependant) y variable and (independent) X variables for the modelling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select data for modeling\n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
    "y = df[\"Close\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "Using this data for the Polynomial Features model and splitting it into training and test sets with a 70-30 split. Make a copy of the dataframe first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14451.8497609  23645.4431646  43301.74970473 30554.99600358\n",
      " 15029.58660573 60042.6849235  68522.7234905  70031.13742336\n",
      " 12498.2880599   9939.39361695 13657.77434716 11749.97202266\n",
      " 43800.25035275 76050.50530402 24758.5564535  45121.02936203\n",
      " 13818.00130113 68906.63435158 46736.12879176 36745.01394664\n",
      " 12467.52975303 74031.13180218 12375.78903863 43236.84905855\n",
      " 14784.26699683 12410.68817201 23710.75823537 22451.13791809\n",
      " 78792.37811112 60853.4173909  72334.44551747 15544.24736262\n",
      " 12988.90680838 71691.97288227 62558.40694316 12321.08862602\n",
      " 13270.73561795 73851.90407614 14015.30226863 27043.84261296\n",
      " 13345.17751371 49408.93141789 73175.32006717 59976.02708372\n",
      " 65084.18982501 13787.60716279 49952.71031336 15063.04015456\n",
      " 14231.86121981 14136.57949379 13380.78833418 46467.65757751\n",
      " 14988.0839077  29755.92348715 35289.7715675  12190.98039995\n",
      " 24786.27773596 24183.61320924 23619.1359942  23957.79504429\n",
      " 13112.54649684 76766.97433793 15125.13946685 12746.8912965\n",
      " 16082.55847323 14182.73188852 31551.97745281 24580.0800544\n",
      " 13705.20613771 41529.19476119 12547.81552201 19878.63371064\n",
      " 12517.39938705 25142.329573   66689.18697223 71515.66288443\n",
      " 78941.49763198 14546.35624356 34038.07309611 14514.37669939\n",
      " 12549.04150053 17947.57251857 70540.74373431 16162.33104179\n",
      " 21029.97309661 24312.93387234 70675.87052838 15636.44167066\n",
      " 14459.16881198 14284.68859153 14378.7335072  13573.1222617\n",
      " 24859.36012585 12173.88298974 15768.99467429 60321.08165174\n",
      " 21113.73989042 35034.73014599 13201.69917152 78600.20074274\n",
      " 10677.51979314 13308.22750617 42915.16988201 12720.63678574\n",
      " 12343.44029904 48463.27127965 18506.01105739 13993.17548399\n",
      " 25255.26941608 75894.60729054 29189.12273961 69217.33246454\n",
      " 13072.83892524 15118.35688985 12425.20497339 15679.14366284\n",
      " 41104.73993435 10587.17408206 59016.61362072 15532.9641695\n",
      " 13355.44886006 12609.67448049 12471.82841257 17922.56287149\n",
      " 17337.84064698 69159.56018363 43980.19077715 74694.4621488\n",
      " 12436.8552032  60435.77289778 63477.75466725 16987.86165202\n",
      " 70951.79640012 12429.99246499 12768.27698545 10303.73211193\n",
      " 12788.49489251 15258.75272786 44842.6008331  12300.68705562\n",
      " 12523.70267843 12536.16461829 73648.98079842 12687.57551084\n",
      " 13456.49457565 12681.69337231 13814.81319002 13163.44679974\n",
      " 23458.13135573 70775.31823058 45365.92357585 61742.8908693\n",
      " 15281.29414255 24610.71351157 12957.00053664 40913.13815899\n",
      " 15146.40201643 17654.39025813 13598.38185778 12942.7766025\n",
      "  9763.13796947 20793.74726821 62785.24174955 14535.07444909\n",
      " 15407.80042468 41423.53045958 12757.31368259 70023.73241426\n",
      " 12943.59473076 61259.54145859 14176.38946087 12441.57924085\n",
      " 12999.65424966 23011.92869242 39859.49029491 24739.83820405\n",
      " 14449.57275126 13584.81282738 16960.51320689 15330.60013784\n",
      " 70727.88075937 69764.16252876 44725.15710169 21568.4218748\n",
      " 12530.15437871 15699.01777396 56909.39110706 71711.44296225\n",
      " 12553.98578686 12554.58832782 46991.06241963 15971.64938241\n",
      " 58850.94550216 12675.29063281 45291.78300256 70586.51532473\n",
      " 12962.4747692  15114.59991934 40662.74377864 64812.39952816\n",
      " 13016.39254315 12779.42718387 61319.87283826 14850.62541613\n",
      " 58816.78943779 13521.28942754 17273.15796748 14151.3138484\n",
      " 14665.02642752 30270.810001   21220.36218298 12826.09348835\n",
      " 12916.48964372 15095.47426069 17352.91938122 65331.01611844\n",
      " 51207.93339905 70694.50414155 14317.75531234 47356.16419736\n",
      " 74199.06276817 15164.84565871 14946.57243283 19411.39101648\n",
      " 30155.37813122 16000.99526935 49552.77134783 12296.11023478\n",
      " 69152.97122158 13354.01672965 14086.25848818 70520.89478045\n",
      " 73328.47124904 15204.44005325 72807.31318242 13198.72265469\n",
      " 18111.41225568 62738.2674014  12408.68271008 12443.39457938\n",
      " 12369.52870404 57294.024991   30295.26685835 40538.80996297\n",
      " 24799.83723208 12605.1319679  20206.08571318 36886.64766673\n",
      " 76000.76834894 17929.19819415 15724.34196841 12830.2820835\n",
      " 15011.56490132]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_train)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 4)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15143.75  46890.625 23544.    42555.75  12208.875 73951.875 12675.75\n",
      " 23140.75  13465.875 41214.625 15144.125 48731.625 16173.375 12844.125\n",
      " 12616.875 13761.625 16800.5   14068.25  49521.5   14424.125 70529.125\n",
      " 15037.75  12776.    15811.125 13663.125 11017.5   45432.125 30574.75\n",
      " 12637.25  14278.75  14707.    12468.25  47295.125 33761.    17117.25\n",
      " 17294.5   31842.125 19437.875 12813.625 12551.625 69778.125 13277.625\n",
      " 13699.125 40919.875 14769.    20117.375 12500.875 13791.    13061.125\n",
      " 13766.125 15337.    12448.5   66068.375 24980.    50203.25  23805.625\n",
      " 15499.125 14099.5   21425.875 12332.5   68364.375 61557.75  61494.25\n",
      " 72298.5   40707.125 14976.75  72443.875 17159.875 43128.5   15195.\n",
      " 12980.25  12851.125 29040.375 18346.625 15199.25  15611.5   49512.5\n",
      " 24427.625 15705.5   68982.25  69734.125 48819.625 72863.25  14067.5\n",
      " 40925.5   34680.25  13984.875 36650.75  10521.75  15778.75  21502.\n",
      " 30864.875 35030.    15546.375 39021.125 23325.5   62721.75  15062.625\n",
      " 12329.875 57031.375 13161.875 12565.5   66220.125 15656.625 60345.\n",
      " 24616.375 15566.25  17135.625 70402.875 13237.75  68260.625 12641.25\n",
      " 61790.    12439.25  18033.875 12487.625 14392.875 12502.875 12195.25\n",
      " 13561.875 14306.375 13524.125 29641.375 14495.    23374.875 24308.875\n",
      " 15389.625 30505.375 12640.625 44921.125 13081.25  74412.75  15347.5\n",
      " 21075.5   15279.25  13778.    12350.    44015.25  21812.5   13720.\n",
      " 13645.    79611.75  13531.125 34135.75  65542.5   10701.875  9739.625\n",
      " 76406.375 14855.75  24783.75  58841.375 24053.125 14768.125 14609.75\n",
      " 24415.25  12776.625 13000.    12875.    19905.125 72586.125 13513.875\n",
      " 43125.875 23334.5   51086.375 14382.875 59423.625 13459.375 12969.375\n",
      " 13491.375 14819.875 12193.375 12673.75  12669.625 14257.25  13103.\n",
      " 72039.375 74316.75  22327.    40598.125 61160.75  27037.125 23239.875\n",
      " 23933.75  73065.125 37003.75  75451.    74886.    46757.5   20475.\n",
      " 14546.625 12937.    22276.125 18376.75  51777.625 15108.75  12811.25\n",
      " 10043.75  13116.5   12513.75  20884.125 12415.    43086.75  12685.125\n",
      " 12838.875 47364.75  29291.5   12591.375 61923.75  12427.5   42024.625\n",
      " 14244.5   78990.375 14935.75  62209.125 73147.125 69278.    29783.75\n",
      " 12824.    13172.875 15162.25  18053.375 70374.375 17684.5   14138.75\n",
      " 12782.5   13059.    24566.375 37433.875 12418.25  64089.125 14533.125\n",
      " 41187.375 18217.125 60846.875 74480.875 46581.625 59025.125 14437.125\n",
      " 15119.875 14646.125 14848.25  12378.625 73090.    24707.875 15652.375\n",
      " 71449.    11990.375 17943.875 12648.5   15553.875 45912.625 70725.25\n",
      " 15666.25 ]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_train)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(253,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14250.53369414 12562.53106627 13204.80690276 71525.98519773\n",
      " 12713.1814578  16908.99156536 12681.05236739 15892.85660387\n",
      " 46093.40166445 15001.95729851 49945.98053713 13303.32750943\n",
      " 24006.03705258 12553.08014625 13273.59584413 20038.55523906\n",
      " 40865.15590067 23933.3930027  70685.66285582 15192.04099925\n",
      " 12105.92302562 17723.81415783 12400.29627733 47914.07357187\n",
      " 69542.20183439 18433.29535006 23069.56302614 14142.64107331\n",
      " 14488.67907615 75906.77459593 13354.2130447  15655.28400306\n",
      " 12912.72538788 73274.48583097 13935.27736552 15368.13836839\n",
      " 13022.01772091 13019.43906493 62545.48979063 15074.26359943\n",
      " 30098.31387384 73823.69169493 12564.37969487 59023.10669966\n",
      " 15614.67853868 73938.4593112  12435.95744831 12899.56267035\n",
      " 10818.08396667 13121.97196706 24445.16507949 14445.83268797\n",
      " 43844.29294844 24418.61994744 14938.31357524 66392.62529631\n",
      " 12829.28798428 10597.78819187 45399.22587175 12499.83581245\n",
      " 65154.95045347 41713.04059715 12683.6264401  13034.48027815\n",
      " 38992.042381   12410.11833647 24622.45277066 15297.84970484\n",
      " 60584.16243238 12568.0420391  12795.17539593 74036.96897764\n",
      " 72504.91383238 23331.46325154 70596.25820799 79437.09226866\n",
      " 12314.51732655 12602.28433606 72979.10959501 13848.73922507\n",
      " 12587.97093626 45705.40028639 10908.96843598 15414.58260364\n",
      " 15719.02326658 70690.22777052 61893.5456912  57313.02065344\n",
      " 72715.28201933 15613.77922672 59816.95073576 12689.25608864\n",
      " 20380.63544719 70768.86012907 15344.3884658  25508.67783804\n",
      " 13547.80757532 13793.60954286 14227.21168229 13168.47083635\n",
      " 20436.53853775 64683.97617598 75246.75312681 24375.1718169\n",
      " 13365.56118811 14326.09263372 24575.46441359 77018.41432066\n",
      " 58850.14300604]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_test, y_test)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109, 15)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14115.09002156 12293.22000636 13128.23001394 71526.08002348\n",
      " 12871.53001822 16973.62002356 12693.78000788 15775.92002137\n",
      " 46081.13999815 14964.7900147  49945.91001196 13359.90001399\n",
      " 24014.90001483 12791.00001521 13284.56001009 19945.73001189\n",
      " 40865.06001348 23905.97001667 70684.33003874 15186.41001428\n",
      " 12083.24001903 17915.13001771 12380.24000908 47908.07001753\n",
      " 69541.94000883 18553.15002139 23017.67001862 14149.28001798\n",
      " 14466.70002044 75906.36005278 13228.95002223 15529.15001578\n",
      " 12873.86001648 73273.84000273 13916.70000966 15581.58001543\n",
      " 13041.35001947 13062.20001936 62545.23992033 15031.95001967\n",
      " 30075.87001104 73827.42005033 12666.33002163 59023.47010298\n",
      " 15733.03002531 73942.95004911 12395.20001901 12907.76001991\n",
      " 10836.6800103  12972.7900175  24409.37001772 14397.66002322\n",
      " 43844.3299777  24407.62000723 14936.71001608 66392.34007902\n",
      " 12843.04001645 10642.810019   45408.61001844 12588.00001563\n",
      " 65160.43012113 41711.19001074 12745.56000678 12956.88000597\n",
      " 38992.06998701 12313.75002129 24803.39002056 15375.8600157\n",
      " 60583.38005705 12613.80001349 12796.08001609 74029.63004179\n",
      " 72505.56003447 23320.17002029 70596.59000273 79437.88004179\n",
      " 12296.15000973 12571.09002252 72978.66001005 13904.59001822\n",
      " 12541.29001547 45711.01000471 10917.12001533 15342.80001582\n",
      " 15742.82001693 70691.18993254 61894.21996183 57312.20000334\n",
      " 72713.56000395 15633.23001785 59816.94003935 12700.57002611\n",
      " 20383.97000523 70772.35001249 15230.27001811 25498.36002059\n",
      " 13413.77002252 13795.97002448 14244.94002075 13162.55001929\n",
      " 20332.17002006 64684.28992338 75243.41995878 24380.23002357\n",
      " 13579.41001261 14325.02001752 24572.39002008 77018.32019682\n",
      " 58850.14007063]\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_test, y_test)\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_polynomial_regression_model(degree):\n",
    "  \"Creates a polynomial regression model for the given degree\"\n",
    "  \n",
    "  poly_features = PolynomialFeatures(degree=degree)\n",
    "  \n",
    "  # transforms the existing features to higher degree features.\n",
    "  X_train_poly = poly_features.fit_transform(X_train)\n",
    "  \n",
    "  # fit the transformed features to Linear Regression\n",
    "  poly_model = LinearRegression()\n",
    "  poly_model.fit(X_train_poly, Y_train)\n",
    "  \n",
    "  # predicting on training data-set\n",
    "  y_train_predicted = poly_model.predict(X_train_poly)\n",
    "  \n",
    "  # predicting on test data-set\n",
    "  y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n",
    "  \n",
    "  # evaluating the model on training dataset\n",
    "  rmse_train = np.sqrt(mean_squared_error(Y_train, y_train_predicted))\n",
    "  r2_train = r2_score(Y_train, y_train_predicted)\n",
    "  \n",
    "  # evaluating the model on test dataset\n",
    "  rmse_test = np.sqrt(mean_squared_error(Y_test, y_test_predict))\n",
    "  r2_test = r2_score(Y_test, y_test_predict)\n",
    "  \n",
    "  print(\"The model performance for the training set\")\n",
    "  print(\"-------------------------------------------\")\n",
    "  print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "  print(\"R2 score of training set is {}\".format(r2_train))\n",
    "  \n",
    "  print(\"\\n\")\n",
    "  \n",
    "  print(\"The model performance for the test set\")\n",
    "  print(\"-------------------------------------------\")\n",
    "  print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "  print(\"R2 score of test set is {}\".format(r2_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "Import the library and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# initialize model\n",
    "poly_model = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "# fit the model to the training set\n",
    "X_train_poly = poly_model.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using indexation to return any value in X, say the 1st value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note here is I am not training the entire dataframe of X, so it might be more accurate to display the first row of X_train. The main point here is there are only 4 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has taken the first value from row 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now repeating for the data contained in the 'X_train_poly' set and we can see that there are 14 values returned so it has created an array with 10 new features for a total of 14 features. This includes element-wise dot product values and some squared values (without going into to much detail). Both the original feature values for 'X1' to 'Xn' and the feature squared value from 'X_poly' are returned in this example. Now this new data matrix containing the additional features with the squared values has been created by expanding the number of features and the parameter weights (or coefficients), the linear regression model can be applied again to this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the linear regression library first\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# initialize the model\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# fit the model but using the X_train_poly dataframe this time\n",
    "linear_regression.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the bias term (intercept) and coefficients are both attributes of the LinearRegression() model so I can examine these from the independent variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.intercept_\n",
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the test set\n",
    "X_test_poly = poly_model.fit_transform(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# initialize model\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# fit the model again but using the test set this time\n",
    "linear_regression.fit(X_test_poly, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.intercept_\n",
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "Here are some predictions using the test set data, before measuring their degree of variance and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred = linear_regression.predict(X_train_poly)\n",
    "print(\"Price Predictions: \", linear_regression.predict(X_train_poly.iloc[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting price based on Open = C$35,000, High = C$40,000, Low = C$32,000 and Volume = 100bn\n",
    "# linear_regression.predict([[35000, 40000, 32000, 100000000000]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validating the polynomial regression function with X and y training data and an attribute of 'degree=2' can be achieved using a mean squared error score and r-squared accuracy measure as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# model evaluation\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly = poly_model.fit_transform(X_test)\n",
    "print(\"R-squared: \", linear_regression.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can see the R-squared value is not as good as the previous score after trying polynomial regression with 'degree=2' squared terms. Having applied the Polynomial Features model and fitted it to the training set I have decided to save the data to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/X_poly.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I change the parameter affecting the degree to which terms are multiplied to 'degree=3' in the polynomial equation?\n",
    "\n",
    "## Training the model\n",
    "Import the library and instantiate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "poly_model = PolynomialFeatures(degree=3, include_bias=False)\n",
    "\n",
    "# fit the model to the training set\n",
    "X_train_poly = poly_model.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the linear model once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the linear regression library first\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# initialize the model\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# fit the model but using the X_poly dataframe this time\n",
    "linear_regression.fit(X_train_poly, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intercept and coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression.intercept_\n",
    "linear_regression.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "This time for 'degree=3' terms the rmse score and r-squared measure based on the test sets give:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_pred = linear_regression.predict(X_test)\n",
    "print(\"Price Predictions: \", linear_regression.predict(X_test.iloc[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model evaluation\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_poly = poly_model.fit_transform(X_test)\n",
    "print(\"R-squared: \", linear_regression.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This causes a significant drop in the R-squared value or the degree of fit to the line, so may not be the most accurate model to use. I have decided to see if I can improve the model's predictive power by electing to use a Decision Tree Regression model: \"https://github.com/lynstanford/machine-learning-projects/tree/master/machine-learning/decision_tree.ipynb\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
