{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.metrics import r2_score\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, y_test_pred)\r\n",
    "print(mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(rmse)\r\n",
    "\r\n",
    "# set the width and height of the plot\r\n",
    "plt.figure(figsize=(6,6))\r\n",
    "\r\n",
    "# visualizing the relationship between actual and predicted values for y\r\n",
    "plt.scatter(y_test, y_test_pred)\r\n",
    "plt.xlabel(\"Actual Prices\")\r\n",
    "plt.ylabel(\"Predicted Prices\")\r\n",
    "plt.title(\"Actual Prices vs Predicted Prices\")\r\n",
    "\r\n",
    "# fit a regression line between high and low values to show linear nature\r\n",
    "sns.regplot(x=y_test, y=y_test_pred)\r\n",
    "\r\n",
    "# evaluating the model on training dataset\r\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\r\n",
    "r2_train = r2_score(y_train, y_train_pred)\r\n",
    "  \r\n",
    "# evaluating the model on test dataset\r\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\r\n",
    "r2_test = r2_score(y_test, y_test_pred)\r\n",
    "\r\n",
    "print(\"\\n\")\r\n",
    "\r\n",
    "print(\"The model performance for the training set\")\r\n",
    "print(\"-------------------------------------------\")\r\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\r\n",
    "print(\"R2 score of training set is {}\".format(r2_train))\r\n",
    "  \r\n",
    "print(\"\\n\")\r\n",
    "  \r\n",
    "print(\"The model performance for the test set\")\r\n",
    "print(\"-------------------------------------------\")\r\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\r\n",
    "print(\"R2 score of test set is {}\".format(r2_test))\r\n",
    "\r\n",
    "print(\"\\n\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining the intercept of the polynomial line and coefficient values for x1, x2 ... xn."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ridge_regression.intercept_\r\n",
    "ridge_regression.coef_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given:\n",
    "\n",
    "    y = a + b1x1 + b2x2 + b3x3 + b4x4\n",
    "    \n",
    "\n",
    "This tells me the prediction for the target output variable, y, based on the input variables specified and using ridge regression giving the value of C$37,990.99.\n",
    "\n",
    "## Model Validation Metrics\n",
    "Now to measure the error score and accuracy of the line of fit.\n",
    "\n",
    "## Applying Regularization\n",
    "To avoid overfitting regularization can be applied, but will require tuning hyperparameters manually to an extent. To see if I can improve on the linear regression model's ability to predict the target variable, I have decided to use Ridge Regression.\n",
    "This regularization term basically employs the use of a penalization method by summing the squared values of each coefficient (whether positive or negative) and helps balance the contribution of all the features but this term is only used during the training phase so I need to remove it for the purpose of testing and evaluation.\n",
    "\n",
    "## Model Selection\n",
    "### Ridge Regression\n",
    "Trying a slightly different type of regression model using the alpha learning rate hyperparameter of 1.0 to see if I can reduce the (rmse) error value and increase the (r2) accuracy score. The purpose of using a ridge regression model is to try to reduce or eliminate the coefficient values of all the various features (especially those with high multi-colinearity between predictors) and will increase bias slightly but should decrease variance significantly. Ridge regression achieves this by assigning equal weights to those coefficient values which have high colinearity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# instantiate model\r\n",
    "ridge_regression = linear_model.Ridge(alpha=1.0)\r\n",
    "\r\n",
    "# fit model\r\n",
    "ridge_regression.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The regularization term should only be added to the cost function during the training phase. Now the 'training' data has been fit, it becomes important to discover the performance measure on the unregularized test set. Making a prediction on the first 5 values in the test set first."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "price_predictions = ridge_regression.predict(X_test)\r\n",
    "print(\"Predictions: \", ridge_regression.predict(X_test.iloc[:5]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This tells me the prediction for the target output variable, y, based on the input variables specified and using ridge regression giving the value of C$37,990.99.\n",
    "\n",
    "## Model Validation MetricsÂ¶\n",
    "Now to measure the error score and accuracy of the line of fit.\n",
    "\n",
    "This new data matrix containing the additional features with the squared values has been created by expanding the number of features and the parameter weights (or coefficients) which represent a quadratic equation. The linear regression model should now be applied again to this newly expanded dataframe containing the polynomial features."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import LinearRegression\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "\r\n",
    "mse = mean_squared_error(y_test, price_predictions)\r\n",
    "print(mse)\r\n",
    "\r\n",
    "rmse = np.sqrt(mse)\r\n",
    "print(rmse)\r\n",
    "\r\n",
    "# set the width and height of the plot\r\n",
    "plt.figure(figsize=(6,6))\r\n",
    "\r\n",
    "# visualizing the relationship between actual and predicted values for y\r\n",
    "plt.scatter(y_test, price_predictions)\r\n",
    "plt.xlabel(\"Actual Prices\")\r\n",
    "plt.ylabel(\"Predicted Prices\")\r\n",
    "plt.title(\"Actual Prices vs Predicted Prices\")\r\n",
    "\r\n",
    "# fit a regression line between high and low values to show linear nature\r\n",
    "sns.regplot(x=y_test, y=price_predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"R-squared: \", ridge_regression.score(X_test, y_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Producing a scatter plot of the data points to display actual vs predicted prices and the regression line of best fit."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, price_predictions)\n",
    "print(mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n",
    "\n",
    "# set the width and height of the plot\n",
    "plt.figure(figsize=(6,6))\n",
    "\n",
    "# visualizing the relationship between actual and predicted values for y\n",
    "plt.scatter(y_test, price_predictions)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual Prices vs Predicted Prices\")\n",
    "\n",
    "# fit a regression line between high and low values to show linear nature\n",
    "sns.regplot(x=y_test, y=price_predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(ridge_regression.intercept_)\n",
    "print(ridge_regression.coef_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a slight improvement on the original R-squared score and I can see that the RMSE hasn't really changed.\n",
    "Trying to improve on the scores above, I decided to introduce a Polynomial model which should be able to fit the line more accurately to the data points."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using a Pipeline\n",
    "The next method involves placing the expanded polynomial features and linear regression of these within a pipeline which can be trained and used to predict the target variable."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "print(y_train_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next I wanted to find the shape of the matrices involved and now these datasets have been expanded by the polynomial model I want to make sure they have the same dimensions (m.n) otherwise the polynomial regression won't work."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(X_train.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y_train.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y_train_pred.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# evaluating the model on training dataset\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The metrics from using the 2nd order quadratic coefficients and terms on the training set, a significant improvement."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_test, y_test)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "print(y_test_pred)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The metrics from generalizing to the test set data for the quadratic equation."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "print(y_train_pred)\n",
    "\n",
    "# evaluating the model on training dataset\n",
    "rmse_train = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of training set is {}\".format(rmse_train))\n",
    "print(\"R2 score of training set is {}\".format(r2_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(X_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(y_test_pred.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Applying the same model to the test set only for validation:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipeline = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('reg', LinearRegression(normalize=True))\n",
    "    ])\n",
    "\n",
    "pipeline.fit(X_test, y_test)\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "print(y_test_pred)\n",
    "\n",
    "# evaluating the model on test dataset\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# print the RMSE metric and R2 accuracy score\n",
    "print(\"\\n\")\n",
    "print(\"RMSE of test set is {}\".format(rmse_test))\n",
    "print(\"R2 score of test set is {}\".format(r2_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Evaluation\n",
    "So the fit of the regression forecast line to the data has increased in its accuracy marginally with the degree of fit (or expansion of terms) and the RMSE has reduced fairly considerably. The R-squared accuracy appears to increase with smaller sets of data such as the test sets overall.\n",
    "\n",
    "Ridge regression rmse score = 689.1925582115692\n",
    "R-squared accuracy = 0.9991392014478754\n",
    "\n",
    "So I can determine the use of ridge regression to regularize the model has produced a slight increase in overall variance but also a slight increase in bias of output prediction. Ridge regression should reduce the variance and if not, I probably need to adjust the 'degrees of freedom' hyperparameter for the cost function for the Polynomial model which feeds into it, or tune the 'alpha' hyperparameter in the actual ridge regression model itself. Tuning these inputs ...........\n",
    "\n",
    "I have decided to see if I can improve the model's predictive power by electing to use a Decision Tree Regression model: \"https://github.com/lynstanford/machine-learning-projects/tree/master/machine-learning/decision_tree.ipynb\".\n",
    "\n",
    "# Print dependencies\n",
    "Dependencies are fundamental to record the **computational environment**.   \n",
    "\n",
    "- Use [watermark](https://github.com/rasbt/watermark) to print version of python, ipython, and packages, and characteristics of the computer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,watermark,tarfile,urllib3,matplotlib,seaborn,sklearn,pickle5\n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Template"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import linear_model\n",
    "\n",
    "# The training set\n",
    "datapoints = np.array([\n",
    "    [12.5,70,81.32], [25,70,88.54], [37.5,70,67.58], [50,70,55.32], \n",
    "    [62.5,70,56.84], [77,70,49.52], [0,11.5,71.32], [77,57.5,67.20], \n",
    "    [0,23,58.54], [25,46,51.32], [37.5,46,49.52], [0,34.5,63.22], \n",
    "    [25,34.5,48.32], [37.5,34.5,82.30], [50,34.5,56.42], [77,34.5,48.32], \n",
    "    [37.5,23,67.32], [0,46,64.20], [77,11.5,41.89], [77,46,55.54], \n",
    "    [77,23,52.22], [0,57.5,93.72], [0,70,98.20], [77,0,42.32]\n",
    "    ])\n",
    "X = datapoints[:,0:2]\n",
    "Y = datapoints[:,-1]\n",
    "\n",
    "# 5 degree polynomial features\n",
    "deg_of_poly = 5\n",
    "poly = PolynomialFeatures(degree=deg_of_poly)\n",
    "X_ = poly.fit_transform(X)\n",
    "# Fit linear model\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(X_, Y)\n",
    "\n",
    "# The test set, or plotting set\n",
    "N = 20\n",
    "Length = 70\n",
    "predict_x0, predict_x1 = np.meshgrid(np.linspace(0, Length, N), \n",
    "                                     np.linspace(0, Length, N))\n",
    "predict_x = np.concatenate((predict_x0.reshape(-1, 1), \n",
    "                            predict_x1.reshape(-1, 1)), \n",
    "                           axis=1)\n",
    "predict_x_ = poly.fit_transform(predict_x)\n",
    "predict_y = clf.predict(predict_x_)\n",
    "\n",
    "# Plot\n",
    "fig = plt.figure(figsize=(16, 6))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "surf = ax1.plot_surface(predict_x0, predict_x1, predict_y.reshape(predict_x0.shape), \n",
    "                        rstride=1, cstride=1, cmap=cm.jet, alpha=0.5)\n",
    "ax1.scatter(datapoints[:, 0], datapoints[:, 1], datapoints[:, 2], c='b', marker='o')\n",
    "\n",
    "ax1.set_xlim((70, 0))\n",
    "ax1.set_ylim((0, 70))\n",
    "fig.colorbar(surf, ax=ax1)\n",
    "ax2 = fig.add_subplot(122)\n",
    "cs = ax2.contourf(predict_x0, predict_x1, predict_y.reshape(predict_x0.shape))\n",
    "ax2.contour(cs, colors='k')\n",
    "fig.colorbar(cs, ax=ax2)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}