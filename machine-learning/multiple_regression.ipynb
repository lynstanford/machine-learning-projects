{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression of Cryptocurrency\n",
    "\n",
    "The following model I have developed to demonstrate how to extract, transform and load data into a pipeline to solve a Multiple Regression problem. I aim to forecast Bitcoin (C$) prices by extrapolating historical data, training, testing and validating the model before evaluating its accuracy using a benchmark score.\n",
    "\n",
    "I am looking to predict the closing price (dependent variable) based on its relationship with other intraday price and volume information (independent variables).\n",
    "\n",
    "## Performance Measure Selection\n",
    "For the purpose of a multiple regression model I am selecting the Root Mean Square Error (RMSE) to measure the overall degree of error when making my predictions. This will act as a measure of the 'Loss', or 'Cost' function and will help assess the degree of variability associated between my target and predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ RMSE(X,h) = \\sqrt {\\frac{1}{m} \\sum_{i=1}^{m}(h(x^ {i})-y^ {i})^2} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib3\n",
    "\n",
    "DOWNLOAD_URL = \"https://ca.finance.yahoo.com/quote/BTC-CAD/history?p=BTC-CAD\"\n",
    "BITCOIN_PATH = os.path.join(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "BITCOIN_URL = DOWNLOAD_URL + \"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uniform resource locater for the downloaded dataset can be changed according to any Ticker symbol listed in Yahoo Finance. The path to extract the data to can be stored in a separate variable which can then be joined to the download URL and stored in yet another container. This just makes it a little easier to reference an object for compression-decompression 'tarfiles' which may save processing time and reduce latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_bitcoin_data(bitcoin_url=BITCOIN_URL, bitcoin_path=BITCOIN_PATH):\n",
    "    os.makedirs(bitcoin_path, exist_ok=True)\n",
    "    tgz_path = os.path.join(bitcoin_path)\n",
    "    urllib3.request.urlretrieve(bitcoin_url, tgz_path)\n",
    "    bitcoin_tgz = tarfile.open(tgz_path)\n",
    "    bitcoin_tgz.extract_all(path=bitcoin_path)\n",
    "    bitcoin_tgz.close()\n",
    "\n",
    "def load_bitcoin_data(bitcoin_path=BITCOIN_PATH):\n",
    "    csv_path = os.path.join(bitcoin_path)\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works for large files which need compressing but an easier alternative for smaller file sizes (maybe less than 10,000 entries) would be to extract the dataset's URL, parse the data in CSV format and then store it as a dataframe variable using Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# import data\n",
    "bitcoin_path = \"https://ca.finance.yahoo.com/quote/BTC-CAD/history?p=BTC-CAD\"\n",
    "\n",
    "# read data into csv chosen format\n",
    "bitcoin = pd.read_csv(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "\n",
    "#load data into dataframe\n",
    "df = pd.DataFrame(bitcoin).dropna(axis=0)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by loading the Canadian denominated Bitcoin data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad = load_bitcoin_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column represents a different feature all of which are listed below. These features are also known as 'attributes', 'input', or 'predictor' variables which can be used to ascertain a stock price prediction (called 'label', 'output', or 'target' variables). Just to be clear, once a 'target' variable is chosen for output, we now have a whole columnal vector containing elements or values for that particular feature which can be used in the linear regression model. These values are used as 'labeled' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stock info\n",
    "print(btc_cad.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to explain briefly, all the columns in this table contain float variables except for the 'Date' column which contains string objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "### Overall Statistics\n",
    "Using the describe() method to view summary statistics helps define the average values or range of data involved in this dataset, as well as a measure of dispersion about its mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can tell the average closing price is C$29,865 to the nearest integer. \n",
    "\n",
    "There isn't much data to work from (one years worth exactly), but I would like to create a dataframe object from the price data and try and use it to forecast future price information. Taking a look at which columns are present in the dataset gives me:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now lets see the first and last 5 entries in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the tail of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data\n",
    "A brief look at the summary info for the dataset tells me there are a different number of date values compared to the rest of the column entries. This will need to be addressed, followed by any missing values, NaN, 'isnull' or entries with 'zero'. I can also see the different number 'count' for entries using the count() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I just want to see what shape the dataframe is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it's 366 rows by 7 columns or a (366x7) matrix. Next, to find the total number of missing entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin_missing_values_count = btc_cad.isnull().sum()\n",
    "bitcoin_missing_values_count[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further examination allows me to calculate the total number of rows containing missing data points as a percentage of the total number of rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_bitcoin = np.product(btc_cad.shape)\n",
    "total_missing_bitcoin = bitcoin_missing_values_count.sum()\n",
    "\n",
    "percent_missing_values = (total_missing_bitcoin/total_bitcoin) * 100\n",
    "print(percent_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this I can see that just under 1% of the total values are missing. I can either choose to leave them out completely (as they may not really affect my model for the purpose of this exercise) or use imputed values such as the mean or median.\n",
    "\n",
    "### Removing Rows\n",
    "Having checked the csv file I can determine that there is no real reason for the missing values (null entries) for these dates. At first I thought these missing values might be due to public holidays but because Bitcoin is a currency, it appears to be traded OTC (Over The Counter) beyong normal trading hours throughout the day, seven days a week. As a result I have chosen to leave these missing values out using the dropna() method to eliminate them. This should provide a total of 162 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the rows with missing or 'na' entries using 'axis=0' for the row dimension\n",
    "btc_cad = btc_cad.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the new shape of the data frame\n",
    "btc_cad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time there are exactly 362 rows for each column. It's important to set the argument (axis=0) otherwise dropping the values doesn't work. \n",
    "\n",
    "### Removing Columns (Feature Selection)\n",
    "\n",
    "Having analyzed the CSV file I can see the values in the 'Adj Close' column are exactly the same as in the 'Close' column, so I will remove 'Adj Close' completely from the dataset in an attempt to regularize the model. This is unecessary data which can be removed as it could contribute to bias and bleed into the test data. It's better to reduce the overall number of variables particularly removing those which are not relevant (the process of regularization to simplify the model and avoid overfitting).\n",
    "\n",
    "Taking a look at all the columns in the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btc_cad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the unwanted columns\n",
    "del btc_cad['Adj Close']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to achieve this would be to use the drop method()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the new dataframe features\n",
    "btc_cad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(btc_cad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "The mean value for the Bitcoin (CAD) close price is C$ 29,864.75 and from the summary statistics using 'describe()' I can also see the maximum values in the upper-quartile range have been tested regularly.\n",
    "\n",
    "As there are no dividends or stock splits because Bitcoin is a currency (medium of exchange / store of value), only 6 columns are of any use to me in this particular dataset: Date, Open, High, Low, Close and Volume. Printing the first 5 lines of the dataframe gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(btc_cad[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will store the independent column variables (features) in a newly named Pandas dataframe called 'bitcoin' which can be used for further analysis and is something which is just easier to remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-naming the btc_cad dataframe as a new variable 'bitcoin'\n",
    "bitcoin = btc_cad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(btc_cad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bitcoin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation (Multivariate Stats)\n",
    "The first stage of any feature selection process for a linear regression model should involve checking multivariate statistics and the degree of correlation between the features and also with the target variable. This multivariate statistic is essential to avoid poor performance as high correlations between variables can prevent the model's loss function from converging to the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correlation = bitcoin.corr()\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would expect the opening and closing prices to be defined by the range of the high and low daily prices so all of these values tend to move within a fairly narrow band. There is a strong degree of positive correlation between these time-series values. I can also determine there is a less powerful degree of association between the volume of currency traded and its closing price (0.582709), but a positive relationship nonetheless. \n",
    "\n",
    " Assuming I wanted to find the top three highest correlated features I could use the .abs() and .nlargest() methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bitcoin.corr().abs().nlargest(3, 'Close').index)          # to find the top 3 features with highest correlation values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bitcoin.corr().abs().nlargest(3, 'Close').values[:,3])         # top 3 correlation values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, the perfectly correclated value of 1.0 represents the 'Close' price correlation with itself, but the nest two correlation values of 0.99918803 and 0.99873489 represent those of the daily 'High' and 'Low' resepectively.\n",
    "\n",
    "Remember, this would only be necessary if I wanted to include only those features with the top 3 correlation scores. For the purpose of this model that would be the open, high and low prices but I want to include the volume also and have decided to keep all 4 features. Next I intend to look at a scatter plot to show the relationship between some of these variables starting with the daily high and close prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bitcoin['High'], bitcoin['Close'], marker='o')\n",
    "plt.xlabel('High')\n",
    "plt.ylabel('Close')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(bitcoin['Low'], bitcoin['Close'], marker='o', color='green')\n",
    "plt.xlabel('Low')\n",
    "plt.ylabel('Close')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(bitcoin['Open'], bitcoin['Close'], marker='o', color='orange')\n",
    "plt.xlabel('Open')\n",
    "plt.ylabel('Close')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strong positive linear relationship exists between the High, Low and Open prices with the Close but the degree of correlation decreases with High, Low and Open prices respectively. Also this can be checked against the 4th row of the correlation matrix under the row with 'Close' in the entry.\n",
    "\n",
    "And finally the relationship plot of daily volume against the closing prices. I can see there is one outlier on the right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(bitcoin['Volume'], bitcoin['Close'], marker='o', color='coral')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Close')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there definitely is a relationship between 'Volume' and 'Close' but it doesn't appear to be very linear. It's all confined to a group with an outlier around the C$60,000 mark which represents a spike in contracts traded. Another problem I can see on the plot is the scale of the axis containing the 'Volume' values which indicates the need for feature normalization to address any adverse effects these values might have on the model training process.\n",
    "\n",
    "## Feature Normalization\n",
    "I first need to define the values of the numeric columns in the dataset to a common scale using MinMaxScaler(). This will make data exploration visualizations easier to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformation will shift the values to a range between 0 and 1 for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove row entries which aren't available\n",
    "bitcoin = bitcoin.dropna(axis=0)\n",
    "\n",
    "# assign feature columns\n",
    "bitcoin.columns = ['Date','Open','High','Low','Close','Volume']\n",
    "\n",
    "# convert data type from string to date format\n",
    "bitcoin['Date'] = pd.to_datetime(bitcoin['Date'])\n",
    "bitcoin.set_index('Date', inplace=True)\n",
    "\n",
    "print(bitcoin['Date'])\n",
    "'''\n",
    "# convert entries to float type\n",
    "X = bitcoin.values.astype(float)\n",
    "\n",
    "# define min max scaler\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# transform data\n",
    "X_scaled = min_max_scaler.fit_transform(X)\n",
    "\n",
    "bitcoin = pd.DataFrame(X_scaled, columns=bitcoin.columns)\n",
    "print(bitcoin)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "Let's take a look at plotting the different price data over time. This first plot will have the different OHLC prices for the last year. For the purpose of displaying this particular line plot I will remove the 'Adj Close' because it's not needed and the 'Volume' feature as the scale is completely different to that of the OHLC prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the seaborn library\n",
    "import seaborn as sns\n",
    "\n",
    "# Location of the file path\n",
    "#bitcoin_path = os.path.join(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "\n",
    "# Open the file into a variable bitcoin\n",
    "#bitcoin = pd.read_csv(bitcoin_path, index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# Plot the width and height of the figure\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Add a decsriptive title\n",
    "plt.title(\"Daily Bitcoin OHLC Prices in $CAD\")\n",
    "\n",
    "# Line chart showing daily global streams of each song \n",
    "sns.lineplot(data=bitcoin.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance I can detect an underlying trend for the price of Bitcoin, but there is also a noticable spike in Feb 2021 before reverting back to the trend. This makes the price data easier to read once the width and height of the axes have been adjusted. If I want to create a subplot of the 'Close' price (as the target variable I will eventually use), then I may decide to list the columns first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(bitcoin.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once again setting the width and height of the figure\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "# Add title\n",
    "plt.title(\"Daily Bitcoin Close Price in $CAD\")\n",
    "\n",
    "# Line chart showing daily 'Close'\n",
    "sns.lineplot(data=bitcoin['Close'], label=\"Close\")\n",
    "\n",
    "# Add label for horizontal axis\n",
    "plt.xlabel(\"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So as we can see there is a definite gradual upward trend. In this particular instance the plot exhibits a price pattern called an 'ascending triangle' which is a term used by technical analysts. This term can be used to make buying and selling decisions based on the shape of the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Frequency Distributions and Data Dispersion\n",
    "\n",
    "Taking a look at the frequency distributions of the different prices can tell me about their distributions about the average.\n",
    "\n",
    "I know that earlier in the timeframe captured there were a large number of cumulative values within the ten to twenty thousand dollar range. The extra weighting given to these observations have obviously caused a fairly low mean value of approximately thirty thousand dollars, but the cryptocurrency finished up at 71.5 thousand dollars by April 2021. The overall range of price values has been divided into 20 separate bins for some clarity, so I can view the frequency or number of daily prices that fall within each interval category price range or bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "import seaborn as sns\n",
    "\n",
    "# create style\n",
    "style.use(\"seaborn-darkgrid\")\n",
    "\n",
    "# set width and height\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Histogram of 'Close' Prices\")\n",
    "\n",
    "# Show the average daily closing price\n",
    "sns.histplot(data=bitcoin, x=bitcoin['Close'], bins=20, color='darkturquoise')\n",
    "\n",
    "# Add labels for both axes\n",
    "plt.xlabel('Close Price')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing these values in a dataframe series and applying the mean method requires a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create filepath for dataset\n",
    "btc_cad_path = os.path.join(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "\n",
    "# create a dict and then store a single column as a dataframe\n",
    "data = {'Open': bitcoin['Open'],\n",
    "        'High': bitcoin['High'],\n",
    "        'Low': bitcoin['Low'],\n",
    "        'Close': bitcoin['Close']}\n",
    "\n",
    "df = pd.DataFrame(data, columns = ['Close'])\n",
    "\n",
    "print(df)\n",
    "print (type(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, converting the DataFrame column to a Series using the squeeze() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_series = df['Close'].squeeze()\n",
    "\n",
    "print(close_series)\n",
    "print (type(close_series))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now the 'dataframe' column has been converted to a 'series' data type, I aim to find the mean of the values for 'Close' prices over the period in question and display their frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create style\n",
    "style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "# print the mean value\n",
    "print(close_series.mean())\n",
    "\n",
    "# configure the plot size\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# create a title\n",
    "plt.title(\"Frequency Distribution of Mean 'Close' Prices\")\n",
    "\n",
    "# plot the series\n",
    "close_plot = sns.distplot(close_series, bins=20, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean value is to the right of the peak on the frequency curve with a long tail containing values toward C$90,000 which exhibits positive skewness. \n",
    "\n",
    "The frequency distribution plots for the open, high, low and daily volume not only explain the range of values and the dispersion about the mean, but they also be some indication of how prices might revert back to some long term average. It would be foolish to assume that Bitcoin prices will continue the current upward trend beyond C$80,000 because the average is so low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create style\n",
    "style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "# set width and height\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Frequency Distribution of 'Open' Prices\")\n",
    "\n",
    "# Show the average daily high price\n",
    "sns.histplot(data=bitcoin, x=bitcoin['Open'], bins=20, color='turquoise')\n",
    "\n",
    "# Add labels for both axes\n",
    "plt.xlabel('Open Price')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create style\n",
    "style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "# set width and height\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Frequency Distribution of 'High' Prices\")\n",
    "\n",
    "# Show the average daily high price\n",
    "sns.histplot(data=bitcoin, x=bitcoin['High'], bins=20, color='turquoise')\n",
    "\n",
    "# Add labels for both axes\n",
    "plt.xlabel('Daily High')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create style\n",
    "style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "# set width and height\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Frequency Distribution of 'Low' Prices\")\n",
    "\n",
    "# Show the average daily low price\n",
    "sns.histplot(data=bitcoin, x=bitcoin['Low'], bins=20, color='turquoise')\n",
    "\n",
    "# Add labels for both axes\n",
    "plt.xlabel('Daily Low')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create style\n",
    "style.use(\"seaborn-whitegrid\")\n",
    "\n",
    "# set width and height\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title(\"Frequency Distribution of Daily Volume\")\n",
    "\n",
    "# Show the average daily volume\n",
    "#sns.histplot(data=bitcoin, x=bitcoin['Volume'], bins=20, color='limegreen')\n",
    "sns.kdeplot(data=bitcoin, x=bitcoin['Volume'], color='limegreen', shade=True)\n",
    "\n",
    "# Add labels for both axes\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the daily volume of transactions rarely exceeds 100 billion but there are a few exceptions. \n",
    "\n",
    "Given the mean values for OHLC prices, another way to find out how many values are above or below the average would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin['Open'].loc[bitcoin['Open'] > 29707.22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_o = (121 / 162) * 100\n",
    "print(tot_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin['High'].loc[bitcoin['High'] > 30555.68]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 121 values greater than the mean daily opening price. Calculating this as a percentage of the total number of observations gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_h = (123 / 162) * 100\n",
    "print(tot_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this step for the other price attributes gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin['Low'].loc[bitcoin['Low'] > 28874.21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_l = (122 / 162) * 100\n",
    "print(tot_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bitcoin['Close'].loc[bitcoin['Close'] > 29864.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_c = (121 / 162) * 100\n",
    "print(tot_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can tell for all of these price categories that roughly 75% of the values lie above the mean which is another way to show their frequency distributions are positively skewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Choosing the right data columns (features) for the Input set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# import data\n",
    "bitcoin = pd.read_csv(\"C:/Users/lynst/Documents/GitHub/machine-learning-projects/machine-learning/BTC_CAD.csv\")\n",
    "df = pd.DataFrame(bitcoin).dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the data in the first couple of rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all column names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all column data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I want to split the data into predictors and a target variable, containing all my feature columns in one matrix or Input variable (X) and the Target or Output variable in a column vector (y). Assigning the dependant, or y variable for the modelling process to the daily close price information (which will represent the labelled data), I have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Close']\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are a selection of columns used to predict 'y', also known as the independent variables. I am choosing to leave the 'Date' and 'Adj Close' columns out of the X variable dataframe.\n",
    "\n",
    "Note, I can either store the individual features in a variable which can be referenced or called when performing some function, or I can store the exact feature names as a list in the dataframe. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the feature columns list\n",
    "bitcoin_features = ['Open','High','Low','Volume']\n",
    "print(bitcoin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features as dataframe\n",
    "X = df[bitcoin_features]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative way\n",
    "X = df[[\"Open\", \"High\", \"Low\", \"Volume\"]]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple of important things to note here. Firstly because I already dropped not-available row entries (3), there are 362 correct entries spanning 365 rows which is correct. I don't need to perform this dropna() method on X and y individually because I have already applied this operation to the dataframe (df).\n",
    "\n",
    "Secondly, I can see that referencing the features and storing them in a separate variable named 'bitcoin_features' really only comes in handy when there are a large number of features, perhaps too many to type into a list; but for the purpose of this exercise I prefer entering each feature name individually.\n",
    "\n",
    "## Splitting the Data\n",
    "Split the data into training and test sets with a 70-30 split but not without making a copy of the dataframe first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the shape of the training sets for both the X matrix and y vector gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the shape of the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can see that 253/362 * 100 = 70% and 109/362 * 100 = 30% for both the train and test sets respectively. Next I will save a copy of the dataframe to use.\n",
    "\n",
    "# Model Selection (Linear Regression)\n",
    "This enables me to explore the linear nature of the relationship between the input and output data and fit a regression line using a scatter plot.\n",
    "\n",
    "## Training the Data and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "linear_regression = LinearRegression()\n",
    "\n",
    "# fit model\n",
    "linear_regression.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the 'training' data has been fit, try making a prediction on the first 4 values in the test set first. This includes all the values in row one of X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_predictions = linear_regression.predict(X_test)\n",
    "print(\"Predictions: \", linear_regression.predict(X_test.iloc[:4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing these values to the actual test set values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to predict the close price based on the values from line 163 in the table above\n",
    "linear_regression.predict([[14354.31, 14506.5, 13948.08, 3.611139e+10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this forecast value for y to the actual value for y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = 14115.09\n",
    "\n",
    "# Assuming y = a + b1x1 + b2x2 + b3x3 + b4x4\n",
    "# where:\n",
    "# a = intercept (bias term)\n",
    "# b = coefficient value\n",
    "# x = feature value\n",
    "\n",
    "y = 107.8575212006981 + (-4.38810208e-01*14354.31) + (9.50132416e-01*14506.5) + (4.76889709e-01*13948.08) + (-2.42699860e-10*3.611139e+10)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I can see that C$ 14,115.09 is reasonably close to the predicted value for y, but not that great! The Linear Regression prediction value is almost identical to my prediction model for y so I know the formula works.\n",
    "\n",
    "## Metrics\n",
    "So checking the values against the BTC_CAD.csv dataset I can see they are not exactly accurate. One way to check is introduce an accuracy score called MSE (mean squared error), but first I will check the R-squared measure to establish the overall degree of fit to the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"R-squared: \", linear_regression.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly high score and I can see this relationship in a scatter plot showing actual prices against predicted prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(y_test, price_predictions)\n",
    "print(mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)\n",
    "\n",
    "# set the width and height of the plot\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "# visualizing the relationship between actual and predicted values for y\n",
    "plt.scatter(y_test, price_predictions)\n",
    "plt.xlabel(\"Actual Prices\")\n",
    "plt.ylabel(\"Predicted Prices\")\n",
    "plt.title(\"Actual Prices vs Predicted Prices\")\n",
    "\n",
    "# fit a regression line between high and low values to show linear nature\n",
    "sns.regplot(x=y_test, y=price_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the intercept and coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(linear_regression.intercept_)\n",
    "print(linear_regression.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output provides me with the intercept value which intersects the y-axis at 107.85 and the respective coefficient values for each independent variable from x1 to x4 (open, high, low and volume values).\n",
    "\n",
    "# Evaluation\n",
    "My first look at the results of the scores and their accuracy measures prompted me to re-visit the code and try and introduce more efficiency (reducing the number of lines), but also more consistency such as replicating similar steps for the use of different types of model. So even though I started storing the same dataset in different variables it can quickly become confusing and simultaneously very important to ensure some similarity exists with regard to training, testing and evaluation/performance measurement among the different models. \n",
    "\n",
    "These procedures will require more modification when I start to apply Polynomial Regression and tackle any problems attributed to over or under-fitting of data with the application of regularization.\n",
    "\n",
    "Initial linear regression rmse score = 689.1925598643533\n",
    "R-squared accuracy = 0.9991392014437468\n",
    "\n",
    "In conclusion, the model requires a much larger dataset and would benefit from a larger time period of price information, perhaps dating back to it's initial inception (2010 - not that long ago). Because the dataset is too small it falls prey to 'Sampling Noise' which is the degree of error from sampling only one subsection of the population. The model could be improved if other sample sizes were used or if the entire population of data for Bitcoin prices was used. \n",
    "\n",
    "Regularization implies reducing the number of features or creating a more efficient use of feature engineering to counteract overfitting the data and should be applied to improve this problem. As well as more price data another way to achieve better generalization on unseen data may involve using features from substitute investments or alternative crypto-currencies. It might pay to include market indices relevant to the sector, or even some form of sentiment analysis from market news which could be interpreted numerically using a form of one-hot encoding. \n",
    "\n",
    "This model could potentially incorporate the risk from substitute investments (Dogecoin for example) which are direct competitors or alternative investments (such as Gold) which are a vital component of several investment portfolios, the risk associated with the industry or market sector, political risk associated with foreign and domestic policy, or even corporate risk associated with management level decisions such as whether or not Blockchain technology could be under threat of being potentially unravelled by Quantum computing. \n",
    "\n",
    "All of these components combined would no doubt provide a better overall picture and contribute to more accurate price discovery in the Crypto markets. Next, I will try to fit a forecast line using a Polynomial Regression model but for now let's print out dependencies which allow this notebook to be reproducable across different platforms so others who are interested can view these findings.\n",
    "\n",
    "# Print dependencies\n",
    "Dependencies are fundamental to record the **computational environment**.   \n",
    "\n",
    "- Use [watermark](https://github.com/rasbt/watermark) to print version of python, ipython, and packages, and characteristics of the computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,watermark,tarfile,urllib3,matplotlib,seaborn,sklearn,pickle5 \n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
