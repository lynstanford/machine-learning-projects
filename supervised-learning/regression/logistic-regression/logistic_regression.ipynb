{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89502158",
   "metadata": {},
   "source": [
    "# Classification Problem\n",
    "Next I will develop a Logistic Regression model to predict different classes. More specifically Logistic Regression is used to estimate the probability that an instance, element or observation belongs to a certain class. The use of one of the most popular collections of information for the purpose of classification is the Titanic dataset and the model I have developed will be submitted to Kaggle's 'Titanic - Machine Learning from Disaster' competition.\n",
    "\n",
    "The purpose of this model is to identify if these passengers 'Survived' or 'Not' which will involve creating a target output column populated with simple binary results of '1' or '0'.\n",
    "\n",
    "1. Explore and clean the data\n",
    "2. Split data into train / validation / test\n",
    "3. Fit an initial model and evaluate\n",
    "4. Tune hyper parameters\n",
    "5. Evaluate on validation set\n",
    "6. Final model selection and evaluation on test set\n",
    "\n",
    "## Import the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4046e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, precision_recall_curve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863215b",
   "metadata": {},
   "source": [
    "## Ingest the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c645fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic = pd.read_csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/logistic-regression/titanic_data.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba7f3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# check the column names and data types\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89cd1b",
   "metadata": {},
   "source": [
    "So I can determine there are a total of 183 entries in this dataset. Initial thoughts are that it might be worth using a more comprehensive dataset, one which might contain the full list of passengers (1309) rather than just a subset (183). This is the most comprehensive list available for the purpose of this exercise that I can find, although estimates for the total number of passengers and crew members are thought to be in the region of 2220. The most comprehensive datasets might be Encyclopedia Titanica and Wikipedia, both of which can be found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a793085",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# importing once again\n",
    "titanic = pd.read_csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/logistic-regression/titanic.csv',\n",
    "                     header=0,\n",
    "                     names = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin',\n",
    "                              'Embarked','WikiId','Name_wiki','Age_wiki','Hometown','Boarded','Destination','Lifeboat','Body',\n",
    "                              'Class'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4cab9",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Removing unwanted columns and rows and feature engineering is the next important step. Straight away I can see the second dataset I have imported from Kaggle which I have named 'titanic.csv' has a more comprehensive number of entries but also contains 21 columns as opposed to just 12 in the first set. Time to establish which of these columns will be kept or removed using some dimensionality reduction and combination, before establishing what is to be included in a Pandas DataFrame table and target Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00b52d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# check the column names and data types\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ebf6b",
   "metadata": {},
   "source": [
    "I can remove 'PassengerId', 'Name', 'Age', 'Ticket', 'Embarked', 'WikiId', 'Name_wiki', 'Hometown', 'Destination', 'Lifeboat', 'Body', 'Fare' and 'Class' which will significantly reduce clutter in my table as these features provide no causal relationship with passenger Survival, some of which also represent duplicated information such as passenger class 'Pclass' and 'Class'. This initial step of reducing the size helps provide a much more useful dataset overall.\n",
    "Next, let's determine the index and column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429c6d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343dee99",
   "metadata": {},
   "source": [
    "This tells me there are 1309 rows and 21 columns. Also, another way to find the number of entries or range of the index would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0088fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1db80",
   "metadata": {},
   "source": [
    "So the index starts at 0 and ends at 1308, a total of 1309 passengers (not including crew members), but in terms of the data entries in this table only 891 are labeled with target predictions. The model will be applied to the labeled data first, followed by the unlabeled data (418 entries).\n",
    "\n",
    "### Some Descriptive Stats\n",
    "Several of these values can come in handy in case I need to impute any averages for missing values later. Bear in mind this only contains data for columns with numeric values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a76df",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f6ae5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f3f23-04f8-48aa-9260-f6bccfca79f4",
   "metadata": {},
   "source": [
    "### Dropping Columns\n",
    "Store a copy of these columns in a new variable or dataset so I don't overwrite the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36861d20",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic_new = titanic[['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked',\n",
    "                       'WikiId','Name_wiki','Age_wiki','Hometown','Boarded','Destination','Lifeboat','Body','Class']]\n",
    "titanic_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7912b",
   "metadata": {},
   "source": [
    "The easiest way to drop the columns not required is to create a new subset of data (2d array) with the specified columns to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0fdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic_reduced = titanic_new[['Survived','Pclass','Sex','SibSp','Parch','Cabin','Age_wiki','Boarded','Fare']]\n",
    "titanic_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838ec81",
   "metadata": {},
   "source": [
    "I am using 'Age_wiki' from the Wikipedia web site which appears to be a much more comprehensive set of data than that of the 'Age' column. The 'Boarded' column has nominal data which I will endeavour to convert to numeric values so each port a passenger embarks from will be represented by a number instead. This will help provide more uniform data types. 'Sex' can also be converted to 0's or 1's for the purpose of this exercise and the 'SibSp' and 'Parch' columns can be combined using feature extraction to concatenate the size of families into a new series. The 'Cabin' data will be converted to binary integer values for simplicity and the 'Pclass' (Passenger Class) observations are already denoted as integer values. These appear to have a significant impact on passenger survival so I'm including them.\n",
    "\n",
    "Reducing the number of features is called dimensionality reduction and is an important technique used to achieve comparable results in a much faster time frame (with little benefit to performance accuracy), but generally works better with much larger datasets. The removal of all the unwanted columns contained in the modified dataset variable helps speed the model up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fce98a-ea68-40cf-9abe-8fdd5dfcf82b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Partial or subset of titanic dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Survived': pd.Series(titanic_reduced['Survived']),\n",
    "    'Pclass': pd.Series(titanic_reduced['Pclass']),\n",
    "    'Sex': pd.Series(titanic_reduced['Sex']),\n",
    "    'SibSp': pd.Series(titanic_reduced['SibSp']),\n",
    "    'Parch': pd.Series(titanic_reduced['Parch']),\n",
    "    'Cabin': pd.Series(titanic_reduced['Cabin']),\n",
    "    'Age_wiki': pd.Series(titanic_reduced['Age_wiki']),\n",
    "    'Boarded': pd.Series(titanic_reduced['Boarded']),\n",
    "    'Fare': pd.Series(titanic_reduced['Fare'])\n",
    "})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95088b36",
   "metadata": {},
   "source": [
    "Which columns or features are left now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fd0dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d01436",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "### Missing Values\n",
    "Next it's really important to remove or impute any Null or missing values. This depends on any row values which are missing and also on the data type for each column. Calculating the total number of missing or Null values across the entire 'titanic' dataset gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853e73b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic_missing = pd.isnull(titanic).sum()\n",
    "print(titanic_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a98e4e",
   "metadata": {},
   "source": [
    "More specifically, to narrow my workable dataset down and find the total number of missing values from the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b4661-3a16-4ef4-8982-64fea16148cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df_missing = pd.isnull(df).sum()\n",
    "print(df_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae186e",
   "metadata": {},
   "source": [
    "Next it becomes useful to determine if these missing values occurred because they weren't recorded or because there was no information for them? Assessing this output I can determine that the null values in 'Cabin' simply represent those who did not have a cabin for sleeping quarters and so these would not have been recorded. These passengers would have traveled in other areas of the ship so it's important not to drop these values as they represent important data and account for over three quarters of the overall number of passengers in this particular set. \n",
    "\n",
    "There are also 5 null values for the 'Boarded' column so for whatever reason these passengers did not have their boarding locations recorded. It's impossible to really know which port location these individuals departed from so I can either leave the values as NaN or remove each of these 5 entries as a value should exist if they boarded legally and other attributes were recorded, e.g. Name, Class, Cabin or even Age.\n",
    "\n",
    "### Null Values for Age\n",
    "The 'Age-wiki' feature records the ages provided by passengers when purchasing their tickets so it was likely based on the D.O.B. in their travel documents or passports. Taking a look at the total number of Null or missing values for the 'Age_wiki' column first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0dd9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_age_null = df['Age_wiki'].isnull().sum()\n",
    "print(num_age_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa7427",
   "metadata": {},
   "source": [
    "Identifying each row in the dataframe which contains a null value for 'Age_wiki'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84040cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[df['Age_wiki'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831c69a",
   "metadata": {},
   "source": [
    "I can make a decision whether to include these 7 passengers and merely impute some average age for their respective 'Sex', impute an average based on the overall mean for both genders, or remove them completely. Seeing as the majority of information for each of these passengers (roughly 4/7ths to 5/7ths) is present I would prefer to keep these entries, so imputing mean values for age based on the individuals sex may be a reasonably accurate average.\n",
    "\n",
    "The mean age for everyone, regardless of sex is 29.88 according to the describe() method above. Another way to find the overall average age is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eadf4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.groupby(df['Age_wiki'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30320c8",
   "metadata": {},
   "source": [
    "This overall mean or average may not be as accurate as calculating the average age for both male and female passengers and imputing them into the 7 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb8773",
   "metadata": {},
   "source": [
    "### Calculate Average Age\n",
    "Calculating the average age for male and female passengers in the table can be done by summing each individual age (by sex) and dividing by the total number of male or female passengers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91befada",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# find the total count of each of the two unique values in the Sex column (total male or female passengers)\n",
    "df['Sex'].value_counts().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade4a8c",
   "metadata": {},
   "source": [
    "This is based on all 1309 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97832f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df.Survived[df.Sex == 'male'].value_counts().plot(kind='bar', alpha=0.5, color='teal')\n",
    "plt.title(\"Male Survival\")\n",
    "# create style\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda95a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.Survived[df.Sex == 'female'].value_counts().plot(kind='bar', alpha=0.5, color='pink')\n",
    "plt.title(\"Female Survival\")\n",
    "# create style\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9dafa",
   "metadata": {},
   "source": [
    "So having determined the unique classes within the 'Sex' column I can further identify the number of Males and Females who survived or not. By taking the 'Survived' column and sub-dividing it according to gender it displays how women were far more likely to have survived the Titanic disaster based on the predictor variables included with this dataset.\n",
    "\n",
    "Next I want to group each category of male and female and store them in a variable called 'gender'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9c646",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gender = df.groupby(df['Sex'])\n",
    "gender.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0f27d",
   "metadata": {},
   "source": [
    "Checking the first few entries for both sexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Import Library\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Countplot\n",
    "sns.catplot(x =\"Sex\", hue =\"Survived\",\n",
    "kind =\"count\", data = df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774499a",
   "metadata": {},
   "source": [
    "Now there are two variables, one with all the male and one with all the female passengers in the Titanic dataset. There are a total of 843 male and 466 female passengers.\n",
    "\n",
    "The next step is to add these totals together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a48d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "male_total = 843\n",
    "female_total = 466\n",
    "total_passengers = male_total + female_total\n",
    "total_passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e27187",
   "metadata": {},
   "source": [
    "Summing the total of all ages for all the passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65692b42",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "sum_age = df['Age_wiki'].sum()\n",
    "print(sum_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f4be7",
   "metadata": {},
   "source": [
    "And dividing by the total number of passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c4657",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "age_ave = sum_age / total_passengers\n",
    "age_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b254b64",
   "metadata": {},
   "source": [
    "So the overall average age for all passengers calculates to just over 29 years old. Using the describe method to check this gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82928d01",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315be455",
   "metadata": {},
   "source": [
    "So the first item to notice is that only numeric data appears to have been captured which will need to be fixed soon, but the answer I was looking for now, the mean age found under the 'Age-wiki' column is 29.415829 which is close to the value just calculated of 29.258525, but not identical.\n",
    "\n",
    "Next, to see the average ages for both male and female classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29b4d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.groupby(by='Sex')['Age_wiki'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f630df",
   "metadata": {},
   "source": [
    "So this produces the mean Age by Sex. \n",
    "\n",
    "What if I wanted to find an average age just for the missing values? There are a total of 7 missing age values in the entire dataset, 4 male and 3 female. Imputing a value of 29 for the missing 'male' age and 28 for 'female' would be the most accurate solution but instead, the mean age value for all passengers will be imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65e037",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6a007",
   "metadata": {},
   "source": [
    "### Imputing Missing Age Values\n",
    "Using the 'impute' library rather than using fillna()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa2771",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use SimpleImputer function to fill in missing values\n",
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "age_imputer = imputer.fit(df[['Age_wiki']])\n",
    "# now the transform method\n",
    "df['Age_wiki'] = age_imputer.transform(df[['Age_wiki']])\n",
    "df['Age_wiki'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40098b4",
   "metadata": {},
   "source": [
    "All 'Age_wiki' observations are entered as floats but the data type needs to be changed to integer (representing years) which is technically more correct as a unit of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddd07f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['Age_wiki'] = df['Age_wiki'].astype('int')\n",
    "df['Age_wiki'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bb3a1",
   "metadata": {},
   "source": [
    "So checking the first missing age value to see what value it contains now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf29ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.Age_wiki[42:43]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a8b44",
   "metadata": {},
   "source": [
    "The average age of 29 has been imputed. Another of the missing values was located at index 1041:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aac89b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.Age_wiki[1041:1042]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdf29b",
   "metadata": {},
   "source": [
    "This also appears to be correct. \n",
    "\n",
    "To check if there are any more null values now they have been replaced with the mean age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd11cc6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# check if Age_wiki has any more null values\n",
    "df['Age_wiki'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357a9f7-180a-4784-b9da-e3fd6c81a82e",
   "metadata": {},
   "source": [
    "This tells me all the null values in the Age column have been replaced properly. \n",
    "\n",
    "Now the missing values have been dealt with, what about the string and categorical variables in the dataset? An important consideration to make when using visualizations would be the data types involved. For example, information can be split into numeric (quantitative) data and categorical (qualitative) data. Categorical data values could be classified as Binary (such as the target outcome 'Survived', or 'Sex'), Nominal (such as 'Cabin', or 'Boarded'), perhaps even Ordinal (such as 'Pclass'). 'Age_wiki' contains continuous values and the rest such as 'SibSp' (number of Siblings or Spouse) and 'Parch' (number of Children accompanied by Parents) are discrete integer values.\n",
    "\n",
    "Having established the different different data types the next step is to convert the string objects into numeric types, starting with the data in 'Cabin' first.\n",
    "\n",
    "### Null Values for Cabin\n",
    "Checking the total number of Null values for the 'Cabin' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301626d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_cabin_null = df['Cabin'].isnull().sum()\n",
    "print(num_cabin_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f37869",
   "metadata": {},
   "source": [
    "### Convert the Cabin Feature\n",
    "Next, I want to address the issue relating to Cabin data. Because each passenger was assigned a Cabin number which is just an alpha-numeric string type, I would prefer to convert all entries for Cabin to a straight forward integer. Assigning a value of 1 for the presence of a cabin number and 0 for someone without.\n",
    "#### Creating a Binary Indicator for Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708a94c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Overwrite the Cabin values having converted them and store them in the same name\n",
    "df['Cabin'] = np.where(df['Cabin'].isnull(), 0, 1)\n",
    "df['Cabin'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e654f09",
   "metadata": {},
   "source": [
    "Now the 'Cabin' data has been converted into binary numeric values equivalent to 'cabin' or 'no cabin' and even though there are 1014 passengers who were not assigned cabins, this data will remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73789579",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c609bae8",
   "metadata": {},
   "source": [
    "### Convert the Sex Feature\n",
    "The male entries will be assigned a value of 1 and female, 0. Viewing the total number of Male passengers who didn't survive (0.0), or did survive (1.0), these can be switched to integer values also. This can be repeated for the Female passengers also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a074b00",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "gender_dict = {'male': 1, 'female': 0}\n",
    "df['Sex'] = df['Sex'].map(gender_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e3596",
   "metadata": {},
   "source": [
    "Check the dataframe again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff9bb27",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d264db-7859-479b-91b2-bb523d6d870d",
   "metadata": {},
   "source": [
    "### Null Values for Fare\n",
    "There is only one missing value. I could replace the missing fare with the overall mean or median value, but I have opted to adjust the strategy argument in the SimpleImputer() method with the 'most_frequent' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0aa19",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use SimpleImputer model to fill in missing entries with most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan)\n",
    "fare_imputer = imputer.fit(df[['Fare']])\n",
    "df['Fare'] = fare_imputer.transform(df[['Fare']])\n",
    "\n",
    "#imputer = fare_imputer.fit_transform(fare_imputer.values.reshape(-1,1))\n",
    "df[['Fare']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05298049",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.Fare.fillna(df.Fare.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162addcb",
   "metadata": {},
   "source": [
    "### Null Values for Boarded\n",
    "The last categorical feature which needs changing is the 'Boarded' column. Checking if there are null values in 'Boarded':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a8b3ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df['Boarded'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377aad39",
   "metadata": {},
   "source": [
    "Which 5 indexed rows are missing in the 'Boarded' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98924814",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# list index values in relation to this column which are missing\n",
    "boarded_values_null = df[df['Boarded'].isnull()]\n",
    "boarded_values_null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65160eb",
   "metadata": {},
   "source": [
    "Because there are only five missing entries, it makes more sense to drop these rows from the database completely as it would be difficult to calculate or impute values for the departure port.\n",
    "\n",
    "This means when it comes to preparing the dataframe to be used for our predictors, the row index numbers for these entries should be removed completely, thus deleting the information across all the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121d9c7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.drop([347, 557, 1041, 1048, 1228], axis=0, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b88b1b",
   "metadata": {},
   "source": [
    "So the dataframe has now been reduced from 1309 to 1304 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1b192e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# check if Boarded has any more missing values\n",
    "df['Boarded'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f36a4f4",
   "metadata": {},
   "source": [
    "An alternative approach would be to replace the missing values with the most frequently occurring entries using the fillna() method. Having observed these five entries in the source dataset there doesn't appear to be much information in the other columns so they probably wouldn't add much value to our model.\n",
    "\n",
    "To identify which columns are categorical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b30e236",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Get list of categorical variables from df\n",
    "cat_vars = (df.dtypes == 'object')\n",
    "object_cols = list(cat_vars[cat_vars].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814b001",
   "metadata": {},
   "source": [
    "### Convert the Boarded Feature\n",
    "Because 'Boarded' values are just 'nominal', it doesn't make sense to use 'ordinal' encoding to categorize each departure location as there is no precedent regarding importance. A more appropriate way to convert the categorical entries for the Boarded column is to change its entries into numeric values using 'One-Hot Encoding'.\n",
    "\n",
    "#### Creating a One-Hot Encoding Matrix for Boarded\n",
    "There are four possible states for the 'Boarded' column including Belfast, Queenstown, Southampton and Cherbourg which were the only departure locations listed. Each of these entries can be assigned a value of 1,2 or 3. This can be achieved using a technique called one-hot encoding.\n",
    "\n",
    "Check the values in the specific series I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f755f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "boarded_cat = df[['Boarded']]\n",
    "boarded_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50ce41",
   "metadata": {},
   "source": [
    "Apply the 'One-Hot Encoder' class from the sci-kit learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f04404",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(categories='auto', drop=None, sparse=True, dtype='int64', handle_unknown='error')\n",
    "boarded_cat_1hot = one_hot_encoder.fit_transform(boarded_cat)\n",
    "boarded_cat_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2c71a",
   "metadata": {},
   "source": [
    "Check to see if the Boarded column has changed to numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bfe1f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ab13f2",
   "metadata": {},
   "source": [
    "So the Boarded column doesn't appear to have changed. \n",
    "\n",
    "#### Use the Column Transformer\n",
    "This will extrapolate the existing data and extend the dtaa into new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d07cd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "transformer = make_column_transformer(\n",
    "    (OneHotEncoder(), ['Boarded', 'Sex']),\n",
    "    remainder='passthrough')\n",
    "\n",
    "transformed = transformer.fit_transform(df)\n",
    "updated_df = pd.DataFrame(transformed, columns=transformer.get_feature_names_out())\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8451229",
   "metadata": {},
   "source": [
    "Tidying up the column names in the new dataframe. Make sure you initiate the 'self' class argument by typing in the name of the dataframe itself (updated_df), or else the rename() method won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ff7ee8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_df = pd.DataFrame.rename(updated_df, columns={'onehotencoder__Boarded_Belfast':'Boarded_Belfast',\n",
    "                            'onehotencoder__Boarded_Cherbourg':'Boarded_Cherbourg',\n",
    "                            'onehotencoder__Boarded_Queenstown':'Boarded_Queenstown',\n",
    "                            'onehotencoder__Boarded_Southampton':'Boarded_Southampton',\n",
    "                            'onehotencoder__Sex_0':'Female',\n",
    "                            'onehotencoder__Sex_1':'Male',\n",
    "                            'remainder__Survived':'Survived',\n",
    "                            'remainder__Pclass':'Pclass',\n",
    "                            'remainder__SibSp':'SibSp',\n",
    "                            'remainder__Parch':'Parch',\n",
    "                            'remainder__Cabin':'Cabin',\n",
    "                            'remainder__Age_wiki':'Age_wiki',\n",
    "                            'remainder__Fare':'Fare'})\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6eeda6e-af95-4c6a-861d-453fa585dc1c",
   "metadata": {},
   "source": [
    "Now all the data has been converted to numeric values into a more comprehensive set the performance of the model will be improved dramatically. One interesting visual summarizing the data dispersion of all the features would be a hist plot. This provides a frequency distribution of discrete and continuous variables. \n",
    "\n",
    "For example, we can qualify discrete variables as those which occupy a specific number of states determined by 1 / k where k equals the number of possible outcomes within this universal set. Determining the likelihood of each state occurring is calculated using a probability mass function. This would include 'Survived', 'Pclass', 'Sex', 'SibSp', 'Parch' and 'Cabin' below.\n",
    "\n",
    "Continuous variables have an infinite number of possible values and act as real numbers. These values are calculated using a probability density function which does not provide the probability of a specific state, for example, 'Age_wiki' and 'Fare'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d593245",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.hist(bins=20, figsize=(20,10))          # Boarded is not included because it contains string data types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e49904e-38fd-4acf-be4f-b8c2db053d6f",
   "metadata": {},
   "source": [
    "Taking a quick glance at the frequency distributions for each column reveals some interesting information. For example there are 2 distinct categories for survival, there don't appear to be any Sibling-Spouse groups larger than 8 in total, there's an average age of roughly 20-21 years, there are 3 distinct passenger classes (1st, 2nd and 3rd), an overwhelming number of People travelling without minors and much smaller numbers of Parents travelling with just 1 or 2 children. Finally, the vast majority of fares appear to be below the 25.0 mark, but there were some almost approaching 275.0. I'm not sure if these are Schillings, Guineas or Pounds Sterling, but this data can be further researched.\n",
    "\n",
    "When it comes to visualizing these different types of data it is generally better to use scatter and line plots for numeric data, but for categorical data, frequency distributions, bar charts and histograms may be a better approach for viewing different classes or sub-sets of values.\n",
    "\n",
    "The following Seaborn plots provide a breakdown of the relationship between the three categorical variables and the rate of survival in each. One thing to note is that the survival rate for passengers with a cabin approximates 0.4 (40%), so it would seem that 0.6 (60%) of those without a cabin had survived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9d785",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(['Cabin', 'Sex', 'Boarded']):\n",
    "    plt.figure(i)\n",
    "    sns.catplot(x=col, y='Survived', data=df, kind='point', aspect=2, color=\"lightseagreen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44de954d",
   "metadata": {},
   "source": [
    "So I have the passenger survival rates from the different locations above and the percentage number of those passengers who embarked from each port, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb147d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df.Boarded.value_counts(normalize=True).plot(kind=\"bar\", alpha=0.6, color=\"darkorange\")\n",
    "plt.title(\"Boarded Location\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c65455",
   "metadata": {},
   "source": [
    "Total passengers by Boarding location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7318934d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[0:891]['Boarded'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a558c0",
   "metadata": {},
   "source": [
    "Group the different Boarding locations by the column I want information returned on which is 'Survived'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1aedb6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "df[0:891].groupby(['Boarded'], as_index=False)['Survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94db4238",
   "metadata": {},
   "source": [
    "To visualize this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a38f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Stacked barplot with pandas\n",
    "Survived = df.loc[df['Survived']==1, :]['Boarded'].value_counts()\n",
    "Died = df.loc[df['Survived']==0, :]['Boarded'].value_counts()\n",
    "df_plot = pd.DataFrame([Survived,Died])\n",
    "df_plot.index = ['Survived','Died']\n",
    "\n",
    "# Plot\n",
    "df_plot.plot(kind='bar',stacked=True, title='Survival Rate by Boarding Location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963e6954",
   "metadata": {},
   "source": [
    "This produces an interesting plot because straight away I am able to see that more people died from Southampton than the entire number of those who survived and nobody from Belfast survived at all.\n",
    "\n",
    "If I flip the x-axis variables to contain the 'Boarded' locations I can compare the number of passengers who embarked from these locations to their actual survival rate in a stacked bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833e2a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Stacked barplot with pandas\n",
    "Southampton = df.loc[df['Boarded']=='Southampton', :]['Survived'].value_counts()\n",
    "Cherbourg = df.loc[df['Boarded']=='Cherbourg', :]['Survived'].value_counts()\n",
    "Queenstown = df.loc[df['Boarded']=='Queenstown', :]['Survived'].value_counts()\n",
    "Belfast = df.loc[df['Boarded']=='Belfast', :]['Survived'].value_counts()\n",
    "\n",
    "df_plot = pd.DataFrame([Southampton,Cherbourg,Queenstown,Belfast])\n",
    "df_plot.index=['Southampton','Cherbourg','Queenstown','Belfast']\n",
    "df_plot.plot(kind='bar',stacked=True, color=['tomato','lightseagreen'], title='Survival by Boarding Port Location')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50feb8f0-82aa-45af-a98d-a68a7f3501ab",
   "metadata": {},
   "source": [
    "0.0 (tomato color) represents passengers who died and 1.0 (lightseagreen) those who survived in terms of the legend. It looks as if at least two-thirds to three-quarters of all passengers did not make it. At least 55% of overall passengers who survived had embarked from Cherbourg, at least 65% of passengers who were allocated a cabin survived and an overwhelming number of survivor's were Female.\n",
    "\n",
    "Another important table I like is the correlation matrix which does a great job of describing the relative relationships between all the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5de34ed-e86f-4338-9633-a3cb18c0619e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "correlation = updated_df.corr()\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4df848",
   "metadata": {},
   "source": [
    "Presenting a correlation matrix in visualized form we can see the darker squares represent higher positive correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c80ffe",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(correlation, annot=True, cmap='BuPu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e451cc1b",
   "metadata": {},
   "source": [
    "Looking at the Survived column it's easy to see that the highest positive correlation is 'Female', so merely being a Women would have the greatest survival rate, more than any other factor. The highest negatively correlated effect belonged to the 'Male' category. This had a much greater impact on survival rates than passenger class (Pclass) or even the fare paid so those who paid more for a first class ticket weren't necessarily secured a place on a lifeboat.\n",
    "\n",
    "### Grouping Data Together\n",
    "Taking a look at the average values for each feature based on their survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9749e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_df.groupby('Survived').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1478d",
   "metadata": {},
   "source": [
    "### Combine SibSp and Parch\n",
    "Creating a for loop to iterate through both columns and plotting them categorically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979362ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i, col in enumerate(['SibSp', 'Parch']):\n",
    "    plt.figure(i)\n",
    "    sns.catplot(x=col, y='Survived', data=updated_df, kind='point', aspect=2, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a18dca",
   "metadata": {},
   "source": [
    "An extremely useful procedure when it comes to feature engineering involves combining columns, in this case 'SibSp' and 'Parch' into a new feature called 'Family_count'. It serves a similar purpose which is to provide counts or frequency of individuals traveling with their Parents or Brothers and Sisters, combining the values together and producing dimensionality reduction by reducing the number of columns and overal noise in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f721b727",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_df['Family_Count'] = updated_df['SibSp'] + updated_df['Parch']\n",
    "sns.catplot(x='Family_Count', y='Survived', data=updated_df, kind='point', aspect=2, color=\"coral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091986d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_df['Family_Count'] = updated_df['SibSp'] + updated_df['Parch']\n",
    "updated_df.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b62502c",
   "metadata": {},
   "source": [
    "A plot of Age and Survival Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bcff93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_df.Age_wiki.value_counts(normalize=True).plot(kind=\"kde\", color=\"turquoise\")\n",
    "plt.title(\"Age and the Probability of Survival\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2d129a",
   "metadata": {},
   "source": [
    "This shows a meaningful relationship across the Age spectrum. The older ages were less likely to survive and as the age diminishes, the likelihood goes up to between 20% and 60% for those less than the age of about 13."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebcbf2",
   "metadata": {},
   "source": [
    "The next step is to prepare the dataset with the correct number of total labeled entries. I can either change the data at source and slice it using Excel, or alternatively slice the data in Python to only include the first 891 passengers in the training data because these are the only results which are labeled. The reason this needs to be done is because of the risk of feeding inaccurate and unlabeled data back into the model (i.e. entries from 892 to 1304). Using un-labeled data will introduce bias into the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7a157b",
   "metadata": {},
   "source": [
    "## Plot Continuous Features\n",
    "These are separate from discrete values and deserve some attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16d0a69",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in ['Fare','Age_wiki']:\n",
    "    died = list(updated_df[updated_df['Survived'] == 0][i].dropna())\n",
    "    survived = list(updated_df[updated_df['Survived'] == 1][i].dropna())\n",
    "    xmin = min(min(died), min(survived))\n",
    "    xmax = max(max(died), max(survived))\n",
    "    width = (xmax - xmin) / 40\n",
    "    sns.distplot(died, color='lightseagreen', kde=False, bins=np.arange(xmin, xmax, width))\n",
    "    sns.distplot(survived, color='orange', kde=False, bins=np.arange(xmin, xmax, width))\n",
    "    plt.legend(['Did not survive', 'Survived'])\n",
    "    plt.title('Histogram Showing Survival Rate v {}'.format(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52ff1f9",
   "metadata": {},
   "source": [
    "This tells me that a far greater number of passengers in the 20-30 year age category did not survive relative to other age groups. Also, it appears that those who paid a lot more had better chances of survival.\n",
    "\n",
    "What's the relationship between the continuous variables? A scatter plot can be used to describe this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6b4246",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "price = sns.scatterplot(data=updated_df, x=updated_df['Age_wiki'], y=updated_df['Fare'], color=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62aefd0",
   "metadata": {},
   "source": [
    "This explains the range of fares paid by passengers according to age."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a8d0cc",
   "metadata": {},
   "source": [
    "## Row Selection\n",
    "This is where I determine the number of instances to train for model selection and at this stage it's best to employ the use of a Train, Test, Split algorithm using the labeled data (rows 1 to 889) which contains values for the target feature entitled 'Survived'. The model will fit to and learn from this data, then I can evaluate the efficiency of the chosen model against the unseen (un-labeled) data, rows 892 to 1304 which don't contain target values for the 'Survived' column. It's important to note that because I have already pre-processed the entire dataset from titanic.csv, I will have to split the data into subsets for 'labeled' and 'unlabeled' portions and apply the LogisticRegression() model to each separately.\n",
    "### Nature of the Data\n",
    "Change all features to integer values except for the 'Fare' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f393",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# change values to integers except for Fare and Survived columns\n",
    "updated_df['Boarded_Belfast'] = updated_df['Boarded_Belfast'].astype('int')\n",
    "updated_df['Boarded_Cherbourg'] = updated_df['Boarded_Cherbourg'].astype('int')\n",
    "updated_df['Boarded_Queenstown'] = updated_df['Boarded_Queenstown'].astype('int')\n",
    "updated_df['Boarded_Southampton'] = updated_df['Boarded_Southampton'].astype('int')\n",
    "updated_df['Female'] = updated_df['Female'].astype('int')\n",
    "updated_df['Male'] = updated_df['Male'].astype('int')\n",
    "updated_df['Pclass'] = updated_df['Pclass'].astype('int')\n",
    "updated_df['Cabin'] = updated_df['Cabin'].astype('int')\n",
    "updated_df['Age_wiki'] = updated_df['Age_wiki'].astype('int')\n",
    "updated_df['Family_Count'] = updated_df['Family_Count'].astype('int')\n",
    "\n",
    "updated_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e4f9d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "updated_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a9923",
   "metadata": {},
   "source": [
    "### Labeled Dataset Only\n",
    "This set is reduced in size. It only contains instances where the target response was labeled which means only supervised learning algorithms can be applied before evaluating the result. An important point to remember here is that two 'Boarded' entries were removed between the index values of 0 and 891 (giving a total of 889 instances), so the new range of labeled entries will be between 0 and 889. The unlabeled entries span from index locations 889 to 1304.\n",
    "\n",
    "The unlabeled portion of data residing between index locations 889 and 1304 would require a different type of machine learning model, or un-supervised algorithms such as visualization through clustering techniques, dimensionality reduction or associative rule modeling. See my k_means_clustering.ipynb model... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7280cbdb",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# removing the 'Survived' column from the predictors DataFrame variable X\n",
    "X = updated_df[:889].drop('Survived', axis=1).copy()\n",
    "# assigning this dropped column to the target Series variable y\n",
    "y = updated_df[:889]['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ed30e",
   "metadata": {},
   "source": [
    "To clean up the dependent variable information, convert the y values to integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae8728",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y = pd.Series(y).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea9e8a5",
   "metadata": {},
   "source": [
    "Checking the first few instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb9f0ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3041d78e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0679c0fd",
   "metadata": {},
   "source": [
    "This is correct! The first Null value should appear at index location 889, so there are a total of 888 labeled entries. Converting these dataframes to separate csv files so they can be viewed in Microsoft Excel if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c7c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X_to_csv = X.to_csv(r'C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/logistic-regression/X.csv', index=False, header=True)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98821878",
   "metadata": {},
   "source": [
    "And the target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7460a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y_to_csv = y.to_csv(r'C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/logistic-regression/y.csv', index=False, header=True)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399473a1",
   "metadata": {},
   "source": [
    "Looking at the new shape of the predictor dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc86b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b214038",
   "metadata": {},
   "source": [
    "This is better! Now I have the labeled instances for my independent variables, it's time to train and fit the data which means it needs to be split into training and validation sets before evaluating the model's accuracy.\n",
    "\n",
    "### Split the Data\n",
    "\n",
    "Having looked at the source data file, it hasn't been split into training or test data yet.\n",
    "\n",
    "#### Single Holdout Set\n",
    "Start with a single holdout validation, or test set to evaluate how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef823643",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ec52f",
   "metadata": {},
   "source": [
    "So 889 labels (y) are split into training (533 entries), validation (178 entries) and test sets (178 entries). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043f0d1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(len(y), len(y_train), len(y_val), len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cfb9f8",
   "metadata": {},
   "source": [
    "#### Scale the Different Ranges of Values\n",
    "To improve accuracy before modeling by removing unecessary scale or unit measurement differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf700c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf0238",
   "metadata": {},
   "source": [
    "### Train the Data\n",
    "#### Logistic Regression\n",
    "Applying a Logistic Regression model to the labeled set and setting the 'multi_class' parameter equal to 'auto' because this will select a binary classifier automatically for the output array. Setting 'multi_class' equal to 'auto' is another way for the model to identify a binary problem to be fit to each label. If it had been a multi-class output then 'auto' would detect that also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8289c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import library then I can try different models\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression(multi_class='auto')\n",
    "log_reg_clf = logistic_regression.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd53d4b",
   "metadata": {},
   "source": [
    "Once the classification has been fit predictions can be made. The predict() method predicts the actual class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8d478f",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "y_pred = log_reg_clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6707872",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "Now I can evaluate the training set using the score method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d27f7f-686a-4f43-8cda-3e3f5607a50a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "log_reg_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79b9d5",
   "metadata": {},
   "source": [
    "Performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e0ed2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "log_reg_clf.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cf639",
   "metadata": {},
   "source": [
    "How does this score generalize to the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a128d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "log_reg_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641a4fc",
   "metadata": {},
   "source": [
    "Try removing the \"Cabin\" feature to see how much information is captured by the model this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68cf0b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "X = updated_df[['Boarded_Belfast', 'Boarded_Cherbourg', 'Boarded_Queenstown', 'Boarded_Southampton', 'Female', 'Male', 'Pclass',\n",
    "       'Age_wiki', 'Fare', 'Family_Count']].copy()\n",
    "X = X[0:889]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1a4893",
   "metadata": {},
   "source": [
    "Train the same model after re-scaling the values using some normalization algorithm first, then re-apply the logistic regression.\n",
    "\n",
    "#### Re-Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a36da58",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logistic_regression = LogisticRegression(multi_class='auto')\n",
    "log_reg_clf = logistic_regression.fit(X_train,y_train)\n",
    "\n",
    "log_reg_clf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700bec8b",
   "metadata": {},
   "source": [
    "Having dropped the Cabin feature there is a negligable improvement so the model captures a small incremental change in the score having removed the information from this feature. Moving on to preprocessing, it may help to train a Polynomial function to fit the curve more accurately to the data.\n",
    "\n",
    "#### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b733e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, preprocessing\n",
    "\n",
    "logistic_regression = LogisticRegression(multi_class='auto')\n",
    "log_reg_clf = logistic_regression.fit(X_train,y_train)\n",
    "\n",
    "polynomial_regression = preprocessing.PolynomialFeatures(degree=2)\n",
    "poly_features = polynomial_regression.fit_transform(X_train)\n",
    "\n",
    "log_reg_clf = logistic_regression.fit(poly_features, y_train)\n",
    "\n",
    "log_reg_clf.score(poly_features, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d0e64c",
   "metadata": {},
   "source": [
    "Again, the polynomal features model is nudging the score in the right direction by attempting to fit the Sigmoid curve more accurately to the values and reducing the overall error. Next, try an ensemble learning classifier model.\n",
    "\n",
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecf8c9c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# train the decision tree classifier and initialize the random seed parameter\n",
    "decision_tree_classifier = tree.DecisionTreeClassifier(random_state=42)\n",
    "decision_tree = decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "decision_tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c9acc9",
   "metadata": {},
   "source": [
    "Interesting score but it appears to be overfitting, so the best way to counteract this result is to apply a cross validation technique.\n",
    "\n",
    "#### Cross-Validation\n",
    "Fit and evaluate a basic model using 5-fold Cross-Validation known as K-Fold Cross Validation. The data will be divided into k subsets and the holdout set will be repeated k number of times until all the data points are used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fdf6c5",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# train the cross validation model using the decision_tree as the first argument, X and y values and iterations\n",
    "cross_val_score(decision_tree, X_train, y_train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647e8669",
   "metadata": {},
   "source": [
    "This is extremely useful as cross-validation will output score based on K number of folds, in this example 5 folds, which is directly controlled by the parameter cv=5. The highest accuracy score was based on the last 'test' subset of values 82.07%, however, when using K-Folds CV the average score is taken so inputting the 5 scores above and calculating the mean gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd4b4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "(0.81308411 + 0.80373832 + 0.74766355 + 0.80188679 + 0.74528302) / 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e9010640",
   "metadata": {},
   "source": [
    "So I can see that 78.23% is slightly worse than the previous score from the 'PolynomialFeatures', but is probably a more accurate reflection of the score() function. The overfitting in the DecisionTree is a result of low bias and high variance so this needs to reach some kind of equilibrium and the only way optimize the solution is to adjust parameters in this algorithm.\n",
    "\n",
    "Checking the parameters or attributes which can be adjusted using the PolynomialFeatures() model I will first try changing the 'degree' parameter to include a tuple for the 'min_degree' and 'max_degree' values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52bd37",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'base (Python 3.9.13)' due to connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, preprocessing\n",
    "\n",
    "logistic_regression = LogisticRegression(multi_class='auto')\n",
    "log_reg_clf = logistic_regression.fit(X_train, y_train)\n",
    "\n",
    "polynomial_regression = preprocessing.PolynomialFeatures(degree=(2,4))\n",
    "poly_features = polynomial_regression.fit_transform(X_train)\n",
    "\n",
    "log_reg_clf = logistic_regression.fit(poly_features, y_train)\n",
    "\n",
    "log_reg_clf.score(poly_features, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ad533f",
   "metadata": {},
   "source": [
    "### Leave 1-Out\n",
    "This should only really be used for really small datasets when I could leave one value or observation out for validation after each training iteration but it would take too long for 889 instances.\n",
    "\n",
    "### Confusion Matrix\n",
    "Using a Confusion Matrix is a much better approach to measuring accuracy for a Classifier model such as Logistic Regression. This will display 'Actual' v 'Predicted' values. The left hand rows display 'Actual' negative and positive classes, and the colums display 'Predicted' negative and positive classes. It's important to make sure the length of the target series' are exactly the same for actual (y_test) and predicted (y_pred) values. \n",
    "\n",
    "It won't work by testing training data (y_train) because these account for 533 total instances, rather than 178 instances in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae704cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "logistic_regression = LogisticRegression(multi_class='auto')\n",
    "log_reg_clf = logistic_regression.fit(X_train,y_train)\n",
    "\n",
    "# creating y_train_pred values:\n",
    "y_train_pred = log_reg_clf.predict(X_train)\n",
    "y_train_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4596c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score\n",
    "\n",
    "conf = confusion_matrix(y_train, y_train_pred)\n",
    "print(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5eff1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score\n",
    "\n",
    "prec = precision_score(y_train, y_train_pred)\n",
    "print(f\"The Precision Accuracy Score is: \", prec)\n",
    "rec = recall_score(y_train, y_train_pred)\n",
    "print(f\"The Recall Accuracy Score is: \", rec)\n",
    "roc_auc = roc_auc_score(y_train, y_train_pred)\n",
    "print(f\"The Receiver Operation Characteristic-Area Under Curve Score is: \", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7e6fb4",
   "metadata": {},
   "source": [
    "None of these scores are particularly great for prediction purposes! These are all predictions based on the training data, but shouldn't accuracy scores be applied to test data (target class y_test, and predicted class y_pred? What happens if I change the arguments for the confusion_matrix() function to contain the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162f2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf = confusion_matrix(y_test, y_pred)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2cab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row:\n",
    "TN = 91\n",
    "FP = 16\n",
    "# second row:\n",
    "FN = 22\n",
    "TP = 49"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ecd7d",
   "metadata": {},
   "source": [
    "Find the Precision score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6473b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of positive predictions\n",
    "precision = TP / (TP + FP)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae1c0b6",
   "metadata": {},
   "source": [
    "Find the recall score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa5768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ratio of correctly predicted instances\n",
    "recall = TP / (TP + FN)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd0186",
   "metadata": {},
   "source": [
    "Plotting the Actual versus Predicted survival classes using the results from the Confusion Matrix above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce638a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# plotting actual v predicted values\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(\"Actual v Predicted Survival Rates\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9856fd2d",
   "metadata": {},
   "source": [
    "Rather than using the confusion_matrix() function, importing separate precision and recall score functions may provide different accuracy results. Let's try!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b59777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "prec = precision_score(y_test, y_pred)\n",
    "print(f\"The Precision Accuracy Score is: \", prec)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"The Recall Accuracy Score is: \", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440cea5f",
   "metadata": {},
   "source": [
    "This indicates a slight improvement when predicting values in the test set, so it seems to generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b6e09b",
   "metadata": {},
   "source": [
    "The predict_proba() method predicts the probability that each element falls within that particular class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2103a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abac0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb80c7c",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_train, y_pred)\n",
    "print(f\"The Receiver Operation Characteristic-Area Under Curve Score is: \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1427994",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d2727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_scores = sgd_clf.decision_function(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b75b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# plot the true-positive rate v false-positive rate\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"fpr\")\n",
    "    plt.ylabel(\"tpr\")\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd3ca6",
   "metadata": {},
   "source": [
    "The blue curve should be pushed as far to the top left-hand corner as possible for a more accurate model. This can be improved upon by applying a different model such as a Random Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec5f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "y_prob_rf = cross_val_predict(rf, X_train, y_train, cv=3, method=\"predict_proba\")\n",
    "\n",
    "y_scores_rf = y_prob_rf[:, :1]\n",
    "fpr_rf, tpr_rf, thresholds_rf = roc_curve(y_train, y_scores_rf)\n",
    "\n",
    "plt.plot(fpr, tpr, \"b:\", label=\"SGD\")\n",
    "plot_roc_curve(tpr_rf, fpr_rf, \"Random Forest Model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5da7584",
   "metadata": {},
   "source": [
    "So the Random Forest Classifier model has improved the Area Under the Curve because it is better at predicting the target values, or survival rate in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a32027",
   "metadata": {},
   "source": [
    "### Unlabeled Dataset\n",
    "Repeating this train-test-split but using the unlabeled values in the dataset, from index location 892 to 1304."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c9d16-05dc-4a23-8a39-8f724f4ce4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the 'Survived' column from the predictors DataFrame variable X\n",
    "X_train = updated_df.drop(\"Survived\", axis=1)\n",
    "X_train = updated_df.iloc[0:889]\n",
    "# assigning this dropped column to the target Series variable y\n",
    "y_train = updated_df[\"Survived\"]\n",
    "y_train = y_train.iloc[0:889]\n",
    "# assigning the test set\n",
    "test_df = updated_df.drop(\"Survived\", axis=1).copy()\n",
    "X_test = test_df.iloc[890:1304]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab375a6",
   "metadata": {},
   "source": [
    "X_train isn't dropping 'Survived' for some reason!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36167e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the shape of each set\n",
    "X_train.shape, y_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20834c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419a3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd1171",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30108faf",
   "metadata": {},
   "source": [
    "Check the respective length of the different sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb035a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove rows with any values that are not finite\n",
    "X_test = X_test[np.isfinite(test_df).all(1)]\n",
    "\n",
    "#view updated DataFrame\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7bb10",
   "metadata": {},
   "source": [
    "Train and fit the model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676f2cd-8427-41d6-8e18-dc6736d09948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_new = LogisticRegression()\n",
    "log_reg_new.fit(X_train, y_train)\n",
    "y_pred = log_reg_new.predict(X_test)\n",
    "acc_log = round(log_reg_new.score(X_train, y_train) * 100, 2)\n",
    "acc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33783484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_clf = LogisticRegression(multi_class='ovr')\n",
    "log_reg_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edfa1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg_clf.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf20bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49fb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(log_reg_clf, X_train, y_train, cv=5, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1881871-f210-49d9-86e9-9ad6d82ab3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating y_train_pred values:\n",
    "y_train_pred = log_reg_clf.predict(X_train)\n",
    "y_train_pred[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d108f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "\n",
    "# plotting actual v predicted values\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title(\"Actual v Predicted Survival Rates\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4895b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first row:\n",
    "TN = \n",
    "FP = \n",
    "# second row:\n",
    "FN = \n",
    "TP = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d85ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = precision_score(y_test, y_pred)\n",
    "print(f\"The Precision Accuracy Score is: \", prec)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "print(f\"The Recall Accuracy Score is: \", rec)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(f\"The Receiver Operation Characteristic-Area Under Curve Score is: \", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b438685",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7d60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966c7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fabdf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_scores = sgd_clf.decision_function(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "\n",
    "# plot the true-positive rate v false-positive rate\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlabel(\"fpr\")\n",
    "    plt.ylabel(\"tpr\")\n",
    "    \n",
    "plot_roc_curve(fpr, tpr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960372a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc436d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4041a76b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58460519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf727b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa56d88-aa15-48c5-bb96-97390d318bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_features.csv', index=False)\n",
    "X_val.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_features.csv', index=False)\n",
    "X_test.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_features.csv', index=False)\n",
    "\n",
    "y_train.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_labels.csv', index=False)\n",
    "y_val.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_labels.csv', index=False)\n",
    "y_test.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7065c1f-4776-40a4-a823-f33977a1399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out cleaned data\n",
    "titanic.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/titanic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de99ac7-40b5-4103-95f8-58afc11aac20",
   "metadata": {},
   "source": [
    "## Improving Evaluation Metrics\n",
    "Iterating the process with new algorithms and improving the evaluation scores. I will repeat this process by trying out iterations changing Parameters and Hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ae789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "tree_clf = dtc.fit()\n",
    "tree_clf = tree_clf.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56395a-de5f-4c9d-94e5-42cd15f7b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "tr_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_features.csv')\n",
    "tr_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22049382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d038036-8cc6-4c53-b27a-503ce73d2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(rf, tr_features, tr_labels.values.ravel(), cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268177b6-5393-4d07-bc9e-5f95f9aa1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aa756-80c6-4102-8830-34f03c0f0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 100],\n",
    "    'max_depth': [2, 10, 20, None]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(rf, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6c8af-1d64-4902-bcbf-d6735fa3f51f",
   "metadata": {},
   "source": [
    "## Pipeline: Evaluate results on validation set\n",
    "Using the Titanic dataset from this Kaggle competition.\n",
    "\n",
    "In this section, we will use what we learned in last section to fit the best few models on the full training set and then evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0e800-f383-43e2-92fc-8969ace1d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "tr_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_features.csv')\n",
    "tr_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_labels.csv', header=None)\n",
    "\n",
    "val_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_features.csv')\n",
    "val_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_labels.csv', header=None)\n",
    "\n",
    "te_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_features.csv')\n",
    "te_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd697c-e9d6-40e1-b261-754b8a3aba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=5, max_depth=10)\n",
    "rf1.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf2.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "rf3.fit(tr_features, tr_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc87d7-5416-4d3d-9be7-0cc38b4912ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdl in [rf1, rf2, rf3]:\n",
    "    y_pred = mdl.predict(val_features)\n",
    "    accuracy = round(accuracy_score(val_labels, y_pred), 3)\n",
    "    precision = round(precision_score(val_labels, y_pred), 3)\n",
    "    recall = round(recall_score(val_labels, y_pred), 3)\n",
    "    print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(mdl.max_depth,\n",
    "                                                                         mdl.n_estimators,\n",
    "                                                                         accuracy,\n",
    "                                                                         precision,\n",
    "                                                                         recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6802853-f217-4000-84a7-c22f04e83499",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf2.predict(te_features)\n",
    "accuracy = round(accuracy_score(te_labels, y_pred), 3)\n",
    "precision = round(precision_score(te_labels, y_pred), 3)\n",
    "recall = round(recall_score(te_labels, y_pred), 3)\n",
    "print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(rf2.max_depth,\n",
    "                                                                     rf2.n_estimators,\n",
    "                                                                     accuracy,\n",
    "                                                                     precision,\n",
    "                                                                     recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08247c-6081-4914-b26d-34d8fb6f5955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f9ccf-524d-4339-9815-25fd6a8189dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0898438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,watermark,matplotlib,seaborn,sklearn,warnings\n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c26972402dce5166fbc873f625c08651cf8cab8ad67af055bc25543d79ffa73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
