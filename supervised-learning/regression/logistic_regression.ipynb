{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89502158",
   "metadata": {},
   "source": [
    "# Classification Problem\n",
    "Next I will develop a Logistic Regression model to predict different classes. More specifically Logistic Regression is used to estimate the probability that an instance, element or observation belongs to a certain class. The use of one of the most popular collections of information for the purpose of classification is the Titanic dataset and the model I have developed will be submitted to Kaggle's 'Titanic - Machine Learning from Disaster' competition.\n",
    "\n",
    "The purpose of this model is to identify if these passengers 'Survived' or 'Not' which will involve creating a target output column populated with simple binary results of '1' or '0'.\n",
    "\n",
    "## Import the Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, precision_recall_curve\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b863215b",
   "metadata": {},
   "source": [
    "## Ingest the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c645fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/titanic_data.csv')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ba7f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column names and data types\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e89cd1b",
   "metadata": {},
   "source": [
    "So I can determine there are a total of 183 entries in this dataset. Initial thoughts are that it might be worth using a more comprehensive dataset, one which might contain the full list of passengers (1309) rather than just a subset (183). This is the most comprehensive list available for the purpose of this exercise that I can find, although estimates for the total number of passengers and crew members are thought to be in the region of 2220. The most comprehensive datasets might be Encyclopedia Titanica and Wikipedia, both of which can be found online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a793085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing once again\n",
    "titanic = pd.read_csv('C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/titanic.csv',\n",
    "                     header=0,\n",
    "                     names = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin',\n",
    "                              'Embarked','WikiId','Name_wiki','Age_wiki','Hometown','Boarded','Destination','Lifeboat','Body',\n",
    "                              'Class'])\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4cab9",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "Removing unwanted columns and rows and feature engineering is the next important step. Straight away I can see the second dataset I have imported from Kaggle which I have named 'titanic.csv' has a more comprehensive number of entries but also contains 21 columns as opposed to just 12 in the first set. Time to establish which of these columns will be kept or removed using some dimensionality reduction and combination, before establishing what is to be included in a Pandas DataFrame table and target Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the column names and data types\n",
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ebf6b",
   "metadata": {},
   "source": [
    "I can remove 'PassengerId', 'Name', 'Age', 'Ticket', 'Embarked', 'WikiId', 'Name_wiki', 'Hometown', 'Destination', 'Lifeboat', 'Body', 'Fare' and 'Class' which will significantly reduce clutter in my table as these features provide no causal relationship with passenger Survival, some of which also represent duplicated information such as passenger class 'Pclass' and 'Class'. This initial step of reducing the size helps provide a much more useful dataset overall.\n",
    "Next, let's determine the index and column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c429c6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343dee99",
   "metadata": {},
   "source": [
    "Also I can find the number of entries or range of the index using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0088fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1db80",
   "metadata": {},
   "source": [
    "So the index starts at 0 and ends at 1309, a total of 1310 passengers (not including crew members), but in terms of the data entries in this table only 891 are labeled with target predictions, 419 are unlabeled, meaning the 'Survived' target column only has the first 891 entries populated.\n",
    "\n",
    "### Some Descriptive Stats\n",
    "Several of these values can come in handy in case I need to impute any averages for missing values later. Bear in mind this only contains data for columns with numeric values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a76df",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f6ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269f3f23-04f8-48aa-9260-f6bccfca79f4",
   "metadata": {},
   "source": [
    "### Dropping Columns\n",
    "Store a copy of these columns in a new variable or dataset so I don't overwrite the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36861d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_new = titanic[['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked',\n",
    "                       'WikiId','Name_wiki','Age_wiki','Hometown','Boarded','Destination','Lifeboat','Body','Class']]\n",
    "titanic_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b7912b",
   "metadata": {},
   "source": [
    "The easiest way to drop the columns not required is to create a new subset of data (2d array) with the specified columns to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d0fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_reduced = titanic_new[['Survived','Pclass','Sex','SibSp','Parch','Cabin','Age_wiki','Boarded','Fare']]\n",
    "titanic_reduced.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5838ec81",
   "metadata": {},
   "source": [
    "I am using 'Age_wiki' from the Wikipedia web site which appears to be a much more comprehensive set of data than that of the 'Age' column. The 'Boarded' column has nominal data which I will endeavour to convert to numeric values so each port a passenger embarks from will be represented by a number instead. This will help provide more uniform data types. 'Sex' can also be converted to 0's or 1's for the purpose of this exercise and the 'SibSp' and 'Parch' columns can be combined using feature extraction to concatenate the size of families into a new series. The 'Cabin' data will be converted to binary integer values for simplicity and the 'Pclass' (Passenger Class) observations are already denoted as integer values. These appear to have a significant impact on passenger survival so I'm including them.\n",
    "\n",
    "Reducing the number of features is called dimensionality reduction and is an important technique used to achieve comparable results in a much faster time frame (with little benefit to performance accuracy), but generally works better with much larger datasets. The removal of all the unwanted columns contained in the modified dataset variable helps speed the model up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fce98a-ea68-40cf-9abe-8fdd5dfcf82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partial or subset of titanic dataframe\n",
    "df = pd.DataFrame({\n",
    "    'Survived': pd.Series(titanic_reduced['Survived']),\n",
    "    'Pclass': pd.Series(titanic_reduced['Pclass']),\n",
    "    'Sex': pd.Series(titanic_reduced['Sex']),\n",
    "    'SibSp': pd.Series(titanic_reduced['SibSp']),\n",
    "    'Parch': pd.Series(titanic_reduced['Parch']),\n",
    "    'Cabin': pd.Series(titanic_reduced['Cabin']),\n",
    "    'Age_wiki': pd.Series(titanic_reduced['Age_wiki']),\n",
    "    'Boarded': pd.Series(titanic_reduced['Boarded']),\n",
    "    'Fare': pd.Series(titanic_reduced['Fare'])\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95088b36",
   "metadata": {},
   "source": [
    "Which columns or features are left now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2fd0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d01436",
   "metadata": {},
   "source": [
    "## Clean the Data\n",
    "### Missing Values\n",
    "Next it's really important to remove or impute any Null or missing values. This depends on any row values which are missing and also on the data type for each column. Calculating the total number of missing or Null values across the entire 'titanic' dataset gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8853e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_missing = pd.isnull(titanic).sum()\n",
    "print(titanic_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a98e4e",
   "metadata": {},
   "source": [
    "More specifically, to narrow my workable dataset down and find the total number of missing values from the training set of predictor variables, X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b4661-3a16-4ef4-8982-64fea16148cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = pd.isnull(df).sum()\n",
    "print(df_missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae186e",
   "metadata": {},
   "source": [
    "Next it becomes useful to determine if these missing values occurred because they weren't recorded or because there was no information for them? Assessing this output I can determine that the null values in 'Cabin' simply represent those who did not have a cabin for sleeping quarters and so these would not have been recorded. These passengers would have traveled in other areas of the ship so it's important not to drop these values as they represent important data and account for over three quarters of the overall number of passengers in this particular set. \n",
    "\n",
    "There are also five null values for the 'Boarded' column so for whatever reason these passengers did not have their boarding locations recorded. It's impossible to really know which port location these individuals departed from so I can either leave the values as NaN or remove each of these five entries as a value should exist if they boarded legally and other attributes were recorded, e.g. Name, Class, Cabin or even Age.\n",
    "\n",
    "The 'Age-wiki' feature records the ages provided by passengers when purchasing their tickets so it was likely based on the D.O.B. in their travel documents or passports. Taking a look at the total number of Null or missing values for the 'Age_wiki' column first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c0dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_age_null = df['Age_wiki'].isnull().sum()\n",
    "print(num_age_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa7427",
   "metadata": {},
   "source": [
    "Identifying each row in the dataframe which contains a null value for 'Age_wiki' and assigning it to a new variable called 'age_null' (as I may require this later).\n",
    "### Null Values for Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84040cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_null = df[df['Age_wiki'].isnull()]\n",
    "print(age_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8831c69a",
   "metadata": {},
   "source": [
    "I can make a decision whether to include these 7 passengers and merely impute some average age for their respective 'Sex', impute an average based on the overall mean for both genders, or remove them completely. Seeing as the majority of information for each of these passengers (roughly 4/7ths to 5/7ths) is present I would prefer to keep these entries, so imputing mean values for age based on the individuals sex may be a reasonably accurate average.\n",
    "\n",
    "The mean age for everyone, regardless of sex is 29.88 according to the describe() method above. Another way to find the overall average age is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eadf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(df['Age_wiki'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30320c8",
   "metadata": {},
   "source": [
    "This overall mean or average may not be as accurate as calculating the average age for both male and female passengers and imputing them into the 7 missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb8773",
   "metadata": {},
   "source": [
    "### Calculate Average Age\n",
    "Calculating the average age for male and female passengers in the table can be done by summing each individual age (by sex) and dividing by the total number of male or female passengers respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91befada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the total count of each of the two unique values in the Sex column (total male or female passengers)\n",
    "df['Sex'].value_counts().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade4a8c",
   "metadata": {},
   "source": [
    "This is based on all 1309 passengers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df.Survived[df.Sex == 'male'].value_counts().plot(kind='bar', alpha=0.5, color='teal')\n",
    "plt.title(\"Male Survival\")\n",
    "# create style\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Survived[df.Sex == 'female'].value_counts().plot(kind='bar', alpha=0.5, color='pink')\n",
    "plt.title(\"Female Survival\")\n",
    "# create style\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe9dafa",
   "metadata": {},
   "source": [
    "So having determined the unique classes within the 'Sex' column I can further identify the number of Males and Females who survived or not. By taking the 'Survived' column and sub-dividing it according to gender it displays how women were far more likely to have survived the Titanic disaster based on the predictor variables included with this dataset.\n",
    "\n",
    "Next I want to group each category of male and female and store them in a variable called 'gender'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9c646",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = df.groupby(df['Sex'])\n",
    "gender.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f0f27d",
   "metadata": {},
   "source": [
    "Checking the first few entries for both sexes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f010879",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Library\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Countplot\n",
    "sns.catplot(x =\"Sex\", hue =\"Survived\",\n",
    "kind =\"count\", data = df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774499a",
   "metadata": {},
   "source": [
    "Now there are two variables, one with all the male and one with all the female passengers in the Titanic dataset. There are a total of 843 male and 466 female passengers.\n",
    "\n",
    "The next step is to add these totals together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4a48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_total = 843\n",
    "female_total = 466\n",
    "total_passengers = male_total + female_total\n",
    "total_passengers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e27187",
   "metadata": {},
   "source": [
    "Summing the total of all ages for all the passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65692b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_age = df['Age_wiki'].sum()\n",
    "print(sum_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f4be7",
   "metadata": {},
   "source": [
    "And dividing by the total number of passengers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8c4657",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_ave = sum_age / total_passengers\n",
    "age_ave"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b254b64",
   "metadata": {},
   "source": [
    "So the overall average age for all passengers calculates to just over 29 years old. Using the describe method to check this gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82928d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315be455",
   "metadata": {},
   "source": [
    "So the first item to notice is that only numeric data appears to have been captured which will need to be fixed soon, but the answer I was looking for now, the mean age found under the 'Age-wiki' column is 29.415829 which is close to the value just calculated of 29.258525, but not identical.\n",
    "\n",
    "Next, to see the average ages for both male and female classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by='Sex')['Age_wiki'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f630df",
   "metadata": {},
   "source": [
    "So this produces the mean Age by Sex. \n",
    "\n",
    "What if I wanted to find an average age just for the missing values? There are a total of 7 missing age values in the entire dataset, 4 male and 3 female. Imputing a value of 29 for the missing 'male' age and 28 for 'female' would be the most accurate solution but instead, the mean age value for all passengers will be imputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aa2771",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Use SimpleImputer model to fill in missing values\n",
    "imputer = SimpleImputer(strategy='mean', missing_values=np.nan)\n",
    "age_imputer = imputer.fit(df[['Age_wiki']])\n",
    "df['Age_wiki'] = age_imputer.transform(df[['Age_wiki']])\n",
    "df['Age_wiki'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baddd07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_wiki'] = df['Age_wiki'].astype('int')\n",
    "df['Age_wiki'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669bb3a1",
   "metadata": {},
   "source": [
    "So checking the first missing age value to see what value it contains now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbf29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age_wiki[42:43]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a8b44",
   "metadata": {},
   "source": [
    "And another of the missing values was index 1041:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aac89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age_wiki[1041:1042]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33bdf29b",
   "metadata": {},
   "source": [
    "This appears to be correct. \n",
    "\n",
    "All 'Age_wiki' observations are entered as floats but the data type needs to be changed to integer (representing years) which is technically more correct as a unit of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd11cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Age_wiki has any more null values\n",
    "df['Age_wiki'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3357a9f7-180a-4784-b9da-e3fd6c81a82e",
   "metadata": {},
   "source": [
    "This tells me all the null values in the Age column have been replaced properly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f37869",
   "metadata": {},
   "source": [
    "### Convert the Cabin Feature\n",
    "Next, I want to address the issue relating to Cabin data. Because each passenger was assigned a Cabin number which is just an alpha-numeric string type, I would prefer to convert all entries for Cabin to a straight forward integer. Assigning a value of 1 for the presence of a cabin number and 0 for someone without.\n",
    "#### Creating a Binary Indicator for Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the Cabin values having converted them and store them in the same name\n",
    "df['Cabin'] = np.where(df['Cabin'].isnull(), 0, 1)\n",
    "df['Cabin'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d54b3",
   "metadata": {},
   "source": [
    "Now the 'Cabin' data has been converted into binary numeric values equivalent to 'cabin' or 'no cabin' and even though there are 1014 passengers who were not assigned cabins, this data will remain. The Boarded column needs to have its categorical entries changed into numeric values using 'One-Hot Encoding'.\n",
    "\n",
    "Checking if there are null values in 'Boarded':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a63507",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['Boarded'].isnull().values.any():\n",
    "    raise Exception(\"Null values found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a940ada",
   "metadata": {},
   "source": [
    "Checking how many there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591ada43-ff87-4913-b434-bd063028d6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_boarded_null = df['Boarded'].isnull().sum()\n",
    "print(num_boarded_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f28c0d2",
   "metadata": {},
   "source": [
    "Which 5 indexed rows are missing for the 'Boarded' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1e97c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Boarded'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e34574b",
   "metadata": {},
   "source": [
    "Because there are only five missing I will drop these rows from the database completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacd39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Boarded'] = df['Boarded'].dropna(inplace=True, axis=1)\n",
    "\n",
    "# check if Boarded has any more missing values\n",
    "df['Boarded'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f7626-2bbd-4fb3-8789-875882592e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove values within this column which are missing\n",
    "boarded_values_removed = X['Boarded'].dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25732580-9a32-4057-ad13-840a7d6320f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Boarded.dropna(df.Boarded(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ac8c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb6fcb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e465ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ec1de55",
   "metadata": {},
   "source": [
    "Finally, checking the total number of Null values for the 'Cabin' feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cabin_null = X['Cabin'].isnull().sum()\n",
    "print(num_cabin_null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca1f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X['Cabin'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488af821",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Cabin.fillna(df.Cabin.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecfdef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7b802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf0aa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputing mean values\n",
    "df['Fare'] = df['Fare'].copy(deep=True)\n",
    "fare_imputer = SimpleImputer(strategy='most_frequent', missing_values=np.nan)\n",
    "imputer = fare_imputer.fit_transform(fare_imputer.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05298049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Fare.fillna(df.Fare.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234fe3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9d785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d503fa1",
   "metadata": {},
   "source": [
    "Now the missing values have been dealt with, what about the string and categorical variables in the dataset? An important consideration to make when using visualizations would be the data types involved. For example, information can be split into numeric (quantitative) data and categorical (qualitative) data. Categorical data values could be Binomial (such as the target outcome 'Survived', or 'Sex'), Nominal (such as 'Cabin', or 'Boarded'), perhaps even Ordinal (such as 'Pclass'). 'Age_wiki' contains continuous values and the rest such as 'SibSp' (number of Siblings or Spouse) and 'Parch' (number of Children accompanied by Parents) are discrete integer values.\n",
    "\n",
    "Having established the different different data types the next step is to convert the string objects into numeric types.\n",
    "\n",
    "### Convert the Sex Feature\n",
    "The male entries will be assigned a value of 1 and female, 0. Viewing the total number of Male passengers who didn't survive (0.0), or did survive (1.0), these can be switched to integer values also. This can be repeated for the Female passengers also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69731f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833675ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d91375c2",
   "metadata": {},
   "source": [
    "### Using One-Hot Encoding to Convert the Boarded Feature\n",
    "There are three possible states for the 'Boarded' column including Queenstown, Southampton and Cherbourg which were the only departure locations listed. Each of these entries can be assigned a value of 1,2 or 3. This can be achieved using a technique called one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4298e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e1c594f",
   "metadata": {},
   "source": [
    "Now all the data has been converted to numeric values into a more comprehensive set the performance of the model will be improved dramatically. One interesting visual summarizing the data dispersion of all the features would be a hist plot. This provides a frequency distribution of discrete and continuous variables. \n",
    "\n",
    "For example, we can qualify discrete variables as those which occupy a specific number of states determined by 1 / k where k equals the number of possible outcomes within this universal set. Determining the likelihood of each state occurring is calculated using a probability mass function.\n",
    "\n",
    "Continuous variables have an infinite number of possible values and act as real numbers. These values are calculated using a probability mass function which does not provide the probability of a specific state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d593245",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(bins=20, figsize=(20,10))          # make sure to include Cabin, Sex values as binary and Boarded as categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe959bf",
   "metadata": {},
   "source": [
    "When it comes to visualizing these different types of data it is generally better to use scatter and line plots for numeric data, but for categorical data, frequency distributions, bar charts and histograms may be a better approach for viewing different classes or sub-sets of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ea958",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24179e30",
   "metadata": {},
   "source": [
    "Taking a brief look at the dispersion of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba4f02d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8351997",
   "metadata": {},
   "source": [
    "Taking a quick glance at the frequency distributions for each column reveals some interesting information. For example there are 2 distinct categories for survival, there don't appear to be any Sibling-Spouse groups larger than 8 in total, there's an average age of roughly 20-21 years, there are 3 distinct passenger classes (1st, 2nd and 3rd), an overwhelming number of People travelling without minors and much smaller numbers of Parents travelling with just 1 or 2 children. Finally, the vast majority of fares appear to be below the 25.0 mark, but there were some almost approaching 275.0. I'm not sure if these are Schillings, Guineas or Pounds Sterling, but this data can be further researched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ed93f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6e35190-f305-4136-bb7d-7d331b1c3143",
   "metadata": {},
   "source": [
    "## Row Selection\n",
    "At this stage it's best to employ the use of a Train, Test, Split algorithm using the labeled data (rows 1 to 891) which contains values for the target feature entitled 'Survived'. The model will fit to and learn from this data, then I can evaluate the efficiency of the chosen model against the unseen (un-labeled) data, rows 892 to 1309 which don't contain target values for the 'Survived' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aeeefe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4057318",
   "metadata": {},
   "source": [
    "## Grouping Data Together\n",
    "Taking a look at the average values for each feature based on their survival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9749e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Survived').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072399aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23f772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fa48cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c4a906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e825ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "men = titanic.loc[titanic.Sex == 'male']['Survived']\n",
    "rate_men = sum(men)/len(men)\n",
    "\n",
    "print(\"% of men who survived:\", rate_men)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbc3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "women = titanic.loc[titanic.Sex == 'female']['Survived']\n",
    "rate_women = sum(women)/len(women)\n",
    "\n",
    "print(\"% of women who survived:\", rate_women)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a91c2b",
   "metadata": {},
   "source": [
    "## Predictor and Target Variables\n",
    "Now I've established which features are to be included in the whole dataset, it's important to conduct a separation of the predictor variables contained in a dataframe and the target series. This will also lay a foundation for further splitting the labeled data into training and test sets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c0f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the 'Survived' column from the predictors DataFrame variable X\n",
    "X = df.drop(pd.Series(df['Survived'], axis=1, inplace=True)\n",
    "# assigning this dropped column to the target Series variable y\n",
    "y = pd.Series(titanic['Survived'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87617ba",
   "metadata": {},
   "source": [
    "Having looked at the source data file, it hasn't been split into training or test data yet. The csv file contains labeled data entries for the first 891 passengers only. Predicted outcomes need to be applied to the unlabeled passengers from 892 up to 1309 inclusive, so the total number of passengers in the 'titanic' dataset includes both the train and test set data. \n",
    "\n",
    "This will need to be fixed. Setting the number of rows equal to the first 891 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615e807-bc5d-48b7-95d4-5e1487d940ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember just to include the first 891 values for X_train and y_train\n",
    "X_train = pd.DataFrame(X[0:890], cols=columns)\n",
    "y_train = pd.Series(y[0:890])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389aeccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6ab906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f290795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec012dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b41b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d16da",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = titanic['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c3316",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = X[0:890]\n",
    "y = y[0:890]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe70747",
   "metadata": {},
   "source": [
    "Convert these dataframes to separate csv files so they can be viewed in Microsoft Excel if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e1109a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_to_csv = X.to_csv(r'C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/X.csv', index=False, header=True)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0078",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_to_csv = y.to_csv(r'C:/Users/lynst/Documents/GitHub/machine-learning-projects/supervised-learning/regression/y.csv', index=False, header=True)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f157f",
   "metadata": {},
   "source": [
    "Looking at the new shape of the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462620a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a03e97",
   "metadata": {},
   "source": [
    "This is better! The reduced number of entries now only contains those instances which are labeled. These will be split into training and validation sets later on to evaluate the model's accuracy. Taking a look at my new table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d3b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e7e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d81a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e332dd3",
   "metadata": {},
   "source": [
    "## Nature of the Data\n",
    "Of the remaining data 'Survived' is a float which needs to be changed to int. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d1d2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Survived'] = titanic['Survived'].astype('int8', copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9616b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9968e265",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05b1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18,6))\n",
    "titanic.Survived.value_counts(normalize=True).plot(kind=\"bar\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a3b6b7",
   "metadata": {},
   "source": [
    "Looking into relationships between the different columns can provide more insight, for example between 'Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Cabin', 'Boarded' and their 'Survived' status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80135351",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.hist(bins=50, figsize(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2549b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4a52d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0d84699",
   "metadata": {},
   "source": [
    "The question is \"How do I prepare the dataset with the correct number of total labeled entries?\". I can either change the data at source and slice it using Excel, or alternatively slice the data in Python to only include the first 891 passengers. The reason this needs to be done is because of the risk of feeding inaccurate and unlabeled data back into the model. I believe using test data will introduce bias into the classification results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993db830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda046b3-8829-4d9f-92ed-55d1b1d20904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae754d-cab6-4c26-9edc-902ac6957649",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3119d-ce4e-43e5-acf9-333ba30e91ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08099cd8-472c-46c1-b869-e07f1dd73ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d27f7f-686a-4f43-8cda-3e3f5607a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323c9d16-05dc-4a23-8a39-8f724f4ce4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression().fit(iris['data'], iris['target'])\n",
    "model.predict(iris['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f573208d-f179-4455-a553-d1ab15a3b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all categorical features\n",
    "cat_feat = ['PassengerId', 'Name', 'Ticket', 'Sex', 'Cabin', 'Embarked']\n",
    "titanic.drop(cat_feat, axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecf5901-41a7-4a27-a324-183c6ec68d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all continuous features\n",
    "cont_feat = ['PassengerId', 'Pclass', 'Name', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "titanic.drop(cont_feat, axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a81ee63-f0fa-4f13-85c8-abba69f24464",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby('Survived').mean()\n",
    "titanic.groupby(titanic['Age'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daa091c-5664-4944-b065-96411b2b9519",
   "metadata": {},
   "source": [
    "## Plot Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097cb21d-3e8c-4303-90d4-5f74176b037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['Age', 'Fare']:\n",
    "    died = list(titanic[titanic['Survived'] == 0][i].dropna())\n",
    "    survived = list(titanic[titanic['Survived'] == 1][i].dropna())\n",
    "    xmin = min(min(died), min(survived))\n",
    "    xmax = max(max(died), max(survived))\n",
    "    width = (xmax - xmin) / 40\n",
    "    sns.distplot(died, color='r', kde=False, bins=np.arange(xmin, xmax, width))\n",
    "    sns.distplot(survived, color='g', kde=False, bins=np.arange(xmin, xmax, width))\n",
    "    plt.legend(['Did not survive', 'Survived'])\n",
    "    plt.title('Overlaid histogram for {}'.format(i))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636e8da2-a973-46ac-b094-4317df91dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(['Pclass', 'SibSp', 'Parch']):\n",
    "    plt.figure(i)\n",
    "    sns.catplot(x=col, y='Survived', data=titanic, kind='point', aspect=2,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa457705-c9b9-489b-b77a-608d6aaa325f",
   "metadata": {},
   "source": [
    "## Combine SibSp and Parch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6016e7-09ea-4bdb-ab19-998b062cc126",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(['SibSp', 'Parch']):\n",
    "    plt.figure(i)\n",
    "    sns.catplot(x=col, y='Survived', data=titanic, kind='point', aspect=2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e042b704-d1f3-4a6e-9f5b-b5369e73dcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Family_cnt'] = titanic['SibSp'] + titanic['Parch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae725c2-870e-4f9d-859e-8255d388a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['family_cnt'] = titanic['SibSp'] + titanic['Parch']\n",
    "sns.catplot(x='family_cnt', y='Survived', data=titanic, kind='point', aspect=2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651fd4b-f024-42cb-b7eb-2ca0e87046de",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Family_cnt'] = titanic['SibSp'] + titanic['Parch']\n",
    "titanic.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922bb51c-896b-4b13-9f50-caad968a974a",
   "metadata": {},
   "source": [
    "## Plot Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f6cb3-21b7-49a9-b4f8-f792c65cf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(['Cabin_ind', 'Sex', 'Embarked']):\n",
    "    plt.figure(i)\n",
    "    sns.catplot(x=col, y='Survived', data=titanic, kind='point', aspect=2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67009eca-a380-4b4c-b27f-befda45093e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.groupby(titanic['Cabin'].isnull()).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc4a0ed-22f5-4a8a-8b29-232787f625ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['Cabin_ind'] = np.where(titanic['Cabin'].isnull(), 0, 1)\n",
    "titanic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78bca0c-ed07-41ea-bca0-6209df47ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('Survived', index='Sex', columns='Embarked', aggfunc='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a36d06-d8fb-4706-b2d4-7b46ed25d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('Survived', index='Cabin_ind', columns='Embarked', aggfunc='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b90fb-dae3-4e76-8a65-d248f22e03bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642532d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf1270e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08337c6c-b9e7-41a2-ac81-22c4fcbe273a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c343b27-696f-48f4-9c04-7bc3e97448f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Sex to numeric values\n",
    "gender_num = {'male': 0, 'female': 1}\n",
    "\n",
    "titanic['Sex'] = titanic['Sex'].map(gender_num)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea5d1d5-ba87-4c7b-8022-995fbd83be29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Cabin and Boarded\n",
    "titanic.drop(['Cabin', 'Boarded'], axis=1, inplace=True)\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6676f2cd-8427-41d6-8e18-dc6736d09948",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.drop('Survived', axis=1)\n",
    "labels = titanic['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8147817-4550-4ae8-9355-6b68a1458af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = titanic.drop('Survived', axis=1)\n",
    "labels = titanic['Survived']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.4, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1881871-f210-49d9-86e9-9ad6d82ab3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in [y_train, y_val, y_test]:\n",
    "    print(round(len(dataset) / len(labels), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa56d88-aa15-48c5-bb96-97390d318bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_features.csv', index=False)\n",
    "X_val.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_features.csv', index=False)\n",
    "X_test.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_features.csv', index=False)\n",
    "\n",
    "y_train.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_labels.csv', index=False)\n",
    "y_val.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_labels.csv', index=False)\n",
    "y_test.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dba65b-54ba-4043-b9f8-91fb324ed17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels), len(y_train), len(y_val), len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7065c1f-4776-40a4-a823-f33977a1399c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out cleaned data\n",
    "titanic.to_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/titanic_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de99ac7-40b5-4103-95f8-58afc11aac20",
   "metadata": {},
   "source": [
    "## Fit and evaluate a basic model using 5-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56395a-de5f-4c9d-94e5-42cd15f7b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "tr_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_features.csv')\n",
    "tr_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d038036-8cc6-4c53-b27a-503ce73d2841",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "scores = cross_val_score(rf, tr_features, tr_labels.values.ravel(), cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268177b6-5393-4d07-bc9e-5f95f9aa1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(results):\n",
    "    print('BEST PARAMS: {}\\n'.format(results.best_params_))\n",
    "\n",
    "    means = results.cv_results_['mean_test_score']\n",
    "    stds = results.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, results.cv_results_['params']):\n",
    "        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022aa756-80c6-4102-8830-34f03c0f0c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "parameters = {\n",
    "    'n_estimators': [5, 50, 100],\n",
    "    'max_depth': [2, 10, 20, None]\n",
    "}\n",
    "\n",
    "cv = GridSearchCV(rf, parameters, cv=5)\n",
    "cv.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "print_results(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a6c8af-1d64-4902-bcbf-d6735fa3f51f",
   "metadata": {},
   "source": [
    "## Pipeline: Evaluate results on validation set\n",
    "Using the Titanic dataset from this Kaggle competition.\n",
    "\n",
    "In this section, we will use what we learned in last section to fit the best few models on the full training set and then evaluate the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf0e800-f383-43e2-92fc-8969ace1d9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "\n",
    "tr_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_features.csv')\n",
    "tr_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/train_labels.csv', header=None)\n",
    "\n",
    "val_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_features.csv')\n",
    "val_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/val_labels.csv', header=None)\n",
    "\n",
    "te_features = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_features.csv')\n",
    "te_labels = pd.read_csv('C:/Users/lynst/Documents/Python Scripts/Ex_Files_Applied_Machine_Learning/Exercise Files/test_labels.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd697c-e9d6-40e1-b261-754b8a3aba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf1 = RandomForestClassifier(n_estimators=5, max_depth=10)\n",
    "rf1.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "rf2 = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rf2.fit(tr_features, tr_labels.values.ravel())\n",
    "\n",
    "rf3 = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "rf3.fit(tr_features, tr_labels.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bc87d7-5416-4d3d-9be7-0cc38b4912ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdl in [rf1, rf2, rf3]:\n",
    "    y_pred = mdl.predict(val_features)\n",
    "    accuracy = round(accuracy_score(val_labels, y_pred), 3)\n",
    "    precision = round(precision_score(val_labels, y_pred), 3)\n",
    "    recall = round(recall_score(val_labels, y_pred), 3)\n",
    "    print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(mdl.max_depth,\n",
    "                                                                         mdl.n_estimators,\n",
    "                                                                         accuracy,\n",
    "                                                                         precision,\n",
    "                                                                         recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f9632a-92a1-4015-9355-38eb3c2035fe",
   "metadata": {},
   "source": [
    "1. Explore and clean the data\n",
    "2. Split data into train / validation / test\n",
    "3. Fit an initial model and evaluate\n",
    "4. Tune hyper parameters\n",
    "5. Evaluate on validation set\n",
    "6. Final model selection and evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6802853-f217-4000-84a7-c22f04e83499",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf2.predict(te_features)\n",
    "accuracy = round(accuracy_score(te_labels, y_pred), 3)\n",
    "precision = round(precision_score(te_labels, y_pred), 3)\n",
    "recall = round(recall_score(te_labels, y_pred), 3)\n",
    "print('MAX DEPTH: {} / # OF EST: {} -- A: {} / P: {} / R: {}'.format(rf2.max_depth,\n",
    "                                                                     rf2.n_estimators,\n",
    "                                                                     accuracy,\n",
    "                                                                     precision,\n",
    "                                                                     recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba08247c-6081-4914-b26d-34d8fb6f5955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993f9ccf-524d-4339-9815-25fd6a8189dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d93e5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire titanic dataframe\n",
    "titanic_df = pd.DataFrame({\n",
    "    'PassengerId': pd.Series(titanic['PassengerId']),\n",
    "    'Survived': pd.Series(titanic['Survived']),\n",
    "    'Pclass': pd.Series(titanic['Pclass']),\n",
    "    'Name': pd.Series(titanic['Name']),\n",
    "    'Sex': pd.Series(titanic['Sex']),\n",
    "    'Age': pd.Series(titanic['Age']),\n",
    "    'SibSp': pd.Series(titanic['SibSp']),\n",
    "    'Parch': pd.Series(titanic['Parch']),\n",
    "    'Ticket': pd.Series(titanic['Ticket']),\n",
    "    'Fare': pd.Series(titanic['Fare']),\n",
    "    'Cabin': pd.Series(titanic['Cabin']),\n",
    "    'Embarked': pd.Series(titanic['Embarked']),\n",
    "    'WikiId': pd.Series(titanic['WikiId']),\n",
    "    'Name_wiki': pd.Series(titanic['Name_wiki']),\n",
    "    'Age_wiki': pd.Series(titanic['Age_wiki']),\n",
    "    'Hometown': pd.Series(titanic['Hometown']),\n",
    "    'Boarded': pd.Series(titanic['Boarded']),\n",
    "    'Destination': pd.Series(titanic['Destination']),\n",
    "    'Lifeboat': pd.Series(titanic['Lifeboat']),\n",
    "    'Body': pd.Series(titanic['Body']),\n",
    "    'Class': pd.Series(titanic['Class']),\n",
    "})\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0898438",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "\n",
    "# python, ipython, packages, and machine characteristics\n",
    "%watermark -v -m -p wget,pandas,numpy,watermark,matplotlib,seaborn,sklearn,warnings\n",
    "\n",
    "# date\n",
    "print (\" \")\n",
    "%watermark -u -n -t -z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4c26972402dce5166fbc873f625c08651cf8cab8ad67af055bc25543d79ffa73"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
