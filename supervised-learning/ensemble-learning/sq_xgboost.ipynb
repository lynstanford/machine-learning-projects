{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9337eed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\lynst\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV     \u001b[38;5;66;03m# cross validation\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay     \u001b[38;5;66;03m# creates a confusion matrix\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'plot_confusion_matrix' from 'sklearn.metrics' (C:\\Users\\lynst\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd     # load and manipulate data for One-Hot Encoding\n",
    "import numpy as np     # calculate the mean and standard deviation\n",
    "import xgboost as xgb     # XGBoost stuff\n",
    "from sklearn.model_selection import train_test_split     # split data into training and test sets\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV     # cross validation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay     # creates a confusion matrix\n",
    "from sklearn.metrics import plot_confusion_matrix     # draws a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcea027",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/lynst/Documents/Datasets/Kaggle/Jack Chang/telco_churn.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14cd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set axis=0 to remove rows, axis=1 to remove cols\n",
    "df.drop(['Churn Label','Churn Score','CLTV','Churn Reason'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a7f07",
   "metadata": {},
   "source": [
    "Some of the other columns only contain a single value and will not be useful for classification. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57a393",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Count'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372a07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96be42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['State'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cd773f",
   "metadata": {},
   "source": [
    "So we can omit those variables. In contrast, City contains a bunch of different city names, so we will leave it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763405a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['City'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733786d4",
   "metadata": {},
   "source": [
    "We will also remove 'CustomerID' because it is different for every customer and useless for classification. Lastly we will drop 'Lat Long' because there are separate columns for Latitude and Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fac1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['CustomerID','Count','Country','State','Lat Long'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691939eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['City'].replace(' ', '_', regex=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ceb4f",
   "metadata": {},
   "source": [
    "### Note\n",
    "Although it's okay to have whitespace in the city names in 'City' for XGBoost and classification, we can't have any whitespace if we want to draw a tree. So let's take care of that now by replacing the white space in the city names with an underscore character _. \n",
    "\n",
    "### Also Note\n",
    "We can easily remove whitespaces from all values, not just city names, but we will wait to do that until after we have identified missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a6228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['City'].unique()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59638162",
   "metadata": {},
   "source": [
    "We also need to eliminate the whitespace in the column names, so we will replace it with underscores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb2d80",
   "metadata": {},
   "source": [
    "Hooray! We've removed all of the data that will not help us create an effective XGBoost model and reformatted the city and column names so that we can draw a tree. Now we are ready to identify and deal with missing data.\n",
    "\n",
    "# Missing Data Part 1: Identifying Missing Data\n",
    "\n",
    "Unfortunately the biggest part of any data analysis project is making sure that data is correctly formatted and fixing it where it is not. The first part of this process is identifying missing data.\n",
    "\n",
    "Missing data is simply a blank space, or a surrogate value like NA, that indicates that we failed to collect data for one of the features. For example, if we forget to ask someone's age, or forget to write it down, then we would have a blank space in the dataset for that person's age.\n",
    "\n",
    "One thing that's relatively unique about XGBoost is that it has default behaviour for missing data. So all we have to do is identify missing values and make sure they are set to 0.\n",
    "\n",
    "In this section we'll focus on identifying missing values in the dataset. First let's see what sort of data is in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057c8690",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bb5e73",
   "metadata": {},
   "source": [
    "A lot of columns are object and this is okay, because, as we saw above when we ran head( ) there were a lot of text responses like Yes and N0. However, let's verify that we're getting what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phone_Service'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99429d67",
   "metadata": {},
   "source": [
    "So Phone_Service has type object because it contains text, and it only contains two values, Yes and No. So this is good. \n",
    "Now, in practice, we would check every other column, and I did this, but right now we will focus on one specific column that\n",
    "looks like it could be a problem: Total_Charges.\n",
    "\n",
    "If we look at the output from head(), Total_Charges looks like it contains numbers, not text, but the Object datatype suggests that it contains numbers typed as strings. If we try the trick of printing out unique values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3341450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Charges'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9006d69c",
   "metadata": {},
   "source": [
    "We see that there are too many values to print and what little we see look like numbers. If we try to convert the column to numeric values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98027630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this commented out\n",
    "# df['Total_Charges'] = pd.to_numeric(df['Total_Charges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae8c694",
   "metadata": {},
   "source": [
    "We get an error message which tells us that there are blank spaces \" \", in the data, so we need to deal with those.\n",
    "\n",
    "# Missing Data Part 2: Dealing with Missing Data XGBoost Style\n",
    "\n",
    "One thing that's relatively unique about XGBoost is that it determines default behaviour for missing data. So all we have to do is identify missing values and make sure they are set to 0.\n",
    "\n",
    "Before we do that, let's see how many rows are missing data first. If there are too many XGBoost may not be able to handle a large amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.loc[df['Total_Charges'] == ' '])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2d150",
   "metadata": {},
   "source": [
    "Since only 11 rows have missing values, let's take a look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40399589",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Total_Charges'] == ' ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37778f4e",
   "metadata": {},
   "source": [
    "We see that 11 people with Total_Charges == ' ' have just signed up, because Tenure_Months is equal to 0. These people also have Churn_Value set to 0 because they just signed up. So we have a few choices here, we can set Total_Charges to 0 for these 11 people or we can remove them. In this example, we'll try setting Total_Charges to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2032975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['Total_Charges'] == ' '), 'Total_Charges'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39e050",
   "metadata": {},
   "source": [
    "Now lets verify that we modified Total_Charges correctly by looking at everyone who had Tenure_Months set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098faa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Total_Charges'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a1825",
   "metadata": {},
   "source": [
    "We have verified that df contains 0's instead of ' 's for missing values. NOTE: Total_Charges still has the object data type. That is no good because XGBoost only allows int, float or boolean data types. We can fix this by converting it with 'to_numeric'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065f375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Total_Charges'] = pd.to_numeric(df['Total_Charges'])\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c46e9",
   "metadata": {},
   "source": [
    "Now that we've dealt with the missing data, we can replace all of the other whitespaces in all of the columns with underscores. NOTE: we are only doing this so we can draw a picture of one of the XGBoost trees. The only reason we're doing this is so we can print out a nice looking XGBoost tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dddf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace(' ', '_', regex=True, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3885c4c0",
   "metadata": {},
   "source": [
    "# Format Data Part 1: Split the Data into Dependent and Independent Variables\n",
    "\n",
    "Now that we've taken care of the missing data, we are ready to start formatting the data for making an XGBoost model.\n",
    "\n",
    "The first step is to split the data into 2 parts:\n",
    "\n",
    "1. The columns of data that we will use to make classifications.\n",
    "2. The column of data that we want to predict.\n",
    "\n",
    "We will use the conventional notation of X (capital X) to represent the columns of data that we will use to make classifications and y (lower case y) to represent the thing we want to predict. In this case, we want to predict Churn_Value (whether or not a customer will leave a company).\n",
    "\n",
    "The reason we deal with missing data before splitting it into X and y is that if we remove rows, splitting after ensures that each row in X correctly corresponds with the appropriate value in y.\n",
    "\n",
    "NOTE: in the code below we are using copy( ) to copy the data by value. By default, Pandas uses copy by reference. Using copy( ) ensures that the original data df_no_missing is not modified when we modify X or y. In other words if we make a mistake when we are formatting the columns for classification trees, we can just re-copy df_no_missing, rather than have to reload the original data and remove the missing values etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cadfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn_Value', axis=1).copy()     # Alternatively: X = df_no_missing.iloc[:, :-1]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028ee8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Churn_Value'].copy()\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed6114",
   "metadata": {},
   "source": [
    "Now that we have created X, which has the data we want to use to make predictions, and y, which has the data we want to predict, we are ready to continue formatting X so that it is suitable for making a model with XGBoost.\n",
    "\n",
    "# Format the Data Part 2: One-Hot Encoding\n",
    "\n",
    "Now that we've split the dataframe into 2 pieces, X, which is the data we will use to make, or predict, classifications, and y, which contains the known classifications in our training dataset, we need to take a closer look at the variables in X. The list below tells us what each variable represents and the type of data (float or categorical) it should contain:\n",
    "\n",
    "City - category\n",
    "\n",
    "Zip Code - category\n",
    "\n",
    "Latitude - float\n",
    "\n",
    "Longitude - float\n",
    "\n",
    "Gender - category\n",
    "\n",
    "Senior Citizen - category\n",
    "\n",
    "Partner - category\n",
    "\n",
    "Dependents - category\n",
    "\n",
    "Tenure Months - float\n",
    "\n",
    "Phone Service - category\n",
    "\n",
    "Multiple Lines - category\n",
    "\n",
    "Internet Service - category\n",
    "\n",
    "Online Security - category\n",
    "\n",
    "Online Backup - category\n",
    "\n",
    "Device Protection - category\n",
    "\n",
    "Tech Support - category\n",
    "\n",
    "Streaming TV - category\n",
    "\n",
    "Streaming Movies - category\n",
    "\n",
    "Contract - category\n",
    "\n",
    "Paperless Billing - category\n",
    "\n",
    "Payment Method - category\n",
    "\n",
    "Monthly Charges - float\n",
    "\n",
    "Total Charges - float\n",
    "\n",
    "Now just to review let's look at the data types in X just to remember how Python is seeing the data right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500a7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c8426c",
   "metadata": {},
   "source": [
    "So we see that Latitude, Longitude, Monthly Charges and Total Charges are all float64, which is good because we want them to be floating point numbers. \n",
    "\n",
    "All of the columns that are object type, however, need to be inspected to make sure that they only contain reasonable values, and most, if not all of them, will need to change. This is because, while XGBoost natively supports continuous data, like Monthly Charges and Total Charges, it does not natively support categorical data, like Phone Service which contains 2 different categories. \n",
    "\n",
    "Thus, in order to use categorical data with XGBoost, we have to use a trick that converts a column of categorical data into multiple columns of binary values. This trick is called One-Hot Encoding.\n",
    "\n",
    "At this point you may be wondering, \"What's wrong with treating categorical data like continuous data?\" To answer that question, let's look at an example. For the Payment_Method column, we have a bunch of options:\n",
    "\n",
    "1. Mailed_Check\n",
    "2. Electronic_Check\n",
    "3. Bank_Transfer\n",
    "4. Credit_Card\n",
    "\n",
    "If we converted these categories to numbers, 1, 2, 3 and 4, treated them like continuous data, then we would assume that 4, which means Credit_Card, is more similar to 3, which means Bank_Transfer, than it is to 1 or 2, which are other forms of payment. That means the XGBoost Tree would be more likely to cluster the people with 4s and 3s together than the people with 4s and 1s together. In contrast, if we treat these Payment_Methods like categorical data, then we treat each one as a separate category that is no more or less similar to any of the other categories. Thus, the likelihood of clustering people who pay with a Mailed_Check with people who pay with an Electronic_Check is the same as clustering Mailed_Check with Credit_Card, and that approach is more reasonable.\n",
    "\n",
    "NOTE: There are many different ways to do One_Hot Encoding in Python. Two of the more popular methods are ColumnTransformer( ) from Scikit-Learn, and get_dummies( ) from Pandas, and both methods have pro's and con's. ColumnTransformer( ) has a very cool feature where it creates a persistent function where it can validate data that you get in the future.\n",
    "\n",
    "For example, if you build your XGBoost model using a categorical variable favorite_color that has red, green and blue options, then ColumnTransformer( ) can remember those options and later on when your XGBoost model is being used in a production system, if someone says their favorite_color is orange, then ColumnTransformer( ) can throw an error or exception. The downside of ColumnTransformer( ) is that it turns your data into an array and loses all of the column names, making it harder to verify that your usage of ColumnTransformer( ) worked as you intended it to. \n",
    "\n",
    "In contrast, get_dummies( ) leaves your data in a DataFrame and retains the column names, making it much easier to verify that it worked as intended. However, it does not have the persistent behaviour that ColumnTransformer( ) has. So, for the sake of learning how One-Hot Encoding works, I prefer to use get_dummies( ). However, once you are comfortable with One_Hot Encoding, I encourage you to investigate using ColumnTransformer( ).\n",
    "\n",
    "First, before we commit to converting columns with One-Hot Encoding, let's just see what happens when we convert Payment_Method without saving the results. This will make it easy to see how get_dummies( ) works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284b6c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(X, columns=['Payment_Method']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf172d",
   "metadata": {},
   "source": [
    "As we can see get_dummies( ) re-orders the dataframe so that non-processed columns are on the left and encoded columns are on the right of the table, so the values for Payment_Method will appear on the right of the dataframe, split into 4 columns as expected.\n",
    "\n",
    "NOTE: In a real situation, not a tutorial like this, we should verify all of the columns to make sure they only contain the accepted categories. However, for this tutorial I have already done that, so we can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20355df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded = pd.get_dummies(X, columns=['City','Dependents','Contract','Device_Protection','Gender','Internet_Service',\n",
    "                                       'Multiple_Lines','Online_Backup','Online_Security','Paperless_Billing',\n",
    "                                       'Partner','Payment_Method','Phone_Service','Senior_Citizen','Streaming_Movies',\n",
    "                                       'Streaming_TV','Tech_Support'])\n",
    "X_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f133900d",
   "metadata": {},
   "source": [
    "Note, there are now a total of 1178 columns mainly because the encoded dataframe now includes a different column for each City name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10102d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517ba75",
   "metadata": {},
   "source": [
    "# Build a Preliminary XGBoost Model\n",
    "\n",
    "Now that we have data that is correctly formatted for making an XGBoost model, we can split the data into training and testing sets. Let's observe that this data is imbalanced by dividing the number of people who left the company, where y = 1, by the total number of people in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c56dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f44fcd",
   "metadata": {},
   "source": [
    "So we see that only 27% of people in the dataset left the company. Because of this, when we split the data into training and testing, we will split using stratification in order to maintain the same percentage of people who left the company in both the training set and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ede6cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433995c6",
   "metadata": {},
   "source": [
    "Now let's verify that using stratify worked as expected..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe9552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_train)/len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c2c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2851c3be",
   "metadata": {},
   "source": [
    "Stratify worked as expected and both y_train and y_test have the same percentage of people that left the company. Now let's build the preliminary model.\n",
    "\n",
    "NOTE: Instead of determining the optimal number of trees using cross validation, we will use early stopping to stop building trees when they no longer improve the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a19667",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set=[(X_train, y_train), (X_test, y_test)]\n",
    "clf_xgb = xgb.XGBClassifier(objective='binary:logistic', missing=None, seed=42)\n",
    "clf_xgb.fit(X_train, y_train, verbose=True, early_stopping_rounds=10, eval_metric='aucpr', eval_set=eval_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a520c3",
   "metadata": {},
   "source": [
    "Okay, we've built an XGBoost model for classification (with only 21 trees in this particular run). Let's see how it performs on the Testing dataset by running the Testing dataset down the model and drawing a Confusion Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf_xgb, X_test, y_test, values_format='d', display_labels=[\"Did not leave\",\"Left\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd17d1",
   "metadata": {},
   "source": [
    "In the confusion matrix, we see that of the 1294 people that did not leave, 1178 (91%) were correctly classified. Of the 467 people that left the company, 239 (51%) were correctly classified. So the XGBoost model was not awesome. Part of the problem is that our data is imbalanced which we saw earlier and we see this in the confusion matrix with the top row showing 1262 people that did not default and the bottom row showing 467 people who did. Because people leaving costs the company a lot of money, we would like to capture more of the people that left. The good news is that XGBoost has a parameter, scale_pos_weight, that helps with imbalanced data. So let's try to improve predictions with cross validation to optimize parameters.\n",
    "\n",
    "# Optimize Parameters Using Cross Validation and Grid Search\n",
    "\n",
    "XGBoost has a lot of hyperparameters, parameters that we have to manually configure and are not determined by XGBoost itself, including max_depth, the maximum tree depth, learning_rate, the learning rate, or \"eta\", gamma, the parameter that encourages pruning, and reg_lambda, the lambda regularization parameter. So let's try to find the optimal values for these hyperparameters in hopes that we can improve the accuracy with the Testing dataset.\n",
    "\n",
    "NOTE: since we have many hyperparameters to optimize, we will use GridSearchCV( ). We specify a bunch of potential values for the hyperparameters and GridSearchCV( ) tests all possible combinations of the parameters for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394ba0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## NOTE: when data are imbalanced, the XGBoost manual says...\n",
    "# ## if you care only about the overall performance metric (AUC) of your prediction\n",
    "# ##     * Balance the positive and negative weights via scale_pos_weight\n",
    "# ##     * Use AUC for evaluation\n",
    "# ## ALSO NOTE: I ran GridSearchCV sequentially on subsets of parameter options rather than all at once, in order to\n",
    "# ## optimize parameters in a short period of time.\n",
    "\n",
    "## ROUND 1:\n",
    "# param_grid = {\n",
    "#    'max_depth': [3, 4, 5],\n",
    "#    'learning_rate': [0.1, 0.01, 0.05],\n",
    "#    'gamma': [0, 0.25, 1.0],\n",
    "#    'reg_lambda': [0, 1.0, 10.0],\n",
    "#    'scale_pos_weight': [1, 3, 5]     # NOTE: XGBoost recommends sum(negative instances) / sum(positive instances)\n",
    "# }\n",
    "\n",
    "## Output: max_depth: 4, learning_rate: 0.1, gamma: 0.25, reg_lambda: 10, scale_pos_weight: 3\n",
    "## Because learning_rate and reg_lambda were at the end of their range we will continue to explore.\n",
    "\n",
    "## ROUND 2:\n",
    "# param_grid = {\n",
    "#    'max_depth': [4],\n",
    "#    'learning_rate': [0.1, 0.5, 1],\n",
    "#    'gamma': [0.25],\n",
    "#    'reg_lambda': [10, 20, 100],\n",
    "#    'scale_pos_weight': [3]\n",
    "# }\n",
    "\n",
    "# ## Output: max_depth: 4, learning_rate: 0.1, reg_lambda: 10\n",
    "# ## NOTE: To speed up cross-validation and to further prevent overfitting,\n",
    "# ## we are only using a random subset of the data (90%) and are only using a random subset of the features\n",
    "# ## or columns (50%) per tree.\n",
    "\n",
    "# optimal_params = GridSearchCV(estimator=xgb.XGBClassifier(objective='binary: logistic', \n",
    "#        seed=42, \n",
    "#        subsample=0.9, \n",
    "#        colsample_bytree=0.5),\n",
    "#        param_grid=param_grid,\n",
    "#        scoring='roc_auc',\n",
    "#        verbose=0,\n",
    "#        n_jobs=10,\n",
    "#        cv=3\n",
    "# )\n",
    "\n",
    "# optimal_params.fit(X_train, y_train, early_stopping_rounds=10, eval_metric='auc', eval_set=[(X_test, y_test)],\n",
    "#        verbose=False)\n",
    "# print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c83cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [3, 4, 5],\n",
    "              'learning_rate': [0.1, 0.01, 0.05],\n",
    "              'gamma': [0, 0.25, 1.0],\n",
    "              'reg_lambda': [0, 1.0, 10.0],\n",
    "              'scale_pos_weight': [1, 3, 5]     # NOTE: XGBoost recommends sum(negative instances) / sum(positive instances)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf99e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'max_depth': [4],\n",
    "              'learning_rate': [0.1, 0.5, 1],\n",
    "              'gamma': [0.25],\n",
    "              'reg_lambda': [10, 20, 100],\n",
    "              'scale_pos_weight': [3]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params = GridSearchCV(estimator=xgb.XGBClassifier(objective='binary:logistic', \n",
    "                            seed=42, \n",
    "                            subsample=0.9, \n",
    "                            colsample_bytree=0.5),\n",
    "                            param_grid=param_grid,\n",
    "                            scoring='roc_auc',\n",
    "                            verbose=0,\n",
    "                            n_jobs=10,\n",
    "                            cv=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2c9e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_params.fit(X_train, y_train, early_stopping_rounds=10, eval_metric='auc', eval_set=[(X_test, y_test)], verbose=False)\n",
    "print(optimal_params.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f6729",
   "metadata": {},
   "source": [
    "So after testing with all possible combinations of the potential parameter values with Cross Validation, we see that we should set gamma=0.25, learning_rate=0.1, max_depth=4 and reg_lambda=10.\n",
    "\n",
    "# Building, Evaluating, Drawing and Interpreting the Optimal XGBoost Model\n",
    "\n",
    "Now that we have the ideal parameter values we can build the final XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89466726",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_xgb = xgb.XGBClassifier(seed=42,\n",
    "                           objective='binary: logistic',\n",
    "                           gamma=0.25,\n",
    "                           learning_rate=0.1,\n",
    "                           max_depth=4,\n",
    "                           reg_lambda=10,\n",
    "                           scale_pos_weight=3,\n",
    "                           subsample=0.9,\n",
    "                           colsample_bytree=0.5)\n",
    "\n",
    "clf_xgb.fit(X_train,\n",
    "           y_train,\n",
    "           verbose=True,\n",
    "           early_stopping_rounds=10,\n",
    "           eval_metric='aucpr',\n",
    "           eval_set=[(X_test, y_test)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ee9ade",
   "metadata": {},
   "source": [
    "Remember, there were 10 trees before this final iteration where the model score did not improve before that!\n",
    "\n",
    "Now let's draw another confusion matrix to see if the optimized XGBoost model performs any better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eed0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(clf_xgb,\n",
    "                     X_test,\n",
    "                     y_test,\n",
    "                     values_format='d',\n",
    "                     display_labels=[\"Did not leave\", \"Left\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f818e0",
   "metadata": {},
   "source": [
    "We see that the optimized XGBoost model is a lot better at identifying people that left the company. Of the 467 people that left the company, 390 (84%) were correctly identified. Before optimization we only correctly identified 239 (51%), however this improvement was at the expense of not being able to correctly classify as many people that did not leave.\n",
    "\n",
    "The last thing we're going to do is draw the first XGBoost tree and discuss how to interpret it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ef5710",
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we want to get information, like gain and cover etc, at each node in the first tree\n",
    "## we will just build the first tree, otherwise we'll get the average over all of the trees.\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(seed=42,\n",
    "                           objective='binary:logistic',\n",
    "                           gamma=0.25,\n",
    "                           learning_rate=0.1,\n",
    "                           max_depth=4,\n",
    "                           reg_lambda=10,\n",
    "                           scale_pos_weight=3,\n",
    "                           subsample=0.9,\n",
    "                           colsample_bytree=0.5,\n",
    "                           n_estimators=1)\n",
    "\n",
    "clf_xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b689d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = clf_xgb.get_booster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d245a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "for importance_type in ('weight','gain','cover','total_gain','total_cover'):\n",
    "    print('%s: ' % importance_type, bst.get_score(importance_type=importance_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1435cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_params = {'shape': 'box',      # make the nodes fancy\n",
    "               'style': 'filled','rounded',\n",
    "               'fillcolor': '#78cbe'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb085145",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaf_params = {'shape': 'box',\n",
    "               'style': 'filled',\n",
    "               'fillcolor': '#e48038'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.to_graphviz(clf_xgb, num_trees=0, size=\"10,10\",\n",
    "               condition_node_params=node_params,\n",
    "               leaf_node_params=leaf_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79500e",
   "metadata": {},
   "source": [
    "Now let's discuss how to interpret the XGBoost tree. In each node we have:\n",
    "\n",
    "1. The variable (column name) and the threshold for splitting the observations. For example, in the trees root we use Contract Month_to_month to split the observations. All observations with Contract 'Month_to_month < 1 go to the left and all others go to the right (i.e. values >= 1)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
